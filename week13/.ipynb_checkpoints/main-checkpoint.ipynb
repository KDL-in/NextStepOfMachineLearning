{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "TiUTLduK6R2P",
    "outputId": "97262d9a-bd56-4441-dd61-5418d45e138a"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "id": "Vd6bakGmQUEm",
    "outputId": "e74b373f-7c8e-4bfc-a5e2-83683d22c464"
   },
   "outputs": [],
   "source": [
    "# !cp -Rf /content/drive/My\\ Drive/hw12/real_or_drawing.zip ./\n",
    "# !unzip -qq real_or_drawing.zip\n",
    "# !pip install pytorch-model-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7JsoxYtmO1v9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pytorch_model_summary import summary as summary\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "      torch.cuda.manual_seed(seed)\n",
    "      torch.cuda.manual_seed_all(seed)\n",
    "set_seed(0)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "path = '/content/drive/Shared drives/shared drive/hw12'\n",
    "path = 'e:/p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j2PjWdhnIKMJ"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IZd8vC3oIeWU"
   },
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "2-9c0yKUmfyX",
    "outputId": "5efd6823-001b-485c-b90a-c4c59cfa3e2a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_distribution(pred_labels):\n",
    "    fig = plt.figure()\n",
    "    plt.bar(labels_set, counts)\n",
    "    plt.title(f'Ditribution after {total_epochs}')\n",
    "    return fig\n",
    "\n",
    "def no_axis_show(img, title='', cmap=None):\n",
    "  # imshow, 縮放模式為nearest。\n",
    "  fig = plt.imshow(img, interpolation='nearest', cmap=cmap)\n",
    "  # 不要顯示axis。\n",
    "  fig.axes.get_xaxis().set_visible(False)\n",
    "  fig.axes.get_yaxis().set_visible(False)\n",
    "  plt.title(title)\n",
    "\n",
    "titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
    "plt.figure(figsize=(18, 18))\n",
    "for i in range(10):\n",
    "  # 注意和subplots的区别，这个方法可以和平常的使用无缝对接\n",
    "  # 它应该类似于全局设置，而subplots则是单独设置\n",
    "  plt.subplot(1, 10, i+1)\n",
    "  fig = no_axis_show(plt.imread(f'real_or_drawing/train_data/{i}/{500*i}.bmp'), title=titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "DT4YwZI_nqVC",
    "outputId": "e0097fcf-ee1b-4892-c872-eced3ef7e81a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18, 18))\n",
    "for i in range(10):\n",
    "  plt.subplot(1, 10, i+1)\n",
    "  fig = no_axis_show(plt.imread(f'real_or_drawing/test_data/0/' + str(i).rjust(5, '0') + '.bmp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Id5yTHsZxulR"
   },
   "source": [
    "**Canny Edge Detection**\n",
    "\n",
    "图片素描风格化的库，有两个阈值，low和high，如果高于high，则为阈值，如果高于low低于high，则进一步做判断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqsJZAfdw06d"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "HgmUw6Yrnw1k",
    "outputId": "3fad8dd6-6efb-4ffa-b76e-28d74404d110"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
    "plt.figure(figsize=(18, 18))\n",
    "\n",
    "original_img = plt.imread(f'real_or_drawing/train_data/0/0.bmp')\n",
    "plt.subplot(1, 5, 1)\n",
    "no_axis_show(original_img, title='original')\n",
    "\n",
    "gray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
    "plt.subplot(1, 5, 2)\n",
    "no_axis_show(gray_img, title='gray scale', cmap='gray')\n",
    "\n",
    "canny_50100 = cv2.Canny(gray_img, 50, 100)\n",
    "plt.subplot(1, 5, 3)\n",
    "no_axis_show(canny_50100, title='Canny(50, 100)', cmap='gray')\n",
    "\n",
    "canny_150200 = cv2.Canny(gray_img, 150, 200)\n",
    "plt.subplot(1, 5, 4)\n",
    "no_axis_show(canny_150200, title='Canny(150, 200)', cmap='gray')\n",
    "\n",
    "canny_250300 = cv2.Canny(gray_img, 250, 300)\n",
    "plt.subplot(1, 5, 5)\n",
    "no_axis_show(canny_250300, title='Canny(250, 300)', cmap='gray')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "74FbJFNVHeUi",
    "outputId": "bacd0b60-28a5-4ad9-a892-d2918f165a5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(gray_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z7L0HfPGIlI4"
   },
   "source": [
    "## Data\n",
    "\n",
    "对于图像数据，torch有非常好的预处理的API。ImageFolder可以直接输入地址，文件夹是标签，需要提供合适的tensforms。\n",
    "\n",
    "对于source。\n",
    "- 转成灰度图像\n",
    "- 边缘风格化处理，用 transforms.Lambda映射cv2.Canny函数，处理原始图像\n",
    "- 转为图片，data augment，转tensor\n",
    "\n",
    "对于target\n",
    "- 转灰度图\n",
    "- resize到和source一样大小\n",
    "- 转为图片，data augment，转tensor\n",
    "\n",
    "但是对于target上做data augment，我先保留看法，我认为这个会让discriminator更容易辨认，所以extrator的反向操作的强度会加剧，所以应该会然discriminator的loss变大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pw98wf_PxMKP"
   },
   "outputs": [],
   "source": [
    "\n",
    "source_transform = transforms.Compose([\n",
    "    # 轉灰階: Canny 不吃 RGB。\n",
    "    transforms.Grayscale(),\n",
    "    # cv2 不吃 skimage.Image，因此轉成np.array後再做cv2.Canny\n",
    "    transforms.Lambda(lambda x: cv2.Canny(np.array(x), 170, 300)),\n",
    "    # cv2.Canny只接受ndarray，且不能是RGB\n",
    "    # 重新將np.array 轉回 skimage.Image\n",
    "    transforms.ToPILImage(),\n",
    "    # 水平翻轉 (Augmentation)\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # 旋轉15度內 (Augmentation)，旋轉後空的地方補0\n",
    "    transforms.RandomRotation(15, fill=(0,)),\n",
    "    # 最後轉成Tensor供model使用。\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "target_transform = transforms.Compose([\n",
    "    # 轉灰階: 將輸入3維壓成1維。\n",
    "    transforms.Grayscale(),\n",
    "    # 縮放: 因為source data是32x32，我們將target data的28x28放大成32x32。\n",
    "    transforms.Resize((32, 32)),\n",
    "    # 水平翻轉 (Augmentation)\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # 旋轉15度內 (Augmentation)，旋轉後空的地方補0\n",
    "    transforms.RandomRotation(15, fill=(0,)),\n",
    "    # 最後轉成Tensor供model使用。\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    # 轉灰階: 將輸入3維壓成1維。\n",
    "    transforms.Grayscale(),\n",
    "    # 縮放: 因為source data是32x32，我們將target data的28x28放大成32x32。\n",
    "    transforms.Resize((32, 32)),\n",
    "    # 最後轉成Tensor供model使用。\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def get_data_loader(dataset, with_aug = True, mode = 'train', batch_size = 64):\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    return DataLoader(dataset, batch_size = batch_size, shuffle = shuffle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FrDRqdsPOKGr"
   },
   "outputs": [],
   "source": [
    "source_dataset = ImageFolder('real_or_drawing/train_data', transform=source_transform)\n",
    "target_dataset = ImageFolder('real_or_drawing/test_data', transform=target_transform)\n",
    "test_dataset = ImageFolder('real_or_drawing/test_data', transform=test_transform)\n",
    "source_loader = get_data_loader(source_dataset, mode = 'train')\n",
    "target_loader = get_data_loader(target_dataset, mode = 'train')\n",
    "test_loader = get_data_loader(target_dataset, mode = 'eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BybA__Wlfav_"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOjSyUbfOiE3"
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1), # (64, 32, 32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), #（128，16， 16）\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # (256, 8, 8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(256, 256, 3, 1, 1), # (256, 256, 4, 4)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 1, 1), # (512, 2, 2)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                 # (512, 1, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x).squeeze()\n",
    "        return x\n",
    "\n",
    "class LabelPredictor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LabelPredictor, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        c = self.layer(h)\n",
    "        return c\n",
    "\n",
    "class DomainClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DomainClassifier, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        y = self.layer(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0dp-RfIoN7c"
   },
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    torch.save(model.state_dict(), os.path.join(path, name))\n",
    "\n",
    "def load_model(model, name):\n",
    "    model.load_state_dict(torch.load(os.path.join(path, name),map_location=device) )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eIDNavYxivOd"
   },
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor().to(device)\n",
    "label_predictor = LabelPredictor().to(device)\n",
    "domain_classifier = DomainClassifier().to(device)\n",
    "\n",
    "class_criterion = nn.CrossEntropyLoss()\n",
    "domain_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_F = optim.Adam(feature_extractor.parameters())\n",
    "optimizer_C = optim.Adam(label_predictor.parameters())\n",
    "optimizer_D = optim.Adam(domain_classifier.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QWR3G7eei58v",
    "outputId": "510dfb03-8efd-443a-ec82-184a37abec75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "      Layer (type)         Output Shape         Param #     Tr. Param #\n",
      "========================================================================\n",
      "          Conv2d-1      [1, 64, 32, 32]             640             640\n",
      "     BatchNorm2d-2      [1, 64, 32, 32]             128             128\n",
      "            ReLU-3      [1, 64, 32, 32]               0               0\n",
      "       MaxPool2d-4      [1, 64, 16, 16]               0               0\n",
      "          Conv2d-5     [1, 128, 16, 16]          73,856          73,856\n",
      "     BatchNorm2d-6     [1, 128, 16, 16]             256             256\n",
      "            ReLU-7     [1, 128, 16, 16]               0               0\n",
      "       MaxPool2d-8       [1, 128, 8, 8]               0               0\n",
      "          Conv2d-9       [1, 256, 8, 8]         295,168         295,168\n",
      "    BatchNorm2d-10       [1, 256, 8, 8]             512             512\n",
      "           ReLU-11       [1, 256, 8, 8]               0               0\n",
      "      MaxPool2d-12       [1, 256, 4, 4]               0               0\n",
      "         Conv2d-13       [1, 256, 4, 4]         590,080         590,080\n",
      "    BatchNorm2d-14       [1, 256, 4, 4]             512             512\n",
      "           ReLU-15       [1, 256, 4, 4]               0               0\n",
      "      MaxPool2d-16       [1, 256, 2, 2]               0               0\n",
      "         Conv2d-17       [1, 512, 2, 2]       1,180,160       1,180,160\n",
      "    BatchNorm2d-18       [1, 512, 2, 2]           1,024           1,024\n",
      "           ReLU-19       [1, 512, 2, 2]               0               0\n",
      "      MaxPool2d-20       [1, 512, 1, 1]               0               0\n",
      "========================================================================\n",
      "Total params: 2,142,336\n",
      "Trainable params: 2,142,336\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Linear-1            [1, 512]         262,656         262,656\n",
      "            ReLU-2            [1, 512]               0               0\n",
      "          Linear-3            [1, 512]         262,656         262,656\n",
      "            ReLU-4            [1, 512]               0               0\n",
      "          Linear-5             [1, 10]           5,130           5,130\n",
      "=======================================================================\n",
      "Total params: 530,442\n",
      "Trainable params: 530,442\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Linear-1            [1, 512]         262,656         262,656\n",
      "     BatchNorm1d-2            [1, 512]           1,024           1,024\n",
      "            ReLU-3            [1, 512]               0               0\n",
      "          Linear-4            [1, 512]         262,656         262,656\n",
      "     BatchNorm1d-5            [1, 512]           1,024           1,024\n",
      "            ReLU-6            [1, 512]               0               0\n",
      "          Linear-7            [1, 512]         262,656         262,656\n",
      "     BatchNorm1d-8            [1, 512]           1,024           1,024\n",
      "            ReLU-9            [1, 512]               0               0\n",
      "         Linear-10            [1, 512]         262,656         262,656\n",
      "    BatchNorm1d-11            [1, 512]           1,024           1,024\n",
      "           ReLU-12            [1, 512]               0               0\n",
      "         Linear-13              [1, 1]             513             513\n",
      "=======================================================================\n",
      "Total params: 1,055,233\n",
      "Trainable params: 1,055,233\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(summary(feature_extractor, torch.zeros(1, 1, 32, 32).to(device)))\n",
    "print(summary(label_predictor, torch.zeros(1, 512).to(device)))\n",
    "print(summary(domain_classifier, torch.zeros(1, 512).to(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hoJTsw6lFI1"
   },
   "source": [
    "## Training\n",
    "\n",
    "DaNN的训练的理解如下：\n",
    "- 两条线，第一是对于带标签数据的acc，包含extractor和predictor，另一条线是无标签的source/target二discriminator的acc\n",
    "- 我们的目标是使得extractor提取的features混合在一起，不能让discriminator简单的分辨出来，最好分辨不出来。\n",
    "- 于是我们同时训练二者，A step，训练d，让它尽可能分类准，B step，训练e和p，取d的负梯度，让features发生变化，这种变化使得d变得无法分辨\n",
    "- d会不断地挣扎，A step会让d尽力去从feature中区分source和target， 而B step则总是混淆features，让d准确率降低\n",
    "- 因为d无法看到feature之前的东西，feature之前的东西不受到d的控制，因而d最后一定会坏掉。\n",
    "\n",
    "DaNN重要的指标：\n",
    "- d的loss，或者二分类的acc变化，我们想让它越差越好\n",
    "\n",
    "先测试一下epoch的影响，由于要使用kaggle，我试试100，200， 300， 400，500三个参数下的大致表现，然后剩下两次用于测试，target data不进行aug，训练的时候aug，测试的时候不aug。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxPf5Va-ix-z"
   },
   "outputs": [],
   "source": [
    "def train_epoch(source_dataloader, target_dataloader, lamb):\n",
    "    '''\n",
    "      Args:\n",
    "        source_dataloader: source data的dataloader\n",
    "        target_dataloader: target data的dataloader\n",
    "        lamb: 調控adversarial的loss係數。\n",
    "    '''\n",
    "\n",
    "    # D loss: Domain Classifier的loss\n",
    "    # F loss: Feature Extrator & Label Predictor的loss\n",
    "    # total_hit: 計算目前對了幾筆 total_num: 目前經過了幾筆\n",
    "    running_D_loss, running_F_loss = 0.0, 0.0\n",
    "    total_hit, total_num = 0.0, 0.0\n",
    "\n",
    "    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_dataloader, target_dataloader)):\n",
    "\n",
    "        source_data = source_data.cuda()\n",
    "        source_label = source_label.cuda()\n",
    "        target_data = target_data.cuda()\n",
    "        \n",
    "        # 我們把source data和target data混在一起，否則batch_norm可能會算錯 (兩邊的data的mean/var不太一樣)\n",
    "        # batch normlization layer\n",
    "        mixed_data = torch.cat([source_data, target_data], dim=0)\n",
    "        domain_label = torch.zeros([source_data.shape[0] + target_data.shape[0], 1]).cuda()\n",
    "        # 設定source data的label為1\n",
    "        domain_label[:source_data.shape[0]] = 1\n",
    "\n",
    "        # Step 1 : 訓練Domain Classifier\n",
    "        feature = feature_extractor(mixed_data)\n",
    "        # 因為我們在Step 1不需要訓練Feature Extractor，所以把feature detach避免loss backprop上去。\n",
    "        domain_logits = domain_classifier(feature.detach())\n",
    "        loss = domain_criterion(domain_logits, domain_label)\n",
    "        running_D_loss+= loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Step 2 : 訓練Feature Extractor和Predictor\n",
    "        class_logits = label_predictor(feature[:source_data.shape[0]])\n",
    "        domain_logits = domain_classifier(feature)\n",
    "        # loss為原本的class CE - lamb * domain BCE，相減的原因同GAN中的Discriminator中的G loss\n",
    "        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label)\n",
    "        running_F_loss+= loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_F.step()\n",
    "        optimizer_C.step()\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        optimizer_F.zero_grad()\n",
    "        optimizer_C.zero_grad()\n",
    "\n",
    "        total_hit += torch.sum(torch.argmax(class_logits, dim=1) == source_label).item()\n",
    "        total_num += source_data.shape[0]\n",
    "        print(i, end='\\r')\n",
    "\n",
    "    return running_D_loss / (i+1), running_F_loss / (i+1), total_hit / total_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iEcqNJ_K4evp"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTGnuAk54hZ4"
   },
   "outputs": [],
   "source": [
    "def testing(extractor, predictor, loader, return_true_labels = False):\n",
    "    pred_labels = []\n",
    "    latents = []\n",
    "    labels = []\n",
    "    pred_probs = []\n",
    "    extractor.to(device)\n",
    "    predictor.to(device)\n",
    "    predictor.eval()\n",
    "    extractor.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (test_data, label) in enumerate(loader):\n",
    "            test_data = test_data.cuda()\n",
    "            latent = feature_extractor(test_data)\n",
    "            class_logits = label_predictor(latent)\n",
    "            pred_prob = torch.softmax(class_logits, dim = 1).cpu().detach().numpy()\n",
    "            pred_label = torch.argmax(class_logits, dim=1).cpu().detach().numpy()\n",
    "\n",
    "            pred_labels.append(pred_label)\n",
    "            pred_probs.append(pred_prob)\n",
    "            latents.append(latent.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "            \n",
    "    pred_labels = np.concatenate(pred_labels)\n",
    "    pred_probs = np.concatenate(pred_probs)\n",
    "    latents = np.concatenate(latents, axis = 0)\n",
    "    labels = np.concatenate(labels, axis = 0)\n",
    "    if return_true_labels == True:\n",
    "        return pred_labels, pred_probs, latents, labels\n",
    "    return pred_labels,pred_probs, latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phHgmqGARX_a",
    "outputId": "c8501c08-7db0-48f7-dd5b-754af7140f4a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# testing\n",
    "set_seed(0)\n",
    "# model name\n",
    "epochs_num = 500\n",
    "ex_name = f'extractor_model_{epochs_num}.bin'\n",
    "pr_name = f'predictor_model_{epochs_num}.bin'\n",
    "# data\n",
    "test_loader = get_data_loader(test_dataset, mode = 'eval')\n",
    "# load model\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "label_predictr = LabelPredictor().to(device)\n",
    "feature_extractor = load_model(feature_extractor, ex_name)\n",
    "label_predictor = load_model(label_predictor, pr_name)\n",
    "#predict\n",
    "pred_labels, pred_probs, latents = testing(feature_extractor, label_predictor, test_loader)\n",
    "# save\n",
    "df = pd.DataFrame({'id': np.arange(0,len(pred_labels)), 'label': pred_labels})\n",
    "df.to_csv(f'DaNN_submission_{ex_name}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "il0MYc1Ohg8X"
   },
   "source": [
    "## Ensemble\n",
    "\n",
    "这边使用soft voting，实现weighted voting的方式以及hard label的方式。\n",
    "\n",
    "结果是，直接从61掉到了59，看来前期的模型是太弱了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aB0w6RI01KvH"
   },
   "outputs": [],
   "source": [
    "def soft_voting(preds, weights = None):\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(preds))\n",
    "    weights = np.array(weights).reshape((-1, 1, 1))\n",
    "    # print(preds[0].shape)\n",
    "    preds = np.concatenate([p[np.newaxis,:] for p in preds],axis=0)\n",
    "    # print(preds.shape)\n",
    "    preds = np.sum(preds * weights, axis=0)\n",
    "    # print(preds)\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EAMrgjClxjTD"
   },
   "outputs": [],
   "source": [
    "models = [100, 200, 300, 400, 500]\n",
    "preds = []\n",
    "preds_hard = []\n",
    "for epochs_num in models:\n",
    "    ex_name = f'extractor_model_{epochs_num}.bin'\n",
    "    pr_name = f'predictor_model_{epochs_num}.bin'\n",
    "    # data\n",
    "    test_loader = get_data_loader(test_dataset, mode = 'eval')\n",
    "    # load model\n",
    "    feature_extractor = FeatureExtractor().to(device)\n",
    "    label_predictr = LabelPredictor().to(device)\n",
    "    feature_extractor = load_model(feature_extractor, ex_name)\n",
    "    label_predictor = load_model(label_predictor, pr_name)\n",
    "    #predict\n",
    "    pred_labels, pred_probs, latents = testing(feature_extractor, label_predictor, test_loader)\n",
    "    preds.append(pred_probs)\n",
    "    preds_hard.append(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOag1f1X10oc"
   },
   "outputs": [],
   "source": [
    "# 100, 0.51349\n",
    "# 200, 0.49530\n",
    "# 300, 0.53430\n",
    "# 400, 0.56480\n",
    "# 500, 0.59531\n",
    "weights = [0.51349, 0.49530,0.53430,0.56480, 0.59531]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHyPPUPayHy9"
   },
   "outputs": [],
   "source": [
    "preds_ens = soft_voting(preds, weights= weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OAWmrmNczkm7"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id': np.arange(0,len(pred_labels)), 'label': preds_ens})\n",
    "df.to_csv(f'DaNN_submission_{ex_name}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LHNmcBLidrYd"
   },
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZeyMf3a2RcB9"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "def cluster(vec, show_plot = False):\n",
    "  # 选择最佳的维度\n",
    "  d = 50\n",
    "  if show_plot:\n",
    "    pca = PCA()\n",
    "    pca.fit(vec)\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    # d = np.argmax(cumsum>0.99)+1\n",
    "    print(d, cumsum[d])\n",
    "    plt.plot(cumsum)\n",
    "    plt.show()\n",
    "  tfs = KernelPCA(n_components=d, kernel='rbf')\n",
    "  vec_1 = tfs.fit_transform(vec)\n",
    "  print('Fisrt Reduction Shape:{}'.format(vec_1.shape))\n",
    "  vec_2 = TSNE(n_components=2, random_state=0).fit_transform(vec_1)\n",
    "  # print(\"Second Reduction Shape:{}\".format(vec_2.shape))\n",
    "#   labels = MiniBatchKMeans(2, random_state=0).fit(vec_2).labels_\n",
    "  return vec_2\n",
    "\n",
    "def plot_scatter(feat, label, savefig=None):\n",
    "    \"\"\" Plot Scatter Image.\n",
    "    Args:\n",
    "      feat: the (x, y) coordinate of clustering result, shape: (9000, 2)\n",
    "      label: ground truth label of image (0/1), shape: (9000,)\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    X = feat[:, 0]\n",
    "    Y = feat[:, 1]\n",
    "    plt.scatter(X, Y, c = label, s =5, alpha=0.5)\n",
    "    plt.legend(loc='best')\n",
    "    if savefig is not None:\n",
    "        plt.savefig(savefig)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P1QfOd0mVeVe"
   },
   "outputs": [],
   "source": [
    "source_transform_noaug = transforms.Compose([\n",
    "    # 轉灰階: Canny 不吃 RGB。\n",
    "    transforms.Grayscale(),\n",
    "    # cv2 不吃 skimage.Image，因此轉成np.array後再做cv2.Canny\n",
    "    transforms.Lambda(lambda x: cv2.Canny(np.array(x), 170, 300)),\n",
    "    # cv2.Canny只接受ndarray，且不能是RGB\n",
    "    # 重新將np.array 轉回 skimage.Image\n",
    "    transforms.ToPILImage(),\n",
    "    # 最後轉成Tensor供model使用。\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "source_dataset_noaug = ImageFolder('real_or_drawing/train_data', transform=source_transform_noaug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "duCTrlW3SjpZ"
   },
   "outputs": [],
   "source": [
    "# plot cluster \n",
    "import pandas as pd\n",
    "# testing\n",
    "set_seed(0)\n",
    "# model name\n",
    "epochs_num = 100\n",
    "ex_name = f'extractor_model_{epochs_num}.bin'\n",
    "pr_name = f'predictor_model_{epochs_num}.bin'\n",
    "# data\n",
    "test_loader = get_data_loader(test_dataset, mode = 'eval')\n",
    "source_loader = get_data_loader(source_dataset_noaug, mode = 'eval')\n",
    "# load model\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "label_predictr = LabelPredictor().to(device)\n",
    "feature_extractor = load_model(feature_extractor, ex_name)\n",
    "label_predictor = load_model(label_predictor, pr_name)\n",
    "#predict\n",
    "pred_s, latents_s,labels_s = testing(feature_extractor, label_predictor, source_loader, return_true_labels=True)\n",
    "pred_t, latents_t = testing(feature_extractor, label_predictor, test_loader)\n",
    "# select idx\n",
    "select_idx = np.arange(len(latents_t))\n",
    "np.random.shuffle(select_idx)\n",
    "select_idx = select_idx[:len(latents_s)]\n",
    "pred_t, latents_t=pred_t[select_idx], latents_t[select_idx,:]\n",
    "# dimension reduction\n",
    "latent_tc = cluster(latents_t, show_plot=True)\n",
    "latent_sc = cluster(latents_s, show_plot=True)\n",
    "latent_all = np.concatenate([latent_tc, latent_sc], axis = 0)\n",
    "labels = np.zeros((len(latent_all),), dtype=np.int32)\n",
    "labels[len(latent_tc):]=1\n",
    "fig = plot_scatter(latent_all, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xc8K8BbfestS"
   },
   "outputs": [],
   "source": [
    "# plot cluster without adv\n",
    "import pandas as pd\n",
    "# testing\n",
    "set_seed(0)\n",
    "# model name\n",
    "epochs_num = 100\n",
    "ex_name = f'extractor_model_noadv_{epochs_num}.bin'\n",
    "pr_name = f'predictor_model_noadv_{epochs_num}.bin'\n",
    "# data\n",
    "test_loader = get_data_loader(test_dataset, mode = 'eval')\n",
    "source_loader = get_data_loader(source_dataset_noaug, mode = 'eval')\n",
    "# load model\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "label_predictr = LabelPredictor().to(device)\n",
    "feature_extractor = load_model(feature_extractor, ex_name)\n",
    "label_predictor = load_model(label_predictor, pr_name)\n",
    "#predict\n",
    "pred_s, latents_s,labels_s = testing(feature_extractor, label_predictor, source_loader, return_true_labels=True)\n",
    "pred_t, latents_t = testing(feature_extractor, label_predictor, test_loader)\n",
    "# select idx\n",
    "select_idx = np.arange(len(latents_t))\n",
    "np.random.shuffle(select_idx)\n",
    "select_idx = select_idx[:len(latents_s)]\n",
    "pred_t, latents_t=pred_t[select_idx], latents_t[select_idx,:]\n",
    "# dimension reduction\n",
    "latent_tc = cluster(latents_t, show_plot=True)\n",
    "latent_sc = cluster(latents_s, show_plot=True)\n",
    "latent_all = np.concatenate([latent_tc, latent_sc], axis = 0)\n",
    "labels = np.zeros((len(latent_all),), dtype=np.int32)\n",
    "labels[len(latent_tc):]=1\n",
    "fig = plot_scatter(latent_all, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qAEr6RqbQEgJ"
   },
   "source": [
    "## Tem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uHoituQrmWvb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.40s, epoch [001\\1500]: train D loss: 0.8902, train F loss: 1.962531, acc 0.243000\n",
      "5.38s, epoch [002\\1500]: train D loss: 0.6804, train F loss: 1.626444, acc 0.395000\n",
      "5.36s, epoch [003\\1500]: train D loss: 0.6368, train F loss: 1.478044, acc 0.452800\n",
      "5.39s, epoch [004\\1500]: train D loss: 0.6202, train F loss: 1.385732, acc 0.484200\n",
      "5.45s, epoch [005\\1500]: train D loss: 0.5201, train F loss: 1.318019, acc 0.515000\n",
      "5.33s, epoch [006\\1500]: train D loss: 0.5349, train F loss: 1.278779, acc 0.538600\n",
      "5.33s, epoch [007\\1500]: train D loss: 0.5456, train F loss: 1.230906, acc 0.544800\n",
      "5.33s, epoch [008\\1500]: train D loss: 0.5257, train F loss: 1.200576, acc 0.564800\n",
      "5.33s, epoch [009\\1500]: train D loss: 0.5452, train F loss: 1.137928, acc 0.580400\n",
      "5.34s, epoch [010\\1500]: train D loss: 0.5172, train F loss: 1.119278, acc 0.593000\n",
      "5.35s, epoch [011\\1500]: train D loss: 0.5367, train F loss: 1.094033, acc 0.600200\n",
      "5.32s, epoch [012\\1500]: train D loss: 0.5161, train F loss: 1.070617, acc 0.612800\n",
      "5.33s, epoch [013\\1500]: train D loss: 0.5020, train F loss: 1.040162, acc 0.624600\n",
      "5.33s, epoch [014\\1500]: train D loss: 0.5042, train F loss: 1.023980, acc 0.620400\n",
      "5.33s, epoch [015\\1500]: train D loss: 0.5235, train F loss: 0.978517, acc 0.641600\n",
      "5.33s, epoch [016\\1500]: train D loss: 0.5438, train F loss: 0.954967, acc 0.648400\n",
      "5.32s, epoch [017\\1500]: train D loss: 0.5229, train F loss: 0.901687, acc 0.670600\n",
      "5.32s, epoch [018\\1500]: train D loss: 0.5487, train F loss: 0.893503, acc 0.674000\n",
      "5.33s, epoch [019\\1500]: train D loss: 0.5096, train F loss: 0.854311, acc 0.688600\n",
      "5.33s, epoch [020\\1500]: train D loss: 0.5020, train F loss: 0.834327, acc 0.698200\n",
      "5.33s, epoch [021\\1500]: train D loss: 0.5272, train F loss: 0.835512, acc 0.692800\n",
      "5.33s, epoch [022\\1500]: train D loss: 0.4980, train F loss: 0.754811, acc 0.723800\n",
      "5.33s, epoch [023\\1500]: train D loss: 0.5119, train F loss: 0.718664, acc 0.731400\n",
      "5.33s, epoch [024\\1500]: train D loss: 0.4945, train F loss: 0.728684, acc 0.731200\n",
      "5.33s, epoch [025\\1500]: train D loss: 0.4848, train F loss: 0.689807, acc 0.741000\n",
      "5.33s, epoch [026\\1500]: train D loss: 0.4787, train F loss: 0.659215, acc 0.757600\n",
      "5.34s, epoch [027\\1500]: train D loss: 0.4689, train F loss: 0.634684, acc 0.760600\n",
      "5.34s, epoch [028\\1500]: train D loss: 0.4928, train F loss: 0.589240, acc 0.777600\n",
      "5.33s, epoch [029\\1500]: train D loss: 0.4747, train F loss: 0.565112, acc 0.796400\n",
      "5.34s, epoch [030\\1500]: train D loss: 0.4647, train F loss: 0.573343, acc 0.787000\n",
      "5.33s, epoch [031\\1500]: train D loss: 0.4830, train F loss: 0.501079, acc 0.802600\n",
      "5.33s, epoch [032\\1500]: train D loss: 0.4641, train F loss: 0.482578, acc 0.815600\n",
      "5.35s, epoch [033\\1500]: train D loss: 0.4687, train F loss: 0.480408, acc 0.823600\n",
      "5.34s, epoch [034\\1500]: train D loss: 0.4577, train F loss: 0.457132, acc 0.831800\n",
      "5.36s, epoch [035\\1500]: train D loss: 0.4634, train F loss: 0.407121, acc 0.848200\n",
      "5.34s, epoch [036\\1500]: train D loss: 0.4578, train F loss: 0.414800, acc 0.843400\n",
      "5.34s, epoch [037\\1500]: train D loss: 0.4217, train F loss: 0.396110, acc 0.847000\n",
      "5.34s, epoch [038\\1500]: train D loss: 0.4274, train F loss: 0.342264, acc 0.871000\n",
      "5.35s, epoch [039\\1500]: train D loss: 0.4081, train F loss: 0.365269, acc 0.870400\n",
      "5.35s, epoch [040\\1500]: train D loss: 0.4423, train F loss: 0.364649, acc 0.862800\n",
      "5.34s, epoch [041\\1500]: train D loss: 0.4143, train F loss: 0.364707, acc 0.861000\n",
      "5.34s, epoch [042\\1500]: train D loss: 0.4241, train F loss: 0.315488, acc 0.875600\n",
      "5.34s, epoch [043\\1500]: train D loss: 0.3935, train F loss: 0.298043, acc 0.889600\n",
      "5.34s, epoch [044\\1500]: train D loss: 0.4324, train F loss: 0.275709, acc 0.898200\n",
      "5.34s, epoch [045\\1500]: train D loss: 0.3934, train F loss: 0.300061, acc 0.889000\n",
      "5.35s, epoch [046\\1500]: train D loss: 0.3980, train F loss: 0.254233, acc 0.907800\n",
      "5.35s, epoch [047\\1500]: train D loss: 0.4088, train F loss: 0.239321, acc 0.906400\n",
      "5.34s, epoch [048\\1500]: train D loss: 0.3902, train F loss: 0.257001, acc 0.905000\n",
      "5.34s, epoch [049\\1500]: train D loss: 0.4003, train F loss: 0.217579, acc 0.913200\n",
      "5.35s, epoch [050\\1500]: train D loss: 0.4002, train F loss: 0.186510, acc 0.926200\n",
      "5.57s, epoch [051\\1500]: train D loss: 0.3897, train F loss: 0.255253, acc 0.906400\n",
      "5.36s, epoch [052\\1500]: train D loss: 0.3853, train F loss: 0.235783, acc 0.907200\n",
      "5.37s, epoch [053\\1500]: train D loss: 0.3973, train F loss: 0.193665, acc 0.917800\n",
      "5.35s, epoch [054\\1500]: train D loss: 0.3906, train F loss: 0.182081, acc 0.924600\n",
      "5.36s, epoch [055\\1500]: train D loss: 0.3903, train F loss: 0.185725, acc 0.927800\n",
      "5.37s, epoch [056\\1500]: train D loss: 0.3762, train F loss: 0.174531, acc 0.932400\n",
      "5.35s, epoch [057\\1500]: train D loss: 0.3801, train F loss: 0.155005, acc 0.934400\n",
      "5.37s, epoch [058\\1500]: train D loss: 0.4012, train F loss: 0.179332, acc 0.927000\n",
      "5.36s, epoch [059\\1500]: train D loss: 0.3907, train F loss: 0.143474, acc 0.941000\n",
      "5.35s, epoch [060\\1500]: train D loss: 0.3864, train F loss: 0.141609, acc 0.937800\n",
      "5.36s, epoch [061\\1500]: train D loss: 0.3850, train F loss: 0.130045, acc 0.941000\n",
      "5.36s, epoch [062\\1500]: train D loss: 0.3742, train F loss: 0.125584, acc 0.945400\n",
      "5.35s, epoch [063\\1500]: train D loss: 0.4034, train F loss: 0.156535, acc 0.931800\n",
      "5.36s, epoch [064\\1500]: train D loss: 0.3781, train F loss: 0.120344, acc 0.948400\n",
      "5.35s, epoch [065\\1500]: train D loss: 0.3922, train F loss: 0.129137, acc 0.941800\n",
      "5.36s, epoch [066\\1500]: train D loss: 0.3837, train F loss: 0.118818, acc 0.947600\n",
      "5.36s, epoch [067\\1500]: train D loss: 0.3870, train F loss: 0.112492, acc 0.955800\n",
      "5.35s, epoch [068\\1500]: train D loss: 0.3918, train F loss: 0.186265, acc 0.925800\n",
      "5.36s, epoch [069\\1500]: train D loss: 0.3858, train F loss: 0.123265, acc 0.944800\n",
      "5.36s, epoch [070\\1500]: train D loss: 0.3921, train F loss: 0.140388, acc 0.939400\n",
      "5.36s, epoch [071\\1500]: train D loss: 0.3657, train F loss: 0.138695, acc 0.940400\n",
      "5.36s, epoch [072\\1500]: train D loss: 0.3555, train F loss: 0.111290, acc 0.952200\n",
      "5.37s, epoch [073\\1500]: train D loss: 0.3713, train F loss: 0.091651, acc 0.955000\n",
      "5.36s, epoch [074\\1500]: train D loss: 0.3821, train F loss: 0.087728, acc 0.960000\n",
      "5.38s, epoch [075\\1500]: train D loss: 0.3735, train F loss: 0.091205, acc 0.961000\n",
      "5.40s, epoch [076\\1500]: train D loss: 0.3669, train F loss: 0.085534, acc 0.959800\n",
      "5.40s, epoch [077\\1500]: train D loss: 0.3861, train F loss: 0.085276, acc 0.959800\n",
      "5.43s, epoch [078\\1500]: train D loss: 0.3847, train F loss: 0.105480, acc 0.954600\n",
      "5.41s, epoch [079\\1500]: train D loss: 0.3694, train F loss: 0.089109, acc 0.958600\n",
      "5.41s, epoch [080\\1500]: train D loss: 0.3750, train F loss: 0.098098, acc 0.955600\n",
      "5.41s, epoch [081\\1500]: train D loss: 0.3736, train F loss: 0.130073, acc 0.946400\n",
      "5.41s, epoch [082\\1500]: train D loss: 0.3700, train F loss: 0.124535, acc 0.943800\n",
      "5.41s, epoch [083\\1500]: train D loss: 0.3632, train F loss: 0.095973, acc 0.959200\n",
      "5.38s, epoch [084\\1500]: train D loss: 0.3668, train F loss: 0.081967, acc 0.961200\n",
      "5.38s, epoch [085\\1500]: train D loss: 0.3731, train F loss: 0.070541, acc 0.965600\n",
      "5.38s, epoch [086\\1500]: train D loss: 0.3712, train F loss: 0.121655, acc 0.948200\n",
      "5.38s, epoch [087\\1500]: train D loss: 0.3778, train F loss: 0.091374, acc 0.958000\n",
      "5.38s, epoch [088\\1500]: train D loss: 0.3588, train F loss: 0.068846, acc 0.962800\n",
      "5.38s, epoch [089\\1500]: train D loss: 0.3545, train F loss: 0.061607, acc 0.969200\n",
      "5.38s, epoch [090\\1500]: train D loss: 0.3682, train F loss: 0.068226, acc 0.970600\n",
      "5.39s, epoch [091\\1500]: train D loss: 0.3715, train F loss: 0.105783, acc 0.955000\n",
      "5.39s, epoch [092\\1500]: train D loss: 0.3848, train F loss: 0.083859, acc 0.961200\n",
      "5.38s, epoch [093\\1500]: train D loss: 0.3663, train F loss: 0.067055, acc 0.965600\n",
      "5.38s, epoch [094\\1500]: train D loss: 0.3730, train F loss: 0.057596, acc 0.969400\n",
      "5.39s, epoch [095\\1500]: train D loss: 0.3684, train F loss: 0.047576, acc 0.974000\n",
      "5.38s, epoch [096\\1500]: train D loss: 0.3810, train F loss: 0.075439, acc 0.964600\n",
      "5.39s, epoch [097\\1500]: train D loss: 0.3757, train F loss: 0.106260, acc 0.953000\n",
      "5.38s, epoch [098\\1500]: train D loss: 0.3680, train F loss: 0.087656, acc 0.959800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.38s, epoch [099\\1500]: train D loss: 0.3644, train F loss: 0.057117, acc 0.969000\n",
      "5.40s, epoch [100\\1500]: train D loss: 0.3736, train F loss: 0.067013, acc 0.963600\n",
      "5.33s, epoch [101\\1500]: train D loss: 0.4257, train F loss: 0.175113, acc 0.928000\n",
      "5.33s, epoch [102\\1500]: train D loss: 0.4048, train F loss: 0.119121, acc 0.948200\n",
      "5.32s, epoch [103\\1500]: train D loss: 0.4150, train F loss: 0.163264, acc 0.933000\n",
      "5.32s, epoch [104\\1500]: train D loss: 0.4230, train F loss: 0.138692, acc 0.940600\n",
      "5.31s, epoch [105\\1500]: train D loss: 0.3856, train F loss: 0.107592, acc 0.951400\n",
      "5.32s, epoch [106\\1500]: train D loss: 0.3888, train F loss: 0.107203, acc 0.951600\n",
      "5.32s, epoch [107\\1500]: train D loss: 0.3813, train F loss: 0.058599, acc 0.970800\n",
      "5.32s, epoch [108\\1500]: train D loss: 0.3824, train F loss: 0.066186, acc 0.965200\n",
      "5.32s, epoch [109\\1500]: train D loss: 0.3890, train F loss: 0.119357, acc 0.950000\n",
      "5.32s, epoch [110\\1500]: train D loss: 0.3955, train F loss: 0.070211, acc 0.963800\n",
      "5.31s, epoch [111\\1500]: train D loss: 0.3947, train F loss: 0.058135, acc 0.963800\n",
      "5.32s, epoch [112\\1500]: train D loss: 0.3960, train F loss: 0.083782, acc 0.960200\n",
      "5.32s, epoch [113\\1500]: train D loss: 0.4054, train F loss: 0.054305, acc 0.968000\n",
      "5.32s, epoch [114\\1500]: train D loss: 0.3843, train F loss: 0.063996, acc 0.965400\n",
      "5.32s, epoch [115\\1500]: train D loss: 0.3871, train F loss: 0.054129, acc 0.969600\n",
      "5.32s, epoch [116\\1500]: train D loss: 0.3842, train F loss: 0.054664, acc 0.969800\n",
      "5.32s, epoch [117\\1500]: train D loss: 0.3744, train F loss: 0.059196, acc 0.967600\n",
      "5.33s, epoch [118\\1500]: train D loss: 0.3965, train F loss: 0.058368, acc 0.968000\n",
      "5.32s, epoch [119\\1500]: train D loss: 0.3916, train F loss: 0.090718, acc 0.956200\n",
      "5.33s, epoch [120\\1500]: train D loss: 0.3903, train F loss: 0.035936, acc 0.977200\n",
      "5.32s, epoch [121\\1500]: train D loss: 0.4014, train F loss: 0.057646, acc 0.965600\n",
      "5.31s, epoch [122\\1500]: train D loss: 0.3839, train F loss: 0.059483, acc 0.969200\n",
      "5.32s, epoch [123\\1500]: train D loss: 0.4064, train F loss: 0.092130, acc 0.956200\n",
      "5.32s, epoch [124\\1500]: train D loss: 0.3942, train F loss: 0.046050, acc 0.970400\n",
      "5.31s, epoch [125\\1500]: train D loss: 0.3885, train F loss: 0.043495, acc 0.969600\n",
      "5.32s, epoch [126\\1500]: train D loss: 0.4144, train F loss: 0.048059, acc 0.968800\n",
      "5.32s, epoch [127\\1500]: train D loss: 0.4055, train F loss: 0.051921, acc 0.970800\n",
      "5.32s, epoch [128\\1500]: train D loss: 0.4098, train F loss: 0.031498, acc 0.978200\n",
      "5.32s, epoch [129\\1500]: train D loss: 0.4058, train F loss: 0.063041, acc 0.968800\n",
      "5.31s, epoch [130\\1500]: train D loss: 0.4004, train F loss: 0.051196, acc 0.969600\n",
      "5.33s, epoch [131\\1500]: train D loss: 0.4060, train F loss: 0.027376, acc 0.976600\n",
      "5.32s, epoch [132\\1500]: train D loss: 0.4032, train F loss: 0.041608, acc 0.974400\n",
      "5.32s, epoch [133\\1500]: train D loss: 0.4069, train F loss: 0.030350, acc 0.976400\n",
      "5.32s, epoch [134\\1500]: train D loss: 0.4017, train F loss: 0.053099, acc 0.967800\n",
      "5.33s, epoch [135\\1500]: train D loss: 0.4077, train F loss: 0.048579, acc 0.970000\n",
      "5.32s, epoch [136\\1500]: train D loss: 0.3881, train F loss: 0.031979, acc 0.979200\n",
      "5.32s, epoch [137\\1500]: train D loss: 0.4063, train F loss: 0.030482, acc 0.977400\n",
      "5.32s, epoch [138\\1500]: train D loss: 0.4277, train F loss: 0.064192, acc 0.966400\n",
      "5.31s, epoch [139\\1500]: train D loss: 0.4185, train F loss: 0.084524, acc 0.960000\n",
      "5.33s, epoch [140\\1500]: train D loss: 0.4229, train F loss: 0.087385, acc 0.957600\n",
      "5.34s, epoch [141\\1500]: train D loss: 0.4079, train F loss: 0.042660, acc 0.973400\n",
      "5.31s, epoch [142\\1500]: train D loss: 0.4064, train F loss: 0.037185, acc 0.975200\n",
      "5.32s, epoch [143\\1500]: train D loss: 0.4119, train F loss: 0.031358, acc 0.975200\n",
      "5.31s, epoch [144\\1500]: train D loss: 0.3987, train F loss: 0.036391, acc 0.978000\n",
      "5.31s, epoch [145\\1500]: train D loss: 0.4221, train F loss: 0.063867, acc 0.963000\n",
      "5.32s, epoch [146\\1500]: train D loss: 0.3940, train F loss: 0.034403, acc 0.974000\n",
      "5.31s, epoch [147\\1500]: train D loss: 0.4124, train F loss: 0.037835, acc 0.973000\n",
      "5.31s, epoch [148\\1500]: train D loss: 0.3944, train F loss: 0.033554, acc 0.978800\n",
      "5.32s, epoch [149\\1500]: train D loss: 0.3953, train F loss: 0.027415, acc 0.978200\n",
      "5.32s, epoch [150\\1500]: train D loss: 0.4222, train F loss: 0.051123, acc 0.972200\n",
      "5.32s, epoch [151\\1500]: train D loss: 0.4322, train F loss: 0.041758, acc 0.972400\n",
      "5.31s, epoch [152\\1500]: train D loss: 0.4187, train F loss: 0.055559, acc 0.970000\n",
      "5.32s, epoch [153\\1500]: train D loss: 0.4277, train F loss: 0.056409, acc 0.968600\n",
      "5.32s, epoch [154\\1500]: train D loss: 0.4283, train F loss: 0.028437, acc 0.976800\n",
      "5.31s, epoch [155\\1500]: train D loss: 0.4170, train F loss: 0.040364, acc 0.970200\n",
      "5.32s, epoch [156\\1500]: train D loss: 0.4126, train F loss: 0.055981, acc 0.968800\n",
      "5.32s, epoch [157\\1500]: train D loss: 0.4221, train F loss: 0.032353, acc 0.975000\n",
      "5.31s, epoch [158\\1500]: train D loss: 0.3998, train F loss: 0.025898, acc 0.977600\n",
      "5.31s, epoch [159\\1500]: train D loss: 0.3832, train F loss: 0.017133, acc 0.982400\n",
      "5.32s, epoch [160\\1500]: train D loss: 0.4247, train F loss: 0.030239, acc 0.974800\n",
      "5.31s, epoch [161\\1500]: train D loss: 0.4152, train F loss: 0.037425, acc 0.975600\n",
      "5.31s, epoch [162\\1500]: train D loss: 0.4187, train F loss: 0.023179, acc 0.979600\n",
      "5.33s, epoch [163\\1500]: train D loss: 0.4107, train F loss: 0.028660, acc 0.976600\n",
      "5.32s, epoch [164\\1500]: train D loss: 0.4186, train F loss: 0.024786, acc 0.981800\n",
      "5.32s, epoch [165\\1500]: train D loss: 0.4320, train F loss: 0.056470, acc 0.967800\n",
      "5.31s, epoch [166\\1500]: train D loss: 0.4091, train F loss: 0.019398, acc 0.981000\n",
      "5.33s, epoch [167\\1500]: train D loss: 0.4030, train F loss: 0.025323, acc 0.978800\n",
      "5.32s, epoch [168\\1500]: train D loss: 0.4380, train F loss: 0.073456, acc 0.961800\n",
      "5.29s, epoch [169\\1500]: train D loss: 0.4238, train F loss: 0.035258, acc 0.978000\n",
      "5.28s, epoch [170\\1500]: train D loss: 0.3928, train F loss: 0.012485, acc 0.983800\n",
      "5.26s, epoch [171\\1500]: train D loss: 0.4235, train F loss: 0.016399, acc 0.981200\n",
      "5.26s, epoch [172\\1500]: train D loss: 0.4180, train F loss: 0.019604, acc 0.980600\n",
      "5.27s, epoch [173\\1500]: train D loss: 0.4161, train F loss: 0.021689, acc 0.980400\n",
      "5.26s, epoch [174\\1500]: train D loss: 0.4107, train F loss: 0.018570, acc 0.978400\n",
      "5.27s, epoch [175\\1500]: train D loss: 0.4163, train F loss: 0.027425, acc 0.977600\n",
      "5.27s, epoch [176\\1500]: train D loss: 0.4248, train F loss: 0.027876, acc 0.977800\n",
      "5.28s, epoch [177\\1500]: train D loss: 0.4201, train F loss: 0.024496, acc 0.977800\n",
      "5.26s, epoch [178\\1500]: train D loss: 0.4155, train F loss: 0.010184, acc 0.983800\n",
      "5.26s, epoch [179\\1500]: train D loss: 0.4231, train F loss: 0.021814, acc 0.982200\n",
      "5.27s, epoch [180\\1500]: train D loss: 0.4853, train F loss: 0.174304, acc 0.939800\n",
      "5.26s, epoch [181\\1500]: train D loss: 0.4295, train F loss: 0.046328, acc 0.973400\n",
      "5.28s, epoch [182\\1500]: train D loss: 0.4124, train F loss: 0.023050, acc 0.979200\n",
      "5.27s, epoch [183\\1500]: train D loss: 0.4112, train F loss: 0.019166, acc 0.979200\n",
      "5.26s, epoch [184\\1500]: train D loss: 0.4160, train F loss: 0.016393, acc 0.981400\n",
      "5.26s, epoch [185\\1500]: train D loss: 0.3961, train F loss: 0.020755, acc 0.979800\n",
      "5.28s, epoch [186\\1500]: train D loss: 0.4249, train F loss: 0.008930, acc 0.983800\n",
      "5.27s, epoch [187\\1500]: train D loss: 0.4085, train F loss: 0.019596, acc 0.982400\n",
      "5.27s, epoch [188\\1500]: train D loss: 0.4229, train F loss: 0.020323, acc 0.978800\n",
      "5.27s, epoch [189\\1500]: train D loss: 0.4255, train F loss: 0.009592, acc 0.982200\n",
      "5.33s, epoch [190\\1500]: train D loss: 0.4290, train F loss: 0.024927, acc 0.978800\n",
      "5.33s, epoch [191\\1500]: train D loss: 0.4227, train F loss: 0.016823, acc 0.980200\n",
      "5.33s, epoch [192\\1500]: train D loss: 0.4196, train F loss: 0.011874, acc 0.984400\n",
      "5.32s, epoch [193\\1500]: train D loss: 0.4426, train F loss: 0.023441, acc 0.977000\n",
      "5.33s, epoch [194\\1500]: train D loss: 0.4448, train F loss: 0.035268, acc 0.973000\n",
      "5.32s, epoch [195\\1500]: train D loss: 0.4301, train F loss: 0.007329, acc 0.982000\n",
      "5.32s, epoch [196\\1500]: train D loss: 0.4175, train F loss: 0.006818, acc 0.982400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.33s, epoch [197\\1500]: train D loss: 0.4457, train F loss: 0.011026, acc 0.982600\n",
      "5.32s, epoch [198\\1500]: train D loss: 0.4297, train F loss: 0.018499, acc 0.981400\n",
      "5.33s, epoch [199\\1500]: train D loss: 0.4233, train F loss: 0.012362, acc 0.982600\n",
      "5.32s, epoch [200\\1500]: train D loss: 0.4311, train F loss: 0.021253, acc 0.978800\n",
      "5.30s, epoch [201\\1500]: train D loss: 0.4321, train F loss: 0.035692, acc 0.975400\n",
      "5.28s, epoch [202\\1500]: train D loss: 0.4292, train F loss: 0.013662, acc 0.982400\n",
      "5.29s, epoch [203\\1500]: train D loss: 0.4297, train F loss: 0.006024, acc 0.984400\n",
      "5.30s, epoch [204\\1500]: train D loss: 0.4377, train F loss: 0.009543, acc 0.985000\n",
      "5.30s, epoch [205\\1500]: train D loss: 0.4525, train F loss: 0.021784, acc 0.978600\n",
      "5.32s, epoch [206\\1500]: train D loss: 0.4270, train F loss: 0.022732, acc 0.977800\n",
      "5.29s, epoch [207\\1500]: train D loss: 0.4395, train F loss: 0.006151, acc 0.983200\n",
      "5.30s, epoch [208\\1500]: train D loss: 0.4330, train F loss: 0.014550, acc 0.982200\n",
      "5.29s, epoch [209\\1500]: train D loss: 0.4260, train F loss: 0.003460, acc 0.984000\n",
      "5.29s, epoch [210\\1500]: train D loss: 0.4336, train F loss: 0.027734, acc 0.978800\n",
      "5.38s, epoch [211\\1500]: train D loss: 0.4170, train F loss: 0.002533, acc 0.986200\n",
      "5.29s, epoch [212\\1500]: train D loss: 0.4277, train F loss: -0.001085, acc 0.985600\n",
      "5.29s, epoch [213\\1500]: train D loss: 0.4433, train F loss: 0.009317, acc 0.982800\n",
      "5.29s, epoch [214\\1500]: train D loss: 0.4578, train F loss: 0.013844, acc 0.979400\n",
      "5.29s, epoch [215\\1500]: train D loss: 0.4429, train F loss: 0.006510, acc 0.983400\n",
      "5.30s, epoch [216\\1500]: train D loss: 0.4332, train F loss: 0.034580, acc 0.976600\n",
      "5.31s, epoch [217\\1500]: train D loss: 0.4301, train F loss: 0.013328, acc 0.982400\n",
      "5.32s, epoch [218\\1500]: train D loss: 0.4362, train F loss: 0.005005, acc 0.984200\n",
      "5.31s, epoch [219\\1500]: train D loss: 0.4347, train F loss: 0.004360, acc 0.985200\n",
      "5.32s, epoch [220\\1500]: train D loss: 0.4450, train F loss: 0.005363, acc 0.984200\n",
      "5.32s, epoch [221\\1500]: train D loss: 0.4290, train F loss: 0.011353, acc 0.982200\n",
      "5.33s, epoch [222\\1500]: train D loss: 0.4627, train F loss: 0.027844, acc 0.977600\n",
      "5.31s, epoch [223\\1500]: train D loss: 0.4475, train F loss: 0.015325, acc 0.981000\n",
      "5.31s, epoch [224\\1500]: train D loss: 0.4403, train F loss: 0.003983, acc 0.982600\n",
      "5.31s, epoch [225\\1500]: train D loss: 0.4388, train F loss: 0.009180, acc 0.983000\n",
      "5.32s, epoch [226\\1500]: train D loss: 0.4216, train F loss: 0.000767, acc 0.986200\n",
      "5.32s, epoch [227\\1500]: train D loss: 0.4204, train F loss: 0.012653, acc 0.983000\n",
      "5.32s, epoch [228\\1500]: train D loss: 0.4763, train F loss: 0.057696, acc 0.969000\n",
      "5.32s, epoch [229\\1500]: train D loss: 0.4403, train F loss: 0.010686, acc 0.986000\n",
      "5.32s, epoch [230\\1500]: train D loss: 0.4580, train F loss: 0.020396, acc 0.980600\n",
      "5.33s, epoch [231\\1500]: train D loss: 0.4513, train F loss: 0.028019, acc 0.977000\n",
      "5.32s, epoch [232\\1500]: train D loss: 0.4475, train F loss: 0.004754, acc 0.983200\n",
      "5.32s, epoch [233\\1500]: train D loss: 0.4542, train F loss: 0.005078, acc 0.984200\n",
      "5.32s, epoch [234\\1500]: train D loss: 0.4432, train F loss: 0.020843, acc 0.980400\n",
      "5.31s, epoch [235\\1500]: train D loss: 0.4296, train F loss: 0.013338, acc 0.981400\n",
      "5.31s, epoch [236\\1500]: train D loss: 0.4310, train F loss: 0.017963, acc 0.980800\n",
      "5.32s, epoch [237\\1500]: train D loss: 0.4266, train F loss: 0.001440, acc 0.987200\n",
      "5.33s, epoch [238\\1500]: train D loss: 0.4522, train F loss: 0.009513, acc 0.982800\n",
      "5.31s, epoch [239\\1500]: train D loss: 0.4368, train F loss: 0.007974, acc 0.982400\n",
      "5.32s, epoch [240\\1500]: train D loss: 0.4334, train F loss: 0.019407, acc 0.981800\n",
      "5.31s, epoch [241\\1500]: train D loss: 0.4255, train F loss: -0.003937, acc 0.986400\n",
      "5.32s, epoch [242\\1500]: train D loss: 0.4303, train F loss: -0.001384, acc 0.986400\n",
      "5.32s, epoch [243\\1500]: train D loss: 0.4608, train F loss: 0.049435, acc 0.971400\n",
      "5.31s, epoch [244\\1500]: train D loss: 0.4317, train F loss: 0.000041, acc 0.987200\n",
      "5.32s, epoch [245\\1500]: train D loss: 0.4375, train F loss: 0.013348, acc 0.981800\n",
      "5.32s, epoch [246\\1500]: train D loss: 0.4284, train F loss: -0.001302, acc 0.989600\n",
      "5.32s, epoch [247\\1500]: train D loss: 0.4519, train F loss: 0.003936, acc 0.983200\n",
      "5.32s, epoch [248\\1500]: train D loss: 0.4630, train F loss: 0.048154, acc 0.968800\n",
      "5.34s, epoch [249\\1500]: train D loss: 0.4516, train F loss: 0.005240, acc 0.983200\n",
      "5.33s, epoch [250\\1500]: train D loss: 0.4214, train F loss: 0.014209, acc 0.984600\n",
      "5.33s, epoch [251\\1500]: train D loss: 0.4678, train F loss: 0.004829, acc 0.983800\n",
      "5.32s, epoch [252\\1500]: train D loss: 0.4530, train F loss: 0.007472, acc 0.982400\n",
      "5.32s, epoch [253\\1500]: train D loss: 0.4419, train F loss: -0.008044, acc 0.988800\n",
      "5.32s, epoch [254\\1500]: train D loss: 0.4387, train F loss: -0.003758, acc 0.987800\n",
      "5.32s, epoch [255\\1500]: train D loss: 0.4370, train F loss: -0.002263, acc 0.986600\n",
      "5.33s, epoch [256\\1500]: train D loss: 0.4268, train F loss: 0.003635, acc 0.985600\n",
      "5.32s, epoch [257\\1500]: train D loss: 0.4601, train F loss: 0.001840, acc 0.985400\n",
      "5.33s, epoch [258\\1500]: train D loss: 0.4591, train F loss: 0.014197, acc 0.983200\n",
      "5.32s, epoch [259\\1500]: train D loss: 0.4541, train F loss: 0.007645, acc 0.984600\n",
      "5.33s, epoch [260\\1500]: train D loss: 0.4431, train F loss: 0.011857, acc 0.982000\n",
      "5.33s, epoch [261\\1500]: train D loss: 0.4454, train F loss: 0.012238, acc 0.981600\n",
      "5.32s, epoch [262\\1500]: train D loss: 0.4531, train F loss: 0.007116, acc 0.983000\n",
      "5.33s, epoch [263\\1500]: train D loss: 0.4427, train F loss: 0.011678, acc 0.982800\n",
      "5.32s, epoch [264\\1500]: train D loss: 0.4499, train F loss: 0.003612, acc 0.984400\n",
      "5.33s, epoch [265\\1500]: train D loss: 0.4386, train F loss: 0.001841, acc 0.986200\n",
      "5.32s, epoch [266\\1500]: train D loss: 0.4358, train F loss: 0.000878, acc 0.985200\n",
      "5.31s, epoch [267\\1500]: train D loss: 0.4432, train F loss: 0.003539, acc 0.983600\n",
      "5.34s, epoch [268\\1500]: train D loss: 0.4522, train F loss: 0.010639, acc 0.983800\n",
      "5.32s, epoch [269\\1500]: train D loss: 0.4193, train F loss: 0.003802, acc 0.986400\n",
      "5.32s, epoch [270\\1500]: train D loss: 0.4524, train F loss: 0.001562, acc 0.985600\n",
      "5.32s, epoch [271\\1500]: train D loss: 0.4587, train F loss: 0.005795, acc 0.985400\n",
      "5.33s, epoch [272\\1500]: train D loss: 0.4598, train F loss: 0.006148, acc 0.984200\n",
      "5.32s, epoch [273\\1500]: train D loss: 0.4585, train F loss: 0.003712, acc 0.983000\n",
      "5.32s, epoch [274\\1500]: train D loss: 0.4363, train F loss: -0.004467, acc 0.986400\n",
      "5.32s, epoch [275\\1500]: train D loss: 0.4565, train F loss: 0.004409, acc 0.984400\n",
      "5.31s, epoch [276\\1500]: train D loss: 0.4583, train F loss: 0.003659, acc 0.985200\n",
      "5.33s, epoch [277\\1500]: train D loss: 0.4433, train F loss: 0.000803, acc 0.985400\n",
      "5.32s, epoch [278\\1500]: train D loss: 0.4617, train F loss: 0.008407, acc 0.983600\n",
      "5.31s, epoch [279\\1500]: train D loss: 0.4644, train F loss: 0.009800, acc 0.982600\n",
      "5.33s, epoch [280\\1500]: train D loss: 0.4702, train F loss: 0.004736, acc 0.984000\n",
      "5.33s, epoch [281\\1500]: train D loss: 0.4665, train F loss: -0.000001, acc 0.984800\n",
      "5.32s, epoch [282\\1500]: train D loss: 0.4600, train F loss: 0.002565, acc 0.986400\n",
      "5.33s, epoch [283\\1500]: train D loss: 0.4446, train F loss: -0.006768, acc 0.988400\n",
      "5.32s, epoch [284\\1500]: train D loss: 0.4325, train F loss: -0.006912, acc 0.987600\n",
      "5.32s, epoch [285\\1500]: train D loss: 0.4628, train F loss: 0.009962, acc 0.981600\n",
      "5.32s, epoch [286\\1500]: train D loss: 0.4652, train F loss: 0.019627, acc 0.980200\n",
      "5.30s, epoch [287\\1500]: train D loss: 0.4457, train F loss: -0.005123, acc 0.985000\n",
      "5.31s, epoch [288\\1500]: train D loss: 0.4469, train F loss: -0.004645, acc 0.987800\n",
      "5.32s, epoch [289\\1500]: train D loss: 0.4480, train F loss: -0.009086, acc 0.987800\n",
      "5.31s, epoch [290\\1500]: train D loss: 0.4567, train F loss: -0.001355, acc 0.985000\n",
      "5.32s, epoch [291\\1500]: train D loss: 0.4468, train F loss: -0.000181, acc 0.987400\n",
      "5.32s, epoch [292\\1500]: train D loss: 0.4563, train F loss: 0.001019, acc 0.985200\n",
      "5.31s, epoch [293\\1500]: train D loss: 0.4604, train F loss: -0.008975, acc 0.988200\n",
      "5.33s, epoch [294\\1500]: train D loss: 0.4563, train F loss: 0.008049, acc 0.982800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.31s, epoch [295\\1500]: train D loss: 0.4570, train F loss: 0.003668, acc 0.984600\n",
      "5.32s, epoch [296\\1500]: train D loss: 0.4640, train F loss: -0.001218, acc 0.985000\n",
      "5.31s, epoch [297\\1500]: train D loss: 0.4384, train F loss: -0.008755, acc 0.989200\n",
      "5.32s, epoch [298\\1500]: train D loss: 0.4526, train F loss: -0.010881, acc 0.990200\n",
      "5.31s, epoch [299\\1500]: train D loss: 0.4591, train F loss: -0.006166, acc 0.988000\n",
      "5.41s, epoch [300\\1500]: train D loss: 0.4655, train F loss: 0.009886, acc 0.983000\n",
      "5.30s, epoch [301\\1500]: train D loss: 0.4524, train F loss: 0.001738, acc 0.985400\n",
      "5.31s, epoch [302\\1500]: train D loss: 0.4646, train F loss: 0.001997, acc 0.984800\n",
      "5.35s, epoch [303\\1500]: train D loss: 0.4472, train F loss: -0.004177, acc 0.986200\n",
      "5.37s, epoch [304\\1500]: train D loss: 0.4429, train F loss: -0.002749, acc 0.987200\n",
      "5.35s, epoch [305\\1500]: train D loss: 0.4435, train F loss: -0.002782, acc 0.986600\n",
      "5.38s, epoch [306\\1500]: train D loss: 0.4618, train F loss: -0.012583, acc 0.989800\n",
      "5.41s, epoch [307\\1500]: train D loss: 0.4659, train F loss: 0.001693, acc 0.985000\n",
      "5.33s, epoch [308\\1500]: train D loss: 0.4840, train F loss: 0.006402, acc 0.982800\n",
      "5.33s, epoch [309\\1500]: train D loss: 0.4889, train F loss: 0.016316, acc 0.981600\n",
      "5.33s, epoch [310\\1500]: train D loss: 0.4693, train F loss: -0.010789, acc 0.988600\n",
      "5.36s, epoch [311\\1500]: train D loss: 0.4570, train F loss: -0.002578, acc 0.986600\n",
      "5.26s, epoch [312\\1500]: train D loss: 0.4811, train F loss: 0.004622, acc 0.984800\n",
      "5.24s, epoch [313\\1500]: train D loss: 0.4625, train F loss: 0.006646, acc 0.982200\n",
      "5.24s, epoch [314\\1500]: train D loss: 0.4550, train F loss: -0.014122, acc 0.990600\n",
      "5.25s, epoch [315\\1500]: train D loss: 0.4496, train F loss: -0.005654, acc 0.987600\n",
      "5.24s, epoch [316\\1500]: train D loss: 0.4679, train F loss: -0.000712, acc 0.986800\n",
      "5.24s, epoch [317\\1500]: train D loss: 0.4659, train F loss: -0.008605, acc 0.986600\n",
      "5.23s, epoch [318\\1500]: train D loss: 0.4653, train F loss: 0.017458, acc 0.986000\n",
      "5.24s, epoch [319\\1500]: train D loss: 0.4931, train F loss: 0.054436, acc 0.968200\n",
      "5.24s, epoch [320\\1500]: train D loss: 0.4499, train F loss: 0.001642, acc 0.985800\n",
      "5.24s, epoch [321\\1500]: train D loss: 0.4593, train F loss: 0.000457, acc 0.986600\n",
      "5.23s, epoch [322\\1500]: train D loss: 0.4559, train F loss: 0.000525, acc 0.985400\n",
      "5.24s, epoch [323\\1500]: train D loss: 0.4585, train F loss: 0.000530, acc 0.985800\n",
      "5.23s, epoch [324\\1500]: train D loss: 0.4644, train F loss: -0.011369, acc 0.988800\n",
      "5.24s, epoch [325\\1500]: train D loss: 0.4423, train F loss: -0.004094, acc 0.987200\n",
      "5.23s, epoch [326\\1500]: train D loss: 0.4527, train F loss: -0.006416, acc 0.988600\n",
      "5.23s, epoch [327\\1500]: train D loss: 0.4576, train F loss: 0.002971, acc 0.981800\n",
      "5.24s, epoch [328\\1500]: train D loss: 0.4497, train F loss: -0.020824, acc 0.992600\n",
      "5.25s, epoch [329\\1500]: train D loss: 0.4472, train F loss: -0.015503, acc 0.991400\n",
      "5.23s, epoch [330\\1500]: train D loss: 0.4614, train F loss: 0.003118, acc 0.985600\n",
      "5.24s, epoch [331\\1500]: train D loss: 0.4444, train F loss: -0.012928, acc 0.991000\n",
      "5.23s, epoch [332\\1500]: train D loss: 0.4582, train F loss: 0.024415, acc 0.978200\n",
      "5.24s, epoch [333\\1500]: train D loss: 0.4385, train F loss: -0.012227, acc 0.991000\n",
      "5.25s, epoch [334\\1500]: train D loss: 0.4448, train F loss: -0.004316, acc 0.988000\n",
      "5.24s, epoch [335\\1500]: train D loss: 0.4537, train F loss: -0.008329, acc 0.987800\n",
      "5.25s, epoch [336\\1500]: train D loss: 0.4467, train F loss: 0.000111, acc 0.986000\n",
      "5.24s, epoch [337\\1500]: train D loss: 0.4522, train F loss: -0.000144, acc 0.986000\n",
      "5.24s, epoch [338\\1500]: train D loss: 0.4649, train F loss: 0.001678, acc 0.986600\n",
      "5.24s, epoch [339\\1500]: train D loss: 0.4442, train F loss: -0.015525, acc 0.990200\n",
      "5.23s, epoch [340\\1500]: train D loss: 0.4652, train F loss: -0.003866, acc 0.987600\n",
      "5.24s, epoch [341\\1500]: train D loss: 0.4576, train F loss: -0.008243, acc 0.988000\n",
      "5.24s, epoch [342\\1500]: train D loss: 0.4566, train F loss: -0.018974, acc 0.991400\n",
      "5.24s, epoch [343\\1500]: train D loss: 0.4828, train F loss: 0.007721, acc 0.980800\n",
      "5.24s, epoch [344\\1500]: train D loss: 0.4541, train F loss: -0.015711, acc 0.990800\n",
      "5.24s, epoch [345\\1500]: train D loss: 0.4752, train F loss: -0.016086, acc 0.990000\n",
      "5.23s, epoch [346\\1500]: train D loss: 0.4616, train F loss: -0.009331, acc 0.987800\n",
      "5.25s, epoch [347\\1500]: train D loss: 0.4830, train F loss: -0.001070, acc 0.983800\n",
      "5.25s, epoch [348\\1500]: train D loss: 0.4798, train F loss: -0.002663, acc 0.986000\n",
      "5.25s, epoch [349\\1500]: train D loss: 0.4833, train F loss: 0.001279, acc 0.985800\n",
      "5.24s, epoch [350\\1500]: train D loss: 0.4712, train F loss: 0.000796, acc 0.985200\n",
      "5.25s, epoch [351\\1500]: train D loss: 0.4765, train F loss: 0.002539, acc 0.984200\n",
      "5.24s, epoch [352\\1500]: train D loss: 0.4860, train F loss: -0.000447, acc 0.983600\n",
      "5.23s, epoch [353\\1500]: train D loss: 0.4514, train F loss: -0.011373, acc 0.987800\n",
      "5.25s, epoch [354\\1500]: train D loss: 0.4783, train F loss: -0.005707, acc 0.987600\n",
      "5.24s, epoch [355\\1500]: train D loss: 0.4713, train F loss: 0.013084, acc 0.981400\n",
      "5.27s, epoch [356\\1500]: train D loss: 0.4787, train F loss: 0.004094, acc 0.983200\n",
      "5.31s, epoch [357\\1500]: train D loss: 0.4689, train F loss: 0.000146, acc 0.984400\n",
      "5.31s, epoch [358\\1500]: train D loss: 0.4623, train F loss: -0.010556, acc 0.989600\n",
      "5.30s, epoch [359\\1500]: train D loss: 0.4646, train F loss: 0.003271, acc 0.983800\n",
      "5.29s, epoch [360\\1500]: train D loss: 0.4653, train F loss: -0.010488, acc 0.988600\n",
      "5.30s, epoch [361\\1500]: train D loss: 0.4625, train F loss: -0.011056, acc 0.989800\n",
      "5.28s, epoch [362\\1500]: train D loss: 0.4662, train F loss: 0.000841, acc 0.987200\n",
      "5.28s, epoch [363\\1500]: train D loss: 0.4776, train F loss: -0.014525, acc 0.989800\n",
      "5.28s, epoch [364\\1500]: train D loss: 0.4624, train F loss: -0.008943, acc 0.987800\n",
      "5.29s, epoch [365\\1500]: train D loss: 0.4622, train F loss: -0.006013, acc 0.988000\n",
      "5.28s, epoch [366\\1500]: train D loss: 0.4685, train F loss: -0.011536, acc 0.990000\n",
      "5.30s, epoch [367\\1500]: train D loss: 0.4688, train F loss: -0.014719, acc 0.989400\n",
      "5.28s, epoch [368\\1500]: train D loss: 0.4668, train F loss: -0.000945, acc 0.985400\n",
      "5.29s, epoch [369\\1500]: train D loss: 0.4619, train F loss: -0.015011, acc 0.989000\n",
      "5.29s, epoch [370\\1500]: train D loss: 0.4582, train F loss: -0.011908, acc 0.987800\n",
      "5.29s, epoch [371\\1500]: train D loss: 0.4712, train F loss: -0.011235, acc 0.989200\n",
      "5.28s, epoch [372\\1500]: train D loss: 0.4694, train F loss: -0.003782, acc 0.988200\n",
      "5.28s, epoch [373\\1500]: train D loss: 0.4771, train F loss: -0.010717, acc 0.988400\n",
      "5.28s, epoch [374\\1500]: train D loss: 0.4744, train F loss: -0.010145, acc 0.987800\n",
      "5.29s, epoch [375\\1500]: train D loss: 0.4781, train F loss: -0.008913, acc 0.987000\n",
      "5.39s, epoch [376\\1500]: train D loss: 0.4746, train F loss: -0.011604, acc 0.987800\n",
      "5.28s, epoch [377\\1500]: train D loss: 0.4777, train F loss: -0.004906, acc 0.987000\n",
      "5.29s, epoch [378\\1500]: train D loss: 0.4753, train F loss: -0.008806, acc 0.988600\n",
      "5.28s, epoch [379\\1500]: train D loss: 0.4857, train F loss: -0.007848, acc 0.986600\n",
      "5.29s, epoch [380\\1500]: train D loss: 0.4863, train F loss: -0.016435, acc 0.990400\n",
      "5.29s, epoch [381\\1500]: train D loss: 0.4689, train F loss: -0.004069, acc 0.986400\n",
      "5.29s, epoch [382\\1500]: train D loss: 0.4900, train F loss: -0.002401, acc 0.984400\n",
      "5.28s, epoch [383\\1500]: train D loss: 0.4673, train F loss: -0.000894, acc 0.985600\n",
      "5.28s, epoch [384\\1500]: train D loss: 0.4942, train F loss: -0.010907, acc 0.988000\n",
      "5.29s, epoch [385\\1500]: train D loss: 0.4748, train F loss: -0.012394, acc 0.987200\n",
      "5.27s, epoch [386\\1500]: train D loss: 0.4755, train F loss: -0.001870, acc 0.987400\n",
      "5.28s, epoch [387\\1500]: train D loss: 0.4848, train F loss: -0.013869, acc 0.988400\n",
      "5.30s, epoch [388\\1500]: train D loss: 0.4675, train F loss: 0.001931, acc 0.983600\n",
      "5.29s, epoch [389\\1500]: train D loss: 0.4827, train F loss: -0.015475, acc 0.988200\n",
      "5.29s, epoch [390\\1500]: train D loss: 0.4830, train F loss: -0.004001, acc 0.987000\n",
      "5.29s, epoch [391\\1500]: train D loss: 0.4674, train F loss: -0.008559, acc 0.988400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.29s, epoch [392\\1500]: train D loss: 0.4588, train F loss: -0.016961, acc 0.992000\n",
      "5.28s, epoch [393\\1500]: train D loss: 0.4616, train F loss: -0.017881, acc 0.990000\n",
      "5.29s, epoch [394\\1500]: train D loss: 0.4745, train F loss: -0.008878, acc 0.988600\n",
      "5.28s, epoch [395\\1500]: train D loss: 0.4822, train F loss: -0.006250, acc 0.986200\n",
      "5.28s, epoch [396\\1500]: train D loss: 0.4571, train F loss: -0.010853, acc 0.988200\n",
      "5.28s, epoch [397\\1500]: train D loss: 0.4563, train F loss: -0.018130, acc 0.990200\n",
      "5.29s, epoch [398\\1500]: train D loss: 0.4768, train F loss: -0.015385, acc 0.991000\n",
      "5.30s, epoch [399\\1500]: train D loss: 0.4918, train F loss: 0.021676, acc 0.978200\n",
      "5.28s, epoch [400\\1500]: train D loss: 0.4830, train F loss: -0.012567, acc 0.990400\n",
      "5.28s, epoch [401\\1500]: train D loss: 0.4693, train F loss: -0.009581, acc 0.990000\n",
      "5.27s, epoch [402\\1500]: train D loss: 0.4846, train F loss: -0.008088, acc 0.986200\n",
      "5.27s, epoch [403\\1500]: train D loss: 0.4768, train F loss: -0.012917, acc 0.989600\n",
      "5.28s, epoch [404\\1500]: train D loss: 0.4687, train F loss: -0.003013, acc 0.987800\n",
      "5.48s, epoch [405\\1500]: train D loss: 0.4613, train F loss: -0.013011, acc 0.991200\n",
      "5.28s, epoch [406\\1500]: train D loss: 0.4565, train F loss: -0.011859, acc 0.989200\n",
      "5.27s, epoch [407\\1500]: train D loss: 0.4694, train F loss: -0.016197, acc 0.991400\n",
      "5.27s, epoch [408\\1500]: train D loss: 0.4683, train F loss: -0.010494, acc 0.988600\n",
      "5.27s, epoch [409\\1500]: train D loss: 0.4832, train F loss: -0.014046, acc 0.987600\n",
      "5.27s, epoch [410\\1500]: train D loss: 0.4861, train F loss: -0.012750, acc 0.988600\n",
      "5.27s, epoch [411\\1500]: train D loss: 0.4726, train F loss: -0.014218, acc 0.989400\n",
      "5.27s, epoch [412\\1500]: train D loss: 0.4760, train F loss: -0.001282, acc 0.987000\n",
      "5.27s, epoch [413\\1500]: train D loss: 0.4829, train F loss: 0.006958, acc 0.984200\n",
      "5.26s, epoch [414\\1500]: train D loss: 0.4690, train F loss: -0.016683, acc 0.989000\n",
      "5.26s, epoch [415\\1500]: train D loss: 0.4653, train F loss: -0.008070, acc 0.988000\n",
      "5.27s, epoch [416\\1500]: train D loss: 0.4612, train F loss: -0.017286, acc 0.990200\n",
      "5.27s, epoch [417\\1500]: train D loss: 0.4829, train F loss: -0.021010, acc 0.991000\n",
      "5.27s, epoch [418\\1500]: train D loss: 0.4707, train F loss: -0.020708, acc 0.992400\n",
      "5.26s, epoch [419\\1500]: train D loss: 0.4907, train F loss: -0.014869, acc 0.988600\n",
      "5.28s, epoch [420\\1500]: train D loss: 0.4898, train F loss: -0.001701, acc 0.987000\n",
      "5.29s, epoch [421\\1500]: train D loss: 0.4730, train F loss: -0.016148, acc 0.988800\n",
      "5.27s, epoch [422\\1500]: train D loss: 0.4613, train F loss: -0.010307, acc 0.989800\n",
      "5.27s, epoch [423\\1500]: train D loss: 0.4889, train F loss: 0.002447, acc 0.985000\n",
      "5.27s, epoch [424\\1500]: train D loss: 0.4753, train F loss: -0.010187, acc 0.987800\n",
      "5.27s, epoch [425\\1500]: train D loss: 0.4698, train F loss: -0.014317, acc 0.990200\n",
      "5.26s, epoch [426\\1500]: train D loss: 0.4828, train F loss: -0.021370, acc 0.991200\n",
      "5.26s, epoch [427\\1500]: train D loss: 0.5076, train F loss: -0.012911, acc 0.987600\n",
      "5.27s, epoch [428\\1500]: train D loss: 0.4639, train F loss: -0.009528, acc 0.988000\n",
      "5.26s, epoch [429\\1500]: train D loss: 0.4992, train F loss: -0.008043, acc 0.988200\n",
      "5.24s, epoch [430\\1500]: train D loss: 0.4927, train F loss: -0.019087, acc 0.990200\n",
      "5.24s, epoch [431\\1500]: train D loss: 0.4820, train F loss: -0.010095, acc 0.989200\n",
      "5.23s, epoch [432\\1500]: train D loss: 0.4925, train F loss: 0.013115, acc 0.980000\n",
      "5.25s, epoch [433\\1500]: train D loss: 0.4870, train F loss: -0.000302, acc 0.984000\n",
      "5.25s, epoch [434\\1500]: train D loss: 0.4691, train F loss: -0.015730, acc 0.991400\n",
      "5.23s, epoch [435\\1500]: train D loss: 0.4687, train F loss: -0.014791, acc 0.990800\n",
      "5.24s, epoch [436\\1500]: train D loss: 0.4746, train F loss: -0.017037, acc 0.990600\n",
      "5.25s, epoch [437\\1500]: train D loss: 0.4740, train F loss: -0.018036, acc 0.991600\n",
      "5.24s, epoch [438\\1500]: train D loss: 0.4911, train F loss: -0.013331, acc 0.988600\n",
      "5.24s, epoch [439\\1500]: train D loss: 0.4874, train F loss: -0.015696, acc 0.990200\n",
      "5.23s, epoch [440\\1500]: train D loss: 0.4773, train F loss: -0.003969, acc 0.988800\n",
      "5.28s, epoch [441\\1500]: train D loss: 0.4859, train F loss: -0.007155, acc 0.987200\n",
      "5.38s, epoch [442\\1500]: train D loss: 0.4842, train F loss: -0.010637, acc 0.988000\n",
      "5.33s, epoch [443\\1500]: train D loss: 0.4719, train F loss: -0.012436, acc 0.991000\n",
      "5.29s, epoch [444\\1500]: train D loss: 0.4931, train F loss: -0.004734, acc 0.985000\n",
      "5.31s, epoch [445\\1500]: train D loss: 0.4994, train F loss: -0.016680, acc 0.988600\n",
      "5.32s, epoch [446\\1500]: train D loss: 0.4742, train F loss: -0.015618, acc 0.990000\n",
      "5.46s, epoch [447\\1500]: train D loss: 0.4828, train F loss: -0.018714, acc 0.990400\n",
      "5.37s, epoch [448\\1500]: train D loss: 0.4739, train F loss: -0.011650, acc 0.990800\n",
      "5.37s, epoch [449\\1500]: train D loss: 0.4736, train F loss: -0.016446, acc 0.989400\n",
      "5.36s, epoch [450\\1500]: train D loss: 0.4964, train F loss: -0.011162, acc 0.989400\n",
      "5.36s, epoch [451\\1500]: train D loss: 0.4814, train F loss: -0.009110, acc 0.988600\n",
      "5.36s, epoch [452\\1500]: train D loss: 0.4906, train F loss: -0.018440, acc 0.990400\n",
      "5.36s, epoch [453\\1500]: train D loss: 0.4840, train F loss: -0.014003, acc 0.988800\n",
      "5.36s, epoch [454\\1500]: train D loss: 0.4882, train F loss: -0.018511, acc 0.990400\n",
      "5.37s, epoch [455\\1500]: train D loss: 0.4859, train F loss: -0.005955, acc 0.987600\n",
      "5.37s, epoch [456\\1500]: train D loss: 0.4670, train F loss: -0.021806, acc 0.992200\n",
      "5.36s, epoch [457\\1500]: train D loss: 0.4676, train F loss: -0.006953, acc 0.987800\n",
      "5.36s, epoch [458\\1500]: train D loss: 0.4870, train F loss: -0.021610, acc 0.991800\n",
      "5.36s, epoch [459\\1500]: train D loss: 0.4944, train F loss: -0.017506, acc 0.990000\n",
      "5.36s, epoch [460\\1500]: train D loss: 0.4761, train F loss: -0.005970, acc 0.987400\n",
      "5.37s, epoch [461\\1500]: train D loss: 0.4961, train F loss: 0.000747, acc 0.983800\n",
      "5.36s, epoch [462\\1500]: train D loss: 0.4904, train F loss: -0.011613, acc 0.989000\n",
      "5.36s, epoch [463\\1500]: train D loss: 0.4779, train F loss: -0.007124, acc 0.987800\n",
      "5.36s, epoch [464\\1500]: train D loss: 0.4916, train F loss: -0.017149, acc 0.990000\n",
      "5.36s, epoch [465\\1500]: train D loss: 0.4903, train F loss: -0.022154, acc 0.992000\n",
      "5.37s, epoch [466\\1500]: train D loss: 0.4851, train F loss: -0.009811, acc 0.987600\n",
      "5.36s, epoch [467\\1500]: train D loss: 0.4804, train F loss: -0.014784, acc 0.990400\n",
      "5.37s, epoch [468\\1500]: train D loss: 0.4892, train F loss: -0.013962, acc 0.989400\n",
      "5.36s, epoch [469\\1500]: train D loss: 0.4800, train F loss: -0.008732, acc 0.987200\n",
      "5.35s, epoch [470\\1500]: train D loss: 0.4833, train F loss: -0.021266, acc 0.990800\n",
      "5.36s, epoch [471\\1500]: train D loss: 0.4980, train F loss: -0.008325, acc 0.987000\n",
      "5.37s, epoch [472\\1500]: train D loss: 0.4841, train F loss: -0.011986, acc 0.988000\n",
      "5.36s, epoch [473\\1500]: train D loss: 0.4863, train F loss: -0.010959, acc 0.988400\n",
      "5.36s, epoch [474\\1500]: train D loss: 0.4872, train F loss: -0.019641, acc 0.990200\n",
      "5.37s, epoch [475\\1500]: train D loss: 0.4924, train F loss: -0.010833, acc 0.987000\n",
      "5.36s, epoch [476\\1500]: train D loss: 0.4642, train F loss: -0.012585, acc 0.990400\n",
      "5.36s, epoch [477\\1500]: train D loss: 0.4688, train F loss: -0.015186, acc 0.989800\n",
      "5.37s, epoch [478\\1500]: train D loss: 0.4852, train F loss: -0.023164, acc 0.992400\n",
      "5.36s, epoch [479\\1500]: train D loss: 0.4754, train F loss: -0.024069, acc 0.991600\n",
      "5.35s, epoch [480\\1500]: train D loss: 0.4828, train F loss: -0.009404, acc 0.989200\n",
      "5.36s, epoch [481\\1500]: train D loss: 0.4709, train F loss: -0.018461, acc 0.991200\n",
      "5.36s, epoch [482\\1500]: train D loss: 0.4706, train F loss: -0.019555, acc 0.990400\n",
      "5.36s, epoch [483\\1500]: train D loss: 0.4929, train F loss: -0.022578, acc 0.992200\n",
      "5.35s, epoch [484\\1500]: train D loss: 0.4897, train F loss: -0.011878, acc 0.990200\n",
      "5.36s, epoch [485\\1500]: train D loss: 0.4949, train F loss: -0.016925, acc 0.988200\n",
      "5.36s, epoch [486\\1500]: train D loss: 0.5182, train F loss: -0.006813, acc 0.986000\n",
      "5.36s, epoch [487\\1500]: train D loss: 0.4850, train F loss: -0.007206, acc 0.988200\n",
      "5.38s, epoch [488\\1500]: train D loss: 0.5068, train F loss: 0.021487, acc 0.979200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.36s, epoch [489\\1500]: train D loss: 0.4731, train F loss: -0.012729, acc 0.988600\n",
      "5.36s, epoch [490\\1500]: train D loss: 0.4673, train F loss: -0.015470, acc 0.991400\n",
      "5.37s, epoch [491\\1500]: train D loss: 0.4931, train F loss: -0.021865, acc 0.988800\n",
      "5.32s, epoch [492\\1500]: train D loss: 0.4693, train F loss: -0.017299, acc 0.990800\n",
      "5.27s, epoch [493\\1500]: train D loss: 0.5067, train F loss: -0.002545, acc 0.984200\n",
      "5.27s, epoch [494\\1500]: train D loss: 0.4891, train F loss: -0.011560, acc 0.986600\n",
      "5.27s, epoch [495\\1500]: train D loss: 0.4666, train F loss: -0.022928, acc 0.991600\n",
      "5.26s, epoch [496\\1500]: train D loss: 0.4629, train F loss: -0.022368, acc 0.992600\n",
      "5.27s, epoch [497\\1500]: train D loss: 0.4672, train F loss: -0.027265, acc 0.994000\n",
      "5.59s, epoch [498\\1500]: train D loss: 0.4710, train F loss: -0.000575, acc 0.986400\n",
      "5.33s, epoch [499\\1500]: train D loss: 0.4731, train F loss: -0.007037, acc 0.988800\n",
      "5.28s, epoch [500\\1500]: train D loss: 0.4787, train F loss: -0.016745, acc 0.990400\n",
      "5.28s, epoch [501\\1500]: train D loss: 0.4754, train F loss: -0.020965, acc 0.992000\n",
      "5.27s, epoch [502\\1500]: train D loss: 0.4827, train F loss: -0.012944, acc 0.990800\n",
      "5.28s, epoch [503\\1500]: train D loss: 0.4670, train F loss: -0.025228, acc 0.993600\n",
      "5.27s, epoch [504\\1500]: train D loss: 0.4863, train F loss: -0.005383, acc 0.987800\n",
      "5.27s, epoch [505\\1500]: train D loss: 0.4894, train F loss: -0.023780, acc 0.991800\n",
      "5.29s, epoch [506\\1500]: train D loss: 0.4918, train F loss: -0.007437, acc 0.987800\n",
      "5.27s, epoch [507\\1500]: train D loss: 0.4758, train F loss: -0.024543, acc 0.992800\n",
      "5.27s, epoch [508\\1500]: train D loss: 0.4912, train F loss: -0.020924, acc 0.992000\n",
      "5.28s, epoch [509\\1500]: train D loss: 0.4826, train F loss: -0.009319, acc 0.987600\n",
      "5.27s, epoch [510\\1500]: train D loss: 0.4965, train F loss: -0.016108, acc 0.990400\n",
      "5.27s, epoch [511\\1500]: train D loss: 0.4858, train F loss: -0.021637, acc 0.991600\n",
      "5.27s, epoch [512\\1500]: train D loss: 0.4902, train F loss: -0.019145, acc 0.990800\n",
      "5.27s, epoch [513\\1500]: train D loss: 0.4833, train F loss: -0.013471, acc 0.988400\n",
      "5.28s, epoch [514\\1500]: train D loss: 0.4991, train F loss: -0.019939, acc 0.989600\n",
      "5.28s, epoch [515\\1500]: train D loss: 0.4991, train F loss: -0.004939, acc 0.988000\n",
      "5.27s, epoch [516\\1500]: train D loss: 0.4818, train F loss: -0.023768, acc 0.991200\n",
      "5.26s, epoch [517\\1500]: train D loss: 0.4983, train F loss: -0.004956, acc 0.988200\n",
      "5.26s, epoch [518\\1500]: train D loss: 0.4988, train F loss: -0.021620, acc 0.991000\n",
      "5.27s, epoch [519\\1500]: train D loss: 0.4955, train F loss: -0.000670, acc 0.987000\n",
      "5.27s, epoch [520\\1500]: train D loss: 0.4802, train F loss: -0.007229, acc 0.988800\n",
      "5.28s, epoch [521\\1500]: train D loss: 0.5047, train F loss: -0.000518, acc 0.986200\n",
      "5.28s, epoch [522\\1500]: train D loss: 0.4792, train F loss: -0.021603, acc 0.991600\n",
      "5.27s, epoch [523\\1500]: train D loss: 0.4760, train F loss: -0.023135, acc 0.992600\n",
      "5.27s, epoch [524\\1500]: train D loss: 0.4846, train F loss: -0.015818, acc 0.989000\n",
      "5.27s, epoch [525\\1500]: train D loss: 0.4934, train F loss: -0.008352, acc 0.987400\n",
      "5.28s, epoch [526\\1500]: train D loss: 0.4650, train F loss: -0.022077, acc 0.991400\n",
      "5.26s, epoch [527\\1500]: train D loss: 0.4845, train F loss: -0.022318, acc 0.992000\n",
      "5.29s, epoch [528\\1500]: train D loss: 0.4666, train F loss: -0.024822, acc 0.993200\n",
      "5.24s, epoch [529\\1500]: train D loss: 0.4736, train F loss: -0.022689, acc 0.992200\n",
      "5.24s, epoch [530\\1500]: train D loss: 0.4801, train F loss: -0.019602, acc 0.992800\n",
      "5.24s, epoch [531\\1500]: train D loss: 0.4797, train F loss: -0.018427, acc 0.991600\n",
      "5.25s, epoch [532\\1500]: train D loss: 0.4852, train F loss: -0.024184, acc 0.992800\n",
      "5.23s, epoch [533\\1500]: train D loss: 0.4972, train F loss: -0.013986, acc 0.988800\n",
      "5.24s, epoch [534\\1500]: train D loss: 0.4985, train F loss: -0.019284, acc 0.990800\n",
      "5.23s, epoch [535\\1500]: train D loss: 0.4891, train F loss: -0.014381, acc 0.989200\n",
      "5.31s, epoch [536\\1500]: train D loss: 0.4793, train F loss: -0.020775, acc 0.991400\n",
      "5.22s, epoch [537\\1500]: train D loss: 0.4814, train F loss: -0.021501, acc 0.991400\n",
      "5.24s, epoch [538\\1500]: train D loss: 0.4832, train F loss: 0.000049, acc 0.989000\n",
      "5.23s, epoch [539\\1500]: train D loss: 0.4917, train F loss: -0.021383, acc 0.991600\n",
      "5.24s, epoch [540\\1500]: train D loss: 0.4700, train F loss: -0.017534, acc 0.989800\n",
      "5.24s, epoch [541\\1500]: train D loss: 0.4840, train F loss: -0.006527, acc 0.990800\n",
      "5.24s, epoch [542\\1500]: train D loss: 0.5014, train F loss: 0.012558, acc 0.981600\n",
      "5.24s, epoch [543\\1500]: train D loss: 0.4958, train F loss: -0.011834, acc 0.987800\n",
      "5.24s, epoch [544\\1500]: train D loss: 0.4770, train F loss: -0.017526, acc 0.990400\n",
      "5.24s, epoch [545\\1500]: train D loss: 0.4876, train F loss: -0.022313, acc 0.992200\n",
      "5.24s, epoch [546\\1500]: train D loss: 0.5002, train F loss: -0.020020, acc 0.990000\n",
      "5.23s, epoch [547\\1500]: train D loss: 0.4877, train F loss: -0.021665, acc 0.991600\n",
      "5.23s, epoch [548\\1500]: train D loss: 0.4846, train F loss: -0.020953, acc 0.991200\n",
      "5.24s, epoch [549\\1500]: train D loss: 0.4965, train F loss: -0.017831, acc 0.990200\n",
      "5.23s, epoch [550\\1500]: train D loss: 0.4872, train F loss: -0.025924, acc 0.992000\n",
      "5.26s, epoch [551\\1500]: train D loss: 0.4999, train F loss: -0.016155, acc 0.991200\n",
      "5.24s, epoch [552\\1500]: train D loss: 0.4820, train F loss: -0.022814, acc 0.990800\n",
      "5.23s, epoch [553\\1500]: train D loss: 0.4892, train F loss: -0.016233, acc 0.990200\n",
      "5.23s, epoch [554\\1500]: train D loss: 0.4838, train F loss: -0.012981, acc 0.990600\n",
      "5.24s, epoch [555\\1500]: train D loss: 0.4951, train F loss: -0.015537, acc 0.990800\n",
      "5.25s, epoch [556\\1500]: train D loss: 0.5044, train F loss: -0.006665, acc 0.983000\n",
      "5.24s, epoch [557\\1500]: train D loss: 0.4911, train F loss: -0.019496, acc 0.992000\n",
      "5.23s, epoch [558\\1500]: train D loss: 0.4834, train F loss: -0.013310, acc 0.991800\n",
      "5.24s, epoch [559\\1500]: train D loss: 0.4957, train F loss: -0.017480, acc 0.990200\n",
      "5.24s, epoch [560\\1500]: train D loss: 0.4993, train F loss: -0.027965, acc 0.992400\n",
      "5.24s, epoch [561\\1500]: train D loss: 0.4947, train F loss: -0.015781, acc 0.989000\n",
      "5.23s, epoch [562\\1500]: train D loss: 0.4899, train F loss: -0.022069, acc 0.991400\n",
      "5.25s, epoch [563\\1500]: train D loss: 0.5028, train F loss: -0.007922, acc 0.985800\n",
      "5.23s, epoch [564\\1500]: train D loss: 0.4895, train F loss: -0.027307, acc 0.993000\n",
      "5.25s, epoch [565\\1500]: train D loss: 0.4861, train F loss: -0.021300, acc 0.991600\n",
      "5.24s, epoch [566\\1500]: train D loss: 0.4902, train F loss: -0.017039, acc 0.989800\n",
      "5.24s, epoch [567\\1500]: train D loss: 0.4951, train F loss: -0.024200, acc 0.992000\n",
      "5.24s, epoch [568\\1500]: train D loss: 0.4981, train F loss: -0.013806, acc 0.989600\n",
      "5.25s, epoch [569\\1500]: train D loss: 0.4873, train F loss: -0.020304, acc 0.992200\n",
      "5.23s, epoch [570\\1500]: train D loss: 0.4841, train F loss: -0.011243, acc 0.988200\n",
      "5.24s, epoch [571\\1500]: train D loss: 0.5038, train F loss: -0.018546, acc 0.991200\n",
      "5.23s, epoch [572\\1500]: train D loss: 0.4897, train F loss: -0.018280, acc 0.989800\n",
      "5.24s, epoch [573\\1500]: train D loss: 0.4893, train F loss: -0.025697, acc 0.992200\n",
      "5.25s, epoch [574\\1500]: train D loss: 0.4873, train F loss: -0.021472, acc 0.990800\n",
      "5.24s, epoch [575\\1500]: train D loss: 0.5107, train F loss: -0.011964, acc 0.986600\n",
      "5.24s, epoch [576\\1500]: train D loss: 0.5053, train F loss: -0.016687, acc 0.988200\n",
      "5.24s, epoch [577\\1500]: train D loss: 0.4987, train F loss: -0.020726, acc 0.990200\n",
      "5.24s, epoch [578\\1500]: train D loss: 0.4870, train F loss: -0.027098, acc 0.993000\n",
      "5.25s, epoch [579\\1500]: train D loss: 0.4901, train F loss: -0.023368, acc 0.992200\n",
      "5.24s, epoch [580\\1500]: train D loss: 0.4916, train F loss: -0.027222, acc 0.992600\n",
      "5.24s, epoch [581\\1500]: train D loss: 0.4798, train F loss: -0.017720, acc 0.989600\n",
      "5.23s, epoch [582\\1500]: train D loss: 0.4850, train F loss: -0.023170, acc 0.992400\n",
      "5.23s, epoch [583\\1500]: train D loss: 0.5063, train F loss: -0.019297, acc 0.991400\n",
      "5.23s, epoch [584\\1500]: train D loss: 0.4775, train F loss: -0.025550, acc 0.991400\n",
      "5.24s, epoch [585\\1500]: train D loss: 0.4805, train F loss: -0.011251, acc 0.988800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.24s, epoch [586\\1500]: train D loss: 0.5108, train F loss: -0.016745, acc 0.990800\n",
      "5.24s, epoch [587\\1500]: train D loss: 0.4939, train F loss: -0.028695, acc 0.992600\n",
      "5.24s, epoch [588\\1500]: train D loss: 0.4973, train F loss: -0.021261, acc 0.991200\n",
      "5.23s, epoch [589\\1500]: train D loss: 0.4940, train F loss: -0.020806, acc 0.990400\n",
      "5.25s, epoch [590\\1500]: train D loss: 0.4897, train F loss: -0.025263, acc 0.993000\n",
      "5.23s, epoch [591\\1500]: train D loss: 0.5136, train F loss: 0.035266, acc 0.976800\n",
      "5.23s, epoch [592\\1500]: train D loss: 0.4814, train F loss: -0.025632, acc 0.993200\n",
      "5.24s, epoch [593\\1500]: train D loss: 0.4911, train F loss: -0.021348, acc 0.991800\n",
      "5.25s, epoch [594\\1500]: train D loss: 0.4949, train F loss: -0.015876, acc 0.990000\n",
      "5.23s, epoch [595\\1500]: train D loss: 0.4823, train F loss: -0.019856, acc 0.990400\n",
      "5.24s, epoch [596\\1500]: train D loss: 0.4948, train F loss: -0.021294, acc 0.991800\n",
      "5.25s, epoch [597\\1500]: train D loss: 0.4993, train F loss: -0.016626, acc 0.990000\n",
      "5.23s, epoch [598\\1500]: train D loss: 0.4772, train F loss: -0.017949, acc 0.993800\n",
      "5.23s, epoch [599\\1500]: train D loss: 0.5168, train F loss: 0.013196, acc 0.980800\n",
      "5.24s, epoch [600\\1500]: train D loss: 0.5036, train F loss: -0.022291, acc 0.991000\n",
      "5.24s, epoch [601\\1500]: train D loss: 0.4771, train F loss: -0.017300, acc 0.992200\n",
      "5.25s, epoch [602\\1500]: train D loss: 0.4924, train F loss: -0.026827, acc 0.993000\n",
      "5.25s, epoch [603\\1500]: train D loss: 0.5074, train F loss: 0.006771, acc 0.981800\n",
      "5.24s, epoch [604\\1500]: train D loss: 0.4887, train F loss: -0.019791, acc 0.990400\n",
      "5.28s, epoch [605\\1500]: train D loss: 0.4874, train F loss: -0.028810, acc 0.993400\n",
      "5.25s, epoch [606\\1500]: train D loss: 0.4896, train F loss: -0.021260, acc 0.990400\n",
      "5.25s, epoch [607\\1500]: train D loss: 0.4925, train F loss: -0.021858, acc 0.991400\n",
      "5.25s, epoch [608\\1500]: train D loss: 0.5026, train F loss: -0.019740, acc 0.989000\n",
      "5.25s, epoch [609\\1500]: train D loss: 0.4918, train F loss: -0.026554, acc 0.992400\n",
      "5.25s, epoch [610\\1500]: train D loss: 0.4797, train F loss: -0.026452, acc 0.993600\n",
      "5.25s, epoch [611\\1500]: train D loss: 0.4941, train F loss: -0.026409, acc 0.991600\n",
      "5.25s, epoch [612\\1500]: train D loss: 0.4861, train F loss: -0.034493, acc 0.995000\n",
      "5.25s, epoch [613\\1500]: train D loss: 0.4959, train F loss: -0.021553, acc 0.991800\n",
      "5.24s, epoch [614\\1500]: train D loss: 0.5012, train F loss: -0.023653, acc 0.992600\n",
      "5.26s, epoch [615\\1500]: train D loss: 0.5022, train F loss: -0.028443, acc 0.994400\n",
      "5.24s, epoch [616\\1500]: train D loss: 0.5120, train F loss: -0.011082, acc 0.989800\n",
      "5.25s, epoch [617\\1500]: train D loss: 0.4944, train F loss: -0.012542, acc 0.990200\n",
      "5.25s, epoch [618\\1500]: train D loss: 0.4848, train F loss: -0.025922, acc 0.992600\n",
      "5.25s, epoch [619\\1500]: train D loss: 0.4853, train F loss: -0.025790, acc 0.992400\n",
      "5.25s, epoch [620\\1500]: train D loss: 0.4956, train F loss: -0.026445, acc 0.992000\n",
      "5.24s, epoch [621\\1500]: train D loss: 0.4900, train F loss: -0.014867, acc 0.991800\n",
      "5.25s, epoch [622\\1500]: train D loss: 0.4850, train F loss: -0.014960, acc 0.989400\n",
      "5.24s, epoch [623\\1500]: train D loss: 0.5035, train F loss: -0.026704, acc 0.992800\n",
      "5.25s, epoch [624\\1500]: train D loss: 0.5026, train F loss: -0.018834, acc 0.989000\n",
      "5.24s, epoch [625\\1500]: train D loss: 0.5045, train F loss: -0.019272, acc 0.991000\n",
      "5.25s, epoch [626\\1500]: train D loss: 0.4951, train F loss: -0.025725, acc 0.993600\n",
      "5.25s, epoch [627\\1500]: train D loss: 0.4970, train F loss: -0.018804, acc 0.990400\n",
      "5.25s, epoch [628\\1500]: train D loss: 0.5045, train F loss: -0.017825, acc 0.988200\n",
      "5.26s, epoch [629\\1500]: train D loss: 0.5063, train F loss: -0.019297, acc 0.990800\n",
      "5.25s, epoch [630\\1500]: train D loss: 0.5069, train F loss: -0.021535, acc 0.991400\n",
      "5.25s, epoch [631\\1500]: train D loss: 0.5091, train F loss: -0.026420, acc 0.991000\n",
      "5.26s, epoch [632\\1500]: train D loss: 0.5003, train F loss: -0.022540, acc 0.992200\n",
      "5.25s, epoch [633\\1500]: train D loss: 0.4917, train F loss: -0.033143, acc 0.995200\n",
      "5.24s, epoch [634\\1500]: train D loss: 0.5271, train F loss: -0.019212, acc 0.989800\n",
      "5.24s, epoch [635\\1500]: train D loss: 0.5184, train F loss: -0.020692, acc 0.991400\n",
      "5.25s, epoch [636\\1500]: train D loss: 0.5081, train F loss: -0.018931, acc 0.990400\n",
      "5.24s, epoch [637\\1500]: train D loss: 0.5093, train F loss: -0.019891, acc 0.990800\n",
      "5.26s, epoch [638\\1500]: train D loss: 0.5119, train F loss: -0.031630, acc 0.994000\n",
      "5.25s, epoch [639\\1500]: train D loss: 0.5269, train F loss: -0.012526, acc 0.988000\n",
      "5.27s, epoch [640\\1500]: train D loss: 0.5143, train F loss: -0.014674, acc 0.989000\n",
      "5.24s, epoch [641\\1500]: train D loss: 0.5110, train F loss: -0.022915, acc 0.990600\n",
      "5.25s, epoch [642\\1500]: train D loss: 0.5107, train F loss: -0.014063, acc 0.989000\n",
      "5.25s, epoch [643\\1500]: train D loss: 0.5084, train F loss: -0.018862, acc 0.989400\n",
      "5.25s, epoch [644\\1500]: train D loss: 0.4918, train F loss: -0.025987, acc 0.992200\n",
      "5.25s, epoch [645\\1500]: train D loss: 0.5014, train F loss: -0.020346, acc 0.990600\n",
      "5.25s, epoch [646\\1500]: train D loss: 0.5043, train F loss: -0.021568, acc 0.990800\n",
      "5.25s, epoch [647\\1500]: train D loss: 0.5028, train F loss: -0.014287, acc 0.989400\n",
      "5.26s, epoch [648\\1500]: train D loss: 0.5056, train F loss: -0.005450, acc 0.988200\n",
      "5.25s, epoch [649\\1500]: train D loss: 0.5036, train F loss: -0.024721, acc 0.991400\n",
      "5.25s, epoch [650\\1500]: train D loss: 0.5039, train F loss: -0.025925, acc 0.992400\n",
      "5.23s, epoch [651\\1500]: train D loss: 0.4935, train F loss: -0.025727, acc 0.992600\n",
      "5.23s, epoch [652\\1500]: train D loss: 0.5041, train F loss: -0.024527, acc 0.992600\n",
      "5.24s, epoch [653\\1500]: train D loss: 0.5015, train F loss: -0.023411, acc 0.992000\n",
      "5.23s, epoch [654\\1500]: train D loss: 0.4886, train F loss: -0.027562, acc 0.991800\n",
      "5.25s, epoch [655\\1500]: train D loss: 0.4983, train F loss: -0.019618, acc 0.990600\n",
      "5.24s, epoch [656\\1500]: train D loss: 0.5240, train F loss: -0.021277, acc 0.990800\n",
      "5.22s, epoch [657\\1500]: train D loss: 0.4999, train F loss: -0.022768, acc 0.991400\n",
      "5.22s, epoch [658\\1500]: train D loss: 0.5047, train F loss: -0.018704, acc 0.990600\n",
      "5.22s, epoch [659\\1500]: train D loss: 0.5039, train F loss: -0.017238, acc 0.989800\n",
      "5.22s, epoch [660\\1500]: train D loss: 0.5187, train F loss: 0.014961, acc 0.985200\n",
      "5.24s, epoch [661\\1500]: train D loss: 0.5074, train F loss: -0.021089, acc 0.991000\n",
      "5.23s, epoch [662\\1500]: train D loss: 0.5160, train F loss: -0.020025, acc 0.991000\n",
      "5.23s, epoch [663\\1500]: train D loss: 0.5238, train F loss: 0.007327, acc 0.985000\n",
      "5.23s, epoch [664\\1500]: train D loss: 0.4970, train F loss: -0.017644, acc 0.989600\n",
      "5.22s, epoch [665\\1500]: train D loss: 0.5096, train F loss: -0.011593, acc 0.987400\n",
      "5.22s, epoch [666\\1500]: train D loss: 0.5019, train F loss: -0.021485, acc 0.991800\n",
      "5.23s, epoch [667\\1500]: train D loss: 0.4991, train F loss: -0.030101, acc 0.994200\n",
      "5.23s, epoch [668\\1500]: train D loss: 0.5224, train F loss: -0.008280, acc 0.989600\n",
      "5.22s, epoch [669\\1500]: train D loss: 0.4975, train F loss: -0.028372, acc 0.993400\n",
      "5.22s, epoch [670\\1500]: train D loss: 0.4866, train F loss: -0.024638, acc 0.993800\n",
      "5.22s, epoch [671\\1500]: train D loss: 0.4998, train F loss: -0.022832, acc 0.991000\n",
      "5.23s, epoch [672\\1500]: train D loss: 0.5059, train F loss: -0.027896, acc 0.991200\n",
      "5.23s, epoch [673\\1500]: train D loss: 0.5051, train F loss: -0.020686, acc 0.991600\n",
      "5.22s, epoch [674\\1500]: train D loss: 0.5109, train F loss: -0.023755, acc 0.991800\n",
      "5.23s, epoch [675\\1500]: train D loss: 0.4961, train F loss: -0.028821, acc 0.994200\n",
      "5.22s, epoch [676\\1500]: train D loss: 0.4938, train F loss: -0.024394, acc 0.991800\n",
      "5.23s, epoch [677\\1500]: train D loss: 0.4935, train F loss: -0.030142, acc 0.994200\n",
      "5.24s, epoch [678\\1500]: train D loss: 0.4963, train F loss: -0.026442, acc 0.992800\n",
      "5.23s, epoch [679\\1500]: train D loss: 0.4998, train F loss: -0.022343, acc 0.992600\n",
      "5.22s, epoch [680\\1500]: train D loss: 0.4960, train F loss: -0.023503, acc 0.992800\n",
      "5.22s, epoch [681\\1500]: train D loss: 0.4902, train F loss: -0.021576, acc 0.993000\n",
      "5.23s, epoch [682\\1500]: train D loss: 0.5117, train F loss: -0.010975, acc 0.987400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.23s, epoch [683\\1500]: train D loss: 0.5089, train F loss: -0.022102, acc 0.990600\n",
      "5.25s, epoch [684\\1500]: train D loss: 0.4940, train F loss: -0.019578, acc 0.989600\n",
      "5.23s, epoch [685\\1500]: train D loss: 0.4899, train F loss: -0.021297, acc 0.990600\n",
      "5.24s, epoch [686\\1500]: train D loss: 0.5012, train F loss: -0.022954, acc 0.991400\n",
      "5.24s, epoch [687\\1500]: train D loss: 0.4997, train F loss: -0.021038, acc 0.991400\n",
      "5.23s, epoch [688\\1500]: train D loss: 0.5025, train F loss: -0.026829, acc 0.991000\n",
      "5.23s, epoch [689\\1500]: train D loss: 0.5008, train F loss: -0.028648, acc 0.994800\n",
      "5.22s, epoch [690\\1500]: train D loss: 0.5039, train F loss: -0.026495, acc 0.992400\n",
      "5.22s, epoch [691\\1500]: train D loss: 0.5011, train F loss: -0.020413, acc 0.991600\n",
      "5.23s, epoch [692\\1500]: train D loss: 0.5232, train F loss: 0.002130, acc 0.985800\n",
      "5.24s, epoch [693\\1500]: train D loss: 0.5039, train F loss: -0.028878, acc 0.992800\n",
      "5.24s, epoch [694\\1500]: train D loss: 0.5176, train F loss: -0.027450, acc 0.993200\n",
      "5.24s, epoch [695\\1500]: train D loss: 0.4945, train F loss: -0.024640, acc 0.991800\n",
      "5.24s, epoch [696\\1500]: train D loss: 0.5004, train F loss: -0.022213, acc 0.992000\n",
      "5.25s, epoch [697\\1500]: train D loss: 0.4982, train F loss: -0.019784, acc 0.990200\n",
      "5.25s, epoch [698\\1500]: train D loss: 0.4956, train F loss: -0.025536, acc 0.991400\n",
      "5.25s, epoch [699\\1500]: train D loss: 0.4949, train F loss: -0.031926, acc 0.994600\n",
      "5.25s, epoch [700\\1500]: train D loss: 0.4970, train F loss: -0.016441, acc 0.991800\n",
      "5.25s, epoch [701\\1500]: train D loss: 0.5059, train F loss: -0.028794, acc 0.992400\n",
      "5.27s, epoch [702\\1500]: train D loss: 0.5043, train F loss: -0.029489, acc 0.993600\n",
      "5.25s, epoch [703\\1500]: train D loss: 0.5208, train F loss: -0.030173, acc 0.993000\n",
      "5.26s, epoch [704\\1500]: train D loss: 0.5154, train F loss: -0.023551, acc 0.991000\n",
      "5.24s, epoch [705\\1500]: train D loss: 0.5150, train F loss: -0.026099, acc 0.992600\n",
      "5.25s, epoch [706\\1500]: train D loss: 0.5145, train F loss: -0.028089, acc 0.993000\n",
      "5.25s, epoch [707\\1500]: train D loss: 0.5004, train F loss: -0.024411, acc 0.992800\n",
      "5.25s, epoch [708\\1500]: train D loss: 0.5125, train F loss: -0.026060, acc 0.991000\n",
      "5.24s, epoch [709\\1500]: train D loss: 0.5230, train F loss: -0.031057, acc 0.994200\n",
      "5.25s, epoch [710\\1500]: train D loss: 0.5030, train F loss: -0.024709, acc 0.992400\n",
      "5.25s, epoch [711\\1500]: train D loss: 0.5295, train F loss: -0.029298, acc 0.992800\n",
      "5.25s, epoch [712\\1500]: train D loss: 0.5156, train F loss: -0.028299, acc 0.992400\n",
      "5.25s, epoch [713\\1500]: train D loss: 0.5113, train F loss: -0.024745, acc 0.991800\n",
      "5.25s, epoch [714\\1500]: train D loss: 0.5094, train F loss: -0.020324, acc 0.990200\n",
      "5.25s, epoch [715\\1500]: train D loss: 0.5245, train F loss: -0.024118, acc 0.991000\n",
      "5.25s, epoch [716\\1500]: train D loss: 0.5069, train F loss: -0.019718, acc 0.990400\n",
      "5.26s, epoch [717\\1500]: train D loss: 0.5155, train F loss: -0.019305, acc 0.989800\n",
      "5.25s, epoch [718\\1500]: train D loss: 0.5125, train F loss: -0.031999, acc 0.993400\n",
      "5.25s, epoch [719\\1500]: train D loss: 0.5128, train F loss: -0.025675, acc 0.992400\n",
      "5.27s, epoch [720\\1500]: train D loss: 0.5150, train F loss: -0.033528, acc 0.994800\n",
      "5.25s, epoch [721\\1500]: train D loss: 0.4963, train F loss: -0.028779, acc 0.993400\n",
      "5.25s, epoch [722\\1500]: train D loss: 0.5146, train F loss: -0.025843, acc 0.991400\n",
      "5.25s, epoch [723\\1500]: train D loss: 0.5048, train F loss: -0.024518, acc 0.992800\n",
      "5.26s, epoch [724\\1500]: train D loss: 0.4820, train F loss: -0.029187, acc 0.992800\n",
      "5.26s, epoch [725\\1500]: train D loss: 0.5109, train F loss: -0.024895, acc 0.992000\n",
      "5.24s, epoch [726\\1500]: train D loss: 0.5167, train F loss: -0.028899, acc 0.994200\n",
      "5.26s, epoch [727\\1500]: train D loss: 0.5219, train F loss: -0.022894, acc 0.990000\n",
      "5.25s, epoch [728\\1500]: train D loss: 0.5263, train F loss: -0.024052, acc 0.991000\n",
      "5.25s, epoch [729\\1500]: train D loss: 0.5150, train F loss: -0.025718, acc 0.993400\n",
      "5.26s, epoch [730\\1500]: train D loss: 0.5020, train F loss: -0.021780, acc 0.991200\n",
      "5.25s, epoch [731\\1500]: train D loss: 0.5084, train F loss: -0.026485, acc 0.991800\n",
      "5.25s, epoch [732\\1500]: train D loss: 0.5111, train F loss: -0.022791, acc 0.990200\n",
      "5.25s, epoch [733\\1500]: train D loss: 0.5238, train F loss: -0.015307, acc 0.989200\n",
      "5.25s, epoch [734\\1500]: train D loss: 0.5028, train F loss: -0.027861, acc 0.992600\n",
      "5.25s, epoch [735\\1500]: train D loss: 0.5162, train F loss: -0.026504, acc 0.991800\n",
      "5.25s, epoch [736\\1500]: train D loss: 0.5355, train F loss: -0.026827, acc 0.992400\n",
      "5.25s, epoch [737\\1500]: train D loss: 0.5214, train F loss: -0.028664, acc 0.992200\n",
      "5.25s, epoch [738\\1500]: train D loss: 0.5281, train F loss: -0.027392, acc 0.991600\n",
      "5.25s, epoch [739\\1500]: train D loss: 0.5106, train F loss: -0.027467, acc 0.992400\n",
      "5.26s, epoch [740\\1500]: train D loss: 0.4967, train F loss: -0.024883, acc 0.991400\n",
      "5.25s, epoch [741\\1500]: train D loss: 0.5047, train F loss: -0.022674, acc 0.991800\n",
      "5.25s, epoch [742\\1500]: train D loss: 0.5085, train F loss: -0.029412, acc 0.992800\n",
      "5.25s, epoch [743\\1500]: train D loss: 0.5156, train F loss: -0.033924, acc 0.994600\n",
      "5.25s, epoch [744\\1500]: train D loss: 0.5141, train F loss: -0.010699, acc 0.988400\n",
      "5.25s, epoch [745\\1500]: train D loss: 0.5057, train F loss: -0.025839, acc 0.992600\n",
      "5.24s, epoch [746\\1500]: train D loss: 0.4895, train F loss: -0.027823, acc 0.991800\n",
      "5.27s, epoch [747\\1500]: train D loss: 0.5088, train F loss: -0.024575, acc 0.991400\n",
      "5.26s, epoch [748\\1500]: train D loss: 0.5078, train F loss: -0.024023, acc 0.990600\n",
      "5.25s, epoch [749\\1500]: train D loss: 0.5055, train F loss: -0.025486, acc 0.992200\n",
      "5.26s, epoch [750\\1500]: train D loss: 0.5121, train F loss: -0.024412, acc 0.992200\n",
      "5.26s, epoch [751\\1500]: train D loss: 0.5111, train F loss: -0.005738, acc 0.987400\n",
      "5.25s, epoch [752\\1500]: train D loss: 0.5135, train F loss: -0.031574, acc 0.994000\n",
      "5.26s, epoch [753\\1500]: train D loss: 0.4985, train F loss: -0.027234, acc 0.993400\n",
      "5.25s, epoch [754\\1500]: train D loss: 0.5025, train F loss: -0.019126, acc 0.991000\n",
      "5.26s, epoch [755\\1500]: train D loss: 0.5052, train F loss: -0.024711, acc 0.991600\n",
      "5.24s, epoch [756\\1500]: train D loss: 0.5189, train F loss: -0.029816, acc 0.991800\n",
      "5.25s, epoch [757\\1500]: train D loss: 0.5107, train F loss: -0.021797, acc 0.991000\n",
      "5.24s, epoch [758\\1500]: train D loss: 0.5092, train F loss: -0.010189, acc 0.987600\n",
      "5.25s, epoch [759\\1500]: train D loss: 0.5046, train F loss: -0.022169, acc 0.991400\n",
      "5.25s, epoch [760\\1500]: train D loss: 0.4950, train F loss: -0.025923, acc 0.992600\n",
      "5.25s, epoch [761\\1500]: train D loss: 0.4877, train F loss: -0.031514, acc 0.994200\n",
      "5.25s, epoch [762\\1500]: train D loss: 0.5002, train F loss: -0.027114, acc 0.993200\n",
      "5.24s, epoch [763\\1500]: train D loss: 0.5050, train F loss: -0.023071, acc 0.992000\n",
      "5.25s, epoch [764\\1500]: train D loss: 0.5200, train F loss: -0.027933, acc 0.992000\n",
      "5.26s, epoch [765\\1500]: train D loss: 0.5125, train F loss: -0.033877, acc 0.994400\n",
      "5.25s, epoch [766\\1500]: train D loss: 0.4876, train F loss: -0.030621, acc 0.994000\n",
      "5.26s, epoch [767\\1500]: train D loss: 0.5168, train F loss: -0.024187, acc 0.992600\n",
      "5.25s, epoch [768\\1500]: train D loss: 0.5161, train F loss: -0.033121, acc 0.994000\n",
      "5.25s, epoch [769\\1500]: train D loss: 0.5039, train F loss: -0.024637, acc 0.991400\n",
      "5.26s, epoch [770\\1500]: train D loss: 0.5177, train F loss: -0.020725, acc 0.990200\n",
      "5.30s, epoch [771\\1500]: train D loss: 0.5227, train F loss: -0.032110, acc 0.992000\n",
      "5.28s, epoch [772\\1500]: train D loss: 0.5280, train F loss: -0.022172, acc 0.988600\n",
      "5.26s, epoch [773\\1500]: train D loss: 0.4998, train F loss: -0.019484, acc 0.989600\n",
      "5.24s, epoch [774\\1500]: train D loss: 0.4999, train F loss: -0.029967, acc 0.992400\n",
      "5.24s, epoch [775\\1500]: train D loss: 0.5061, train F loss: -0.025893, acc 0.993600\n",
      "5.24s, epoch [776\\1500]: train D loss: 0.5264, train F loss: -0.006584, acc 0.985600\n",
      "5.25s, epoch [777\\1500]: train D loss: 0.5135, train F loss: -0.019634, acc 0.991600\n",
      "5.24s, epoch [778\\1500]: train D loss: 0.5127, train F loss: -0.029509, acc 0.993000\n",
      "5.24s, epoch [779\\1500]: train D loss: 0.5195, train F loss: -0.024252, acc 0.991600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.24s, epoch [780\\1500]: train D loss: 0.5132, train F loss: -0.025073, acc 0.991400\n",
      "5.24s, epoch [781\\1500]: train D loss: 0.5180, train F loss: -0.023732, acc 0.990800\n",
      "5.24s, epoch [782\\1500]: train D loss: 0.5109, train F loss: -0.032665, acc 0.993000\n",
      "5.25s, epoch [783\\1500]: train D loss: 0.5107, train F loss: -0.027011, acc 0.992200\n",
      "5.25s, epoch [784\\1500]: train D loss: 0.4971, train F loss: -0.038187, acc 0.996600\n",
      "5.25s, epoch [785\\1500]: train D loss: 0.5039, train F loss: -0.029494, acc 0.993000\n",
      "5.26s, epoch [786\\1500]: train D loss: 0.5185, train F loss: -0.032532, acc 0.994000\n",
      "5.26s, epoch [787\\1500]: train D loss: 0.5140, train F loss: -0.030116, acc 0.993600\n",
      "5.25s, epoch [788\\1500]: train D loss: 0.5219, train F loss: -0.029123, acc 0.993200\n",
      "5.27s, epoch [789\\1500]: train D loss: 0.5281, train F loss: -0.022815, acc 0.988800\n",
      "5.27s, epoch [790\\1500]: train D loss: 0.5246, train F loss: -0.025436, acc 0.991600\n",
      "5.27s, epoch [791\\1500]: train D loss: 0.5060, train F loss: -0.029498, acc 0.992800\n",
      "5.27s, epoch [792\\1500]: train D loss: 0.5153, train F loss: -0.015491, acc 0.990800\n",
      "5.28s, epoch [793\\1500]: train D loss: 0.5061, train F loss: -0.033247, acc 0.994000\n",
      "5.26s, epoch [794\\1500]: train D loss: 0.5202, train F loss: -0.029195, acc 0.992600\n",
      "5.26s, epoch [795\\1500]: train D loss: 0.5147, train F loss: -0.034854, acc 0.994200\n",
      "5.27s, epoch [796\\1500]: train D loss: 0.5057, train F loss: -0.033447, acc 0.996000\n",
      "5.26s, epoch [797\\1500]: train D loss: 0.5332, train F loss: -0.010603, acc 0.986200\n",
      "5.26s, epoch [798\\1500]: train D loss: 0.5141, train F loss: -0.030003, acc 0.993400\n",
      "5.26s, epoch [799\\1500]: train D loss: 0.5290, train F loss: -0.027687, acc 0.990600\n",
      "5.26s, epoch [800\\1500]: train D loss: 0.5249, train F loss: -0.025667, acc 0.990200\n",
      "5.27s, epoch [801\\1500]: train D loss: 0.5302, train F loss: -0.009658, acc 0.989000\n",
      "5.27s, epoch [802\\1500]: train D loss: 0.5239, train F loss: -0.027280, acc 0.991200\n",
      "5.25s, epoch [803\\1500]: train D loss: 0.5138, train F loss: -0.026189, acc 0.993600\n",
      "5.26s, epoch [804\\1500]: train D loss: 0.5170, train F loss: -0.027714, acc 0.993600\n",
      "5.27s, epoch [805\\1500]: train D loss: 0.5221, train F loss: -0.027241, acc 0.992600\n",
      "5.27s, epoch [806\\1500]: train D loss: 0.5099, train F loss: -0.025420, acc 0.992600\n",
      "5.27s, epoch [807\\1500]: train D loss: 0.5142, train F loss: -0.024527, acc 0.991200\n",
      "5.26s, epoch [808\\1500]: train D loss: 0.5066, train F loss: -0.025535, acc 0.990400\n",
      "5.26s, epoch [809\\1500]: train D loss: 0.5138, train F loss: -0.029195, acc 0.992000\n",
      "5.26s, epoch [810\\1500]: train D loss: 0.5099, train F loss: -0.019255, acc 0.991400\n",
      "5.28s, epoch [811\\1500]: train D loss: 0.5070, train F loss: -0.034472, acc 0.994800\n",
      "5.27s, epoch [812\\1500]: train D loss: 0.5080, train F loss: -0.028056, acc 0.993000\n",
      "5.25s, epoch [813\\1500]: train D loss: 0.5092, train F loss: -0.034727, acc 0.995000\n",
      "5.27s, epoch [814\\1500]: train D loss: 0.5237, train F loss: -0.011851, acc 0.989000\n",
      "5.27s, epoch [815\\1500]: train D loss: 0.5233, train F loss: -0.026451, acc 0.992000\n",
      "5.26s, epoch [816\\1500]: train D loss: 0.5191, train F loss: -0.028174, acc 0.992400\n",
      "5.27s, epoch [817\\1500]: train D loss: 0.5306, train F loss: -0.016745, acc 0.989600\n",
      "5.26s, epoch [818\\1500]: train D loss: 0.5180, train F loss: -0.030146, acc 0.992400\n",
      "5.25s, epoch [819\\1500]: train D loss: 0.5138, train F loss: -0.022360, acc 0.991200\n",
      "5.26s, epoch [820\\1500]: train D loss: 0.5100, train F loss: -0.028725, acc 0.992800\n",
      "5.27s, epoch [821\\1500]: train D loss: 0.5232, train F loss: -0.020713, acc 0.990000\n",
      "5.26s, epoch [822\\1500]: train D loss: 0.5261, train F loss: -0.020439, acc 0.989400\n",
      "5.26s, epoch [823\\1500]: train D loss: 0.5069, train F loss: -0.031652, acc 0.994200\n",
      "5.26s, epoch [824\\1500]: train D loss: 0.5141, train F loss: -0.035468, acc 0.995400\n",
      "5.27s, epoch [825\\1500]: train D loss: 0.5043, train F loss: -0.034048, acc 0.996000\n",
      "5.26s, epoch [826\\1500]: train D loss: 0.4994, train F loss: -0.014901, acc 0.992800\n",
      "5.27s, epoch [827\\1500]: train D loss: 0.5295, train F loss: -0.002901, acc 0.987400\n",
      "5.27s, epoch [828\\1500]: train D loss: 0.5097, train F loss: -0.033573, acc 0.995200\n",
      "5.26s, epoch [829\\1500]: train D loss: 0.5134, train F loss: -0.022225, acc 0.990200\n",
      "5.27s, epoch [830\\1500]: train D loss: 0.5226, train F loss: -0.021409, acc 0.991200\n",
      "5.27s, epoch [831\\1500]: train D loss: 0.5110, train F loss: -0.031342, acc 0.993800\n",
      "5.27s, epoch [832\\1500]: train D loss: 0.5119, train F loss: -0.018776, acc 0.990600\n",
      "5.27s, epoch [833\\1500]: train D loss: 0.5120, train F loss: -0.029571, acc 0.993000\n",
      "5.28s, epoch [834\\1500]: train D loss: 0.5149, train F loss: -0.026777, acc 0.991600\n",
      "5.26s, epoch [835\\1500]: train D loss: 0.5101, train F loss: -0.029957, acc 0.993800\n",
      "5.26s, epoch [836\\1500]: train D loss: 0.4974, train F loss: -0.030647, acc 0.994200\n",
      "5.26s, epoch [837\\1500]: train D loss: 0.5159, train F loss: -0.030071, acc 0.993200\n",
      "5.26s, epoch [838\\1500]: train D loss: 0.5163, train F loss: -0.031021, acc 0.993400\n",
      "5.26s, epoch [839\\1500]: train D loss: 0.5229, train F loss: -0.005406, acc 0.987600\n",
      "5.27s, epoch [840\\1500]: train D loss: 0.5153, train F loss: -0.033196, acc 0.994400\n",
      "5.26s, epoch [841\\1500]: train D loss: 0.5267, train F loss: -0.033246, acc 0.993000\n",
      "5.26s, epoch [842\\1500]: train D loss: 0.5114, train F loss: -0.024574, acc 0.992000\n",
      "5.27s, epoch [843\\1500]: train D loss: 0.5129, train F loss: -0.029286, acc 0.992800\n",
      "5.26s, epoch [844\\1500]: train D loss: 0.5172, train F loss: -0.028811, acc 0.992200\n",
      "5.27s, epoch [845\\1500]: train D loss: 0.5110, train F loss: -0.032070, acc 0.994000\n",
      "5.26s, epoch [846\\1500]: train D loss: 0.5267, train F loss: -0.024035, acc 0.991800\n",
      "5.26s, epoch [847\\1500]: train D loss: 0.5218, train F loss: -0.028360, acc 0.992200\n",
      "5.27s, epoch [848\\1500]: train D loss: 0.5218, train F loss: -0.016829, acc 0.991600\n",
      "5.26s, epoch [849\\1500]: train D loss: 0.5224, train F loss: -0.024914, acc 0.990000\n",
      "5.26s, epoch [850\\1500]: train D loss: 0.5102, train F loss: -0.029295, acc 0.991800\n",
      "5.26s, epoch [851\\1500]: train D loss: 0.5248, train F loss: -0.006700, acc 0.990400\n",
      "5.27s, epoch [852\\1500]: train D loss: 0.5129, train F loss: -0.028115, acc 0.992800\n",
      "5.27s, epoch [853\\1500]: train D loss: 0.5051, train F loss: -0.030943, acc 0.994400\n",
      "5.26s, epoch [854\\1500]: train D loss: 0.5102, train F loss: -0.028301, acc 0.994200\n",
      "5.26s, epoch [855\\1500]: train D loss: 0.5114, train F loss: -0.029810, acc 0.995000\n",
      "5.27s, epoch [856\\1500]: train D loss: 0.5190, train F loss: -0.025615, acc 0.992400\n",
      "5.28s, epoch [857\\1500]: train D loss: 0.5191, train F loss: -0.018771, acc 0.989000\n",
      "5.27s, epoch [858\\1500]: train D loss: 0.5248, train F loss: -0.029488, acc 0.992000\n",
      "5.28s, epoch [859\\1500]: train D loss: 0.5181, train F loss: -0.023180, acc 0.989000\n",
      "5.26s, epoch [860\\1500]: train D loss: 0.5158, train F loss: -0.024869, acc 0.992600\n",
      "5.27s, epoch [861\\1500]: train D loss: 0.5345, train F loss: -0.029548, acc 0.992000\n",
      "5.27s, epoch [862\\1500]: train D loss: 0.5070, train F loss: -0.022888, acc 0.992400\n",
      "5.26s, epoch [863\\1500]: train D loss: 0.5195, train F loss: -0.032746, acc 0.994000\n",
      "5.26s, epoch [864\\1500]: train D loss: 0.5039, train F loss: -0.036181, acc 0.995400\n",
      "5.33s, epoch [865\\1500]: train D loss: 0.5192, train F loss: -0.030034, acc 0.993600\n",
      "5.26s, epoch [866\\1500]: train D loss: 0.5110, train F loss: -0.028034, acc 0.991600\n",
      "5.26s, epoch [867\\1500]: train D loss: 0.5157, train F loss: -0.031033, acc 0.995000\n",
      "5.27s, epoch [868\\1500]: train D loss: 0.5146, train F loss: -0.034016, acc 0.992800\n",
      "5.26s, epoch [869\\1500]: train D loss: 0.5284, train F loss: -0.014833, acc 0.991600\n",
      "5.27s, epoch [870\\1500]: train D loss: 0.5216, train F loss: -0.032017, acc 0.994000\n",
      "5.27s, epoch [871\\1500]: train D loss: 0.5371, train F loss: -0.028397, acc 0.991200\n",
      "5.26s, epoch [872\\1500]: train D loss: 0.5158, train F loss: -0.032919, acc 0.993800\n",
      "5.26s, epoch [873\\1500]: train D loss: 0.5197, train F loss: -0.027369, acc 0.991800\n",
      "5.26s, epoch [874\\1500]: train D loss: 0.5323, train F loss: -0.025896, acc 0.992400\n",
      "5.27s, epoch [875\\1500]: train D loss: 0.5260, train F loss: -0.024252, acc 0.990800\n",
      "5.26s, epoch [876\\1500]: train D loss: 0.5166, train F loss: -0.028932, acc 0.992800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.27s, epoch [877\\1500]: train D loss: 0.5182, train F loss: -0.027385, acc 0.991600\n",
      "5.26s, epoch [878\\1500]: train D loss: 0.5209, train F loss: -0.028811, acc 0.993800\n",
      "5.29s, epoch [879\\1500]: train D loss: 0.5136, train F loss: -0.029681, acc 0.992600\n",
      "5.29s, epoch [880\\1500]: train D loss: 0.5214, train F loss: -0.026597, acc 0.992800\n",
      "5.30s, epoch [881\\1500]: train D loss: 0.5331, train F loss: -0.017772, acc 0.990000\n",
      "5.28s, epoch [882\\1500]: train D loss: 0.5219, train F loss: -0.027684, acc 0.993400\n",
      "5.26s, epoch [883\\1500]: train D loss: 0.5142, train F loss: -0.032120, acc 0.994200\n",
      "5.26s, epoch [884\\1500]: train D loss: 0.5104, train F loss: -0.031014, acc 0.994600\n",
      "5.27s, epoch [885\\1500]: train D loss: 0.5147, train F loss: -0.029444, acc 0.993200\n",
      "5.26s, epoch [886\\1500]: train D loss: 0.5393, train F loss: -0.017134, acc 0.989400\n",
      "5.27s, epoch [887\\1500]: train D loss: 0.5233, train F loss: -0.029957, acc 0.992400\n",
      "5.27s, epoch [888\\1500]: train D loss: 0.5110, train F loss: -0.021594, acc 0.991200\n",
      "5.27s, epoch [889\\1500]: train D loss: 0.5194, train F loss: -0.033483, acc 0.994000\n",
      "5.27s, epoch [890\\1500]: train D loss: 0.5215, train F loss: -0.027189, acc 0.991400\n",
      "5.27s, epoch [891\\1500]: train D loss: 0.5175, train F loss: -0.035085, acc 0.994000\n",
      "5.27s, epoch [892\\1500]: train D loss: 0.5122, train F loss: -0.037197, acc 0.995200\n",
      "5.27s, epoch [893\\1500]: train D loss: 0.5205, train F loss: -0.019823, acc 0.990400\n",
      "5.27s, epoch [894\\1500]: train D loss: 0.5311, train F loss: -0.022198, acc 0.989600\n",
      "5.27s, epoch [895\\1500]: train D loss: 0.5139, train F loss: -0.032706, acc 0.994200\n",
      "5.27s, epoch [896\\1500]: train D loss: 0.5254, train F loss: -0.021832, acc 0.991600\n",
      "5.27s, epoch [897\\1500]: train D loss: 0.5264, train F loss: -0.026303, acc 0.992800\n",
      "5.26s, epoch [898\\1500]: train D loss: 0.5193, train F loss: -0.024499, acc 0.992600\n",
      "5.27s, epoch [899\\1500]: train D loss: 0.5385, train F loss: -0.017530, acc 0.989200\n",
      "5.27s, epoch [900\\1500]: train D loss: 0.5061, train F loss: -0.031226, acc 0.992800\n",
      "5.25s, epoch [901\\1500]: train D loss: 0.5119, train F loss: -0.034452, acc 0.995200\n",
      "5.26s, epoch [902\\1500]: train D loss: 0.5172, train F loss: -0.032096, acc 0.994200\n",
      "5.25s, epoch [903\\1500]: train D loss: 0.5277, train F loss: -0.026438, acc 0.991800\n",
      "5.26s, epoch [904\\1500]: train D loss: 0.5189, train F loss: -0.017402, acc 0.989600\n",
      "5.25s, epoch [905\\1500]: train D loss: 0.5171, train F loss: -0.027823, acc 0.992200\n",
      "5.25s, epoch [906\\1500]: train D loss: 0.5138, train F loss: -0.032747, acc 0.994600\n",
      "5.25s, epoch [907\\1500]: train D loss: 0.5345, train F loss: -0.012344, acc 0.989400\n",
      "5.25s, epoch [908\\1500]: train D loss: 0.5150, train F loss: -0.031272, acc 0.993200\n",
      "5.25s, epoch [909\\1500]: train D loss: 0.5180, train F loss: -0.029583, acc 0.993600\n",
      "5.25s, epoch [910\\1500]: train D loss: 0.5076, train F loss: -0.025978, acc 0.994800\n",
      "5.25s, epoch [911\\1500]: train D loss: 0.5361, train F loss: 0.005005, acc 0.984600\n",
      "5.25s, epoch [912\\1500]: train D loss: 0.5107, train F loss: -0.029453, acc 0.992400\n",
      "5.26s, epoch [913\\1500]: train D loss: 0.5101, train F loss: -0.030194, acc 0.993800\n",
      "5.26s, epoch [914\\1500]: train D loss: 0.5234, train F loss: -0.031704, acc 0.992200\n",
      "5.25s, epoch [915\\1500]: train D loss: 0.5199, train F loss: -0.030860, acc 0.993400\n",
      "5.25s, epoch [916\\1500]: train D loss: 0.5111, train F loss: -0.032997, acc 0.994000\n",
      "5.25s, epoch [917\\1500]: train D loss: 0.5201, train F loss: -0.028406, acc 0.993600\n",
      "5.25s, epoch [918\\1500]: train D loss: 0.5174, train F loss: -0.011325, acc 0.991400\n",
      "5.25s, epoch [919\\1500]: train D loss: 0.5511, train F loss: 0.062294, acc 0.975400\n",
      "5.26s, epoch [920\\1500]: train D loss: 0.5203, train F loss: -0.025561, acc 0.991800\n",
      "5.25s, epoch [921\\1500]: train D loss: 0.5090, train F loss: -0.034888, acc 0.994400\n",
      "5.26s, epoch [922\\1500]: train D loss: 0.4972, train F loss: -0.019630, acc 0.991400\n",
      "5.26s, epoch [923\\1500]: train D loss: 0.5084, train F loss: -0.024922, acc 0.992000\n",
      "5.25s, epoch [924\\1500]: train D loss: 0.5171, train F loss: -0.028897, acc 0.993800\n",
      "5.25s, epoch [925\\1500]: train D loss: 0.5219, train F loss: -0.031160, acc 0.993400\n",
      "5.27s, epoch [926\\1500]: train D loss: 0.5146, train F loss: -0.037056, acc 0.995400\n",
      "5.25s, epoch [927\\1500]: train D loss: 0.5133, train F loss: -0.033861, acc 0.994400\n",
      "5.25s, epoch [928\\1500]: train D loss: 0.5130, train F loss: -0.031417, acc 0.993000\n",
      "5.25s, epoch [929\\1500]: train D loss: 0.5160, train F loss: -0.033063, acc 0.993800\n",
      "5.25s, epoch [930\\1500]: train D loss: 0.5149, train F loss: -0.028584, acc 0.993000\n",
      "5.25s, epoch [931\\1500]: train D loss: 0.5199, train F loss: -0.025802, acc 0.993000\n",
      "5.26s, epoch [932\\1500]: train D loss: 0.5210, train F loss: -0.034120, acc 0.994400\n",
      "5.26s, epoch [933\\1500]: train D loss: 0.5102, train F loss: -0.036829, acc 0.995600\n",
      "5.25s, epoch [934\\1500]: train D loss: 0.5149, train F loss: -0.036994, acc 0.996000\n",
      "5.26s, epoch [935\\1500]: train D loss: 0.5072, train F loss: -0.038283, acc 0.996000\n",
      "5.25s, epoch [936\\1500]: train D loss: 0.5309, train F loss: -0.019099, acc 0.992200\n",
      "5.25s, epoch [937\\1500]: train D loss: 0.5398, train F loss: -0.010677, acc 0.986400\n",
      "5.25s, epoch [938\\1500]: train D loss: 0.5210, train F loss: -0.028603, acc 0.993400\n",
      "5.25s, epoch [939\\1500]: train D loss: 0.5224, train F loss: -0.020607, acc 0.991600\n",
      "5.26s, epoch [940\\1500]: train D loss: 0.5227, train F loss: -0.024013, acc 0.992600\n",
      "5.25s, epoch [941\\1500]: train D loss: 0.5330, train F loss: -0.036591, acc 0.995600\n",
      "5.26s, epoch [942\\1500]: train D loss: 0.5179, train F loss: -0.030447, acc 0.993800\n",
      "5.27s, epoch [943\\1500]: train D loss: 0.5206, train F loss: -0.032033, acc 0.994400\n",
      "5.24s, epoch [944\\1500]: train D loss: 0.5106, train F loss: -0.035368, acc 0.995400\n",
      "5.25s, epoch [945\\1500]: train D loss: 0.5059, train F loss: -0.031069, acc 0.993600\n",
      "5.25s, epoch [946\\1500]: train D loss: 0.5235, train F loss: -0.034840, acc 0.993600\n",
      "5.26s, epoch [947\\1500]: train D loss: 0.5151, train F loss: -0.024586, acc 0.992800\n",
      "5.25s, epoch [948\\1500]: train D loss: 0.5346, train F loss: -0.026968, acc 0.991600\n",
      "5.25s, epoch [949\\1500]: train D loss: 0.5181, train F loss: -0.029269, acc 0.993400\n",
      "5.25s, epoch [950\\1500]: train D loss: 0.5100, train F loss: -0.034133, acc 0.994400\n",
      "5.24s, epoch [951\\1500]: train D loss: 0.5078, train F loss: -0.036944, acc 0.995400\n",
      "5.25s, epoch [952\\1500]: train D loss: 0.5218, train F loss: -0.033651, acc 0.994400\n",
      "5.27s, epoch [953\\1500]: train D loss: 0.5086, train F loss: -0.024668, acc 0.991800\n",
      "5.24s, epoch [954\\1500]: train D loss: 0.5337, train F loss: -0.023414, acc 0.989400\n",
      "5.25s, epoch [955\\1500]: train D loss: 0.5306, train F loss: -0.028433, acc 0.992400\n",
      "5.25s, epoch [956\\1500]: train D loss: 0.5295, train F loss: -0.026908, acc 0.992400\n",
      "5.25s, epoch [957\\1500]: train D loss: 0.5397, train F loss: -0.022600, acc 0.991000\n",
      "5.25s, epoch [958\\1500]: train D loss: 0.5233, train F loss: -0.024432, acc 0.993400\n",
      "5.25s, epoch [959\\1500]: train D loss: 0.5162, train F loss: -0.032939, acc 0.993600\n",
      "5.25s, epoch [960\\1500]: train D loss: 0.5191, train F loss: -0.023407, acc 0.991400\n",
      "5.25s, epoch [961\\1500]: train D loss: 0.5320, train F loss: -0.026506, acc 0.991800\n",
      "5.25s, epoch [962\\1500]: train D loss: 0.5279, train F loss: -0.033736, acc 0.993000\n",
      "5.25s, epoch [963\\1500]: train D loss: 0.5183, train F loss: -0.028263, acc 0.993000\n",
      "5.25s, epoch [964\\1500]: train D loss: 0.5263, train F loss: -0.026135, acc 0.991800\n",
      "5.25s, epoch [965\\1500]: train D loss: 0.5156, train F loss: -0.031064, acc 0.993800\n",
      "5.26s, epoch [966\\1500]: train D loss: 0.5254, train F loss: -0.037952, acc 0.995600\n",
      "5.27s, epoch [967\\1500]: train D loss: 0.5121, train F loss: -0.033422, acc 0.994800\n",
      "5.25s, epoch [968\\1500]: train D loss: 0.5145, train F loss: -0.027509, acc 0.993600\n",
      "5.24s, epoch [969\\1500]: train D loss: 0.5312, train F loss: -0.024290, acc 0.991800\n",
      "5.25s, epoch [970\\1500]: train D loss: 0.5396, train F loss: -0.036527, acc 0.994400\n",
      "5.26s, epoch [971\\1500]: train D loss: 0.5445, train F loss: -0.035733, acc 0.993000\n",
      "5.25s, epoch [972\\1500]: train D loss: 0.5157, train F loss: -0.024824, acc 0.991600\n",
      "5.26s, epoch [973\\1500]: train D loss: 0.5449, train F loss: -0.025874, acc 0.990000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.25s, epoch [974\\1500]: train D loss: 0.5285, train F loss: -0.029129, acc 0.992600\n",
      "5.25s, epoch [975\\1500]: train D loss: 0.5398, train F loss: -0.031335, acc 0.992000\n",
      "5.25s, epoch [976\\1500]: train D loss: 0.5230, train F loss: -0.026537, acc 0.991800\n",
      "5.25s, epoch [977\\1500]: train D loss: 0.5260, train F loss: -0.035304, acc 0.994200\n",
      "5.25s, epoch [978\\1500]: train D loss: 0.5219, train F loss: -0.033460, acc 0.994000\n",
      "5.25s, epoch [979\\1500]: train D loss: 0.5369, train F loss: -0.032442, acc 0.994600\n",
      "5.25s, epoch [980\\1500]: train D loss: 0.5216, train F loss: -0.038471, acc 0.995600\n",
      "5.24s, epoch [981\\1500]: train D loss: 0.5160, train F loss: -0.034684, acc 0.994200\n",
      "5.25s, epoch [982\\1500]: train D loss: 0.5390, train F loss: -0.017325, acc 0.990200\n",
      "5.28s, epoch [983\\1500]: train D loss: 0.5374, train F loss: -0.019995, acc 0.990400\n",
      "5.25s, epoch [984\\1500]: train D loss: 0.5317, train F loss: -0.025430, acc 0.990200\n",
      "5.26s, epoch [985\\1500]: train D loss: 0.5308, train F loss: -0.036849, acc 0.994400\n",
      "5.25s, epoch [986\\1500]: train D loss: 0.5228, train F loss: -0.035568, acc 0.994600\n",
      "5.26s, epoch [987\\1500]: train D loss: 0.5302, train F loss: -0.035147, acc 0.994000\n",
      "5.30s, epoch [988\\1500]: train D loss: 0.5396, train F loss: -0.036535, acc 0.993800\n",
      "5.32s, epoch [989\\1500]: train D loss: 0.5249, train F loss: -0.033314, acc 0.993600\n",
      "5.33s, epoch [990\\1500]: train D loss: 0.5259, train F loss: -0.017233, acc 0.989600\n",
      "5.30s, epoch [991\\1500]: train D loss: 0.5260, train F loss: -0.029769, acc 0.992800\n",
      "5.28s, epoch [992\\1500]: train D loss: 0.5312, train F loss: -0.034827, acc 0.993200\n",
      "5.32s, epoch [993\\1500]: train D loss: 0.5252, train F loss: -0.032213, acc 0.992800\n",
      "5.36s, epoch [994\\1500]: train D loss: 0.5322, train F loss: -0.036130, acc 0.994200\n",
      "5.37s, epoch [995\\1500]: train D loss: 0.5263, train F loss: -0.031169, acc 0.993200\n",
      "5.36s, epoch [996\\1500]: train D loss: 0.5439, train F loss: -0.030358, acc 0.993000\n",
      "5.32s, epoch [997\\1500]: train D loss: 0.5401, train F loss: -0.031285, acc 0.993000\n",
      "5.32s, epoch [998\\1500]: train D loss: 0.5373, train F loss: -0.032948, acc 0.992800\n",
      "5.27s, epoch [999\\1500]: train D loss: 0.5231, train F loss: -0.035489, acc 0.995200\n",
      "5.35s, epoch [1000\\1500]: train D loss: 0.5286, train F loss: -0.038010, acc 0.995000\n",
      "5.34s, epoch [1001\\1500]: train D loss: 0.5305, train F loss: -0.037883, acc 0.996200\n",
      "5.29s, epoch [1002\\1500]: train D loss: 0.5395, train F loss: -0.032335, acc 0.993000\n",
      "5.38s, epoch [1003\\1500]: train D loss: 0.5360, train F loss: -0.026568, acc 0.992400\n",
      "5.45s, epoch [1004\\1500]: train D loss: 0.5209, train F loss: -0.029003, acc 0.993000\n",
      "5.47s, epoch [1005\\1500]: train D loss: 0.5512, train F loss: -0.034144, acc 0.992000\n",
      "24"
     ]
    }
   ],
   "source": [
    "source_dataset = ImageFolder('real_or_drawing/train_data', transform=source_transform)\n",
    "target_dataset = ImageFolder('real_or_drawing/test_data', transform=target_transform)\n",
    "test_dataset = ImageFolder('real_or_drawing/test_data', transform=test_transform)\n",
    "source_loader = get_data_loader(source_dataset, batch_size=256, mode = 'train')\n",
    "target_loader = get_data_loader(target_dataset, batch_size=256, mode = 'train')\n",
    "test_loader = get_data_loader(test_dataset,batch_size=512, mode = 'eval')\n",
    "\n",
    "# epoch test 100,200, 300, 400,500\n",
    "# 100, 0.51349\n",
    "# 200, 0.49530\n",
    "# 300, 0.53430\n",
    "# 400, 0.56480\n",
    "# 500, 0.59531\n",
    "# 500, test without aug, 0.61848\n",
    "\n",
    "set_seed(0)\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "label_predictor = LabelPredictor().to(device)\n",
    "domain_classifier = DomainClassifier().to(device)\n",
    "\n",
    "class_criterion = nn.CrossEntropyLoss()\n",
    "domain_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_F = optim.Adam(feature_extractor.parameters())\n",
    "optimizer_C = optim.Adam(label_predictor.parameters())\n",
    "optimizer_D = optim.Adam(domain_classifier.parameters())\n",
    "\n",
    "# epoch test 100,200, 300, 400,500\n",
    "source_loader = get_data_loader(source_dataset, mode = 'train')\n",
    "target_loader = get_data_loader(target_dataset, mode = 'train')\n",
    "test_loader = get_data_loader(target_dataset, mode = 'eval')\n",
    "epochs = 1500\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    st = time.time()\n",
    "    train_D_loss, train_F_loss, train_acc = train_epoch(source_loader, target_loader, lamb=0.1)\n",
    "    ed = time.time()\n",
    "    if epoch % 100 == 0:\n",
    "        save_model(feature_extractor,  f'extractor_model_{epoch}.bin')\n",
    "        save_model(label_predictor, f'predictor_model_{epoch}.bin')\n",
    "        save_model(domain_classifier, f'domain_classifier_{epoch}.bin')\n",
    "        pred_labels, pred_probs,_ = testing(feature_extractor, label_predictor, test_loader)\n",
    "        df = pd.DataFrame({'id': np.arange(0,len(pred_labels)), 'label': pred_labels})\n",
    "        df.to_csv(f'DaNN_submission_{epoch}.csv',index=False)\n",
    "\n",
    "    print('{:4.2f}s, epoch [{:0>3d}\\{}]: train D loss: {:6.4f}, train F loss: {:6.6f}, acc {:6.6f}'.format(ed-st, epoch, epochs,  train_D_loss, train_F_loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOYFwkLsp9Qa"
   },
   "outputs": [],
   "source": [
    "# train without adversial\n",
    "set_seed(0)\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "label_predictor = LabelPredictor().to(device)\n",
    "domain_classifier = DomainClassifier().to(device)\n",
    "\n",
    "class_criterion = nn.CrossEntropyLoss()\n",
    "domain_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_F = optim.Adam(feature_extractor.parameters())\n",
    "optimizer_C = optim.Adam(label_predictor.parameters())\n",
    "optimizer_D = optim.Adam(domain_classifier.parameters())\n",
    "\n",
    "# epoch test 100,200, 300, 400,500\n",
    "source_loader = get_data_loader(source_dataset, mode = 'train')\n",
    "target_loader = get_data_loader(target_dataset, mode = 'train')\n",
    "test_loader = get_data_loader(target_dataset, mode = 'eval')\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    st = time.time()\n",
    "    train_D_loss, train_F_loss, train_acc = train_epoch(source_loader, target_loader, lamb=0)\n",
    "    ed = time.time()\n",
    "    if epoch % 50 == 0:\n",
    "        save_model(feature_extractor,  f'extractor_model_noadv_{epoch}.bin')\n",
    "        save_model(label_predictor, f'predictor_model_noadv_{epoch}.bin')\n",
    "        save_model(domain_classifier, f'domain_classifier_nodav_{epoch}.bin')\n",
    "\n",
    "    print('{:4.2f}s, epoch [{:0>3d}\\{}]: train D loss: {:6.4f}, train F loss: {:6.6f}, acc {:6.6f}'.format(ed-st, epoch, epochs,  train_D_loss, train_F_loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j12dF9Kyfm8v"
   },
   "outputs": [],
   "source": [
    "# train \n",
    "# 增大 lamb的值效果不好\n",
    "set_seed(0)\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "label_predictor = LabelPredictor().to(device)\n",
    "domain_classifier = DomainClassifier().to(device)\n",
    "\n",
    "class_criterion = nn.CrossEntropyLoss()\n",
    "domain_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_F = optim.Adam(feature_extractor.parameters())\n",
    "optimizer_C = optim.Adam(label_predictor.parameters())\n",
    "optimizer_D = optim.Adam(domain_classifier.parameters())\n",
    "\n",
    "# testing\n",
    "set_seed(0)\n",
    "# model name\n",
    "epochs_num = 500\n",
    "ex_name = f'extractor_model_{epochs_num}.bin'\n",
    "pr_name = f'predictor_model_{epochs_num}.bin'\n",
    "dc_name = f'domain_classifier_{epochs_num}.bin'\n",
    "# data\n",
    "test_loader = get_data_loader(test_dataset, mode = 'eval')\n",
    "# load model\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "label_predictr = LabelPredictor().to(device)\n",
    "domain_classifier = DomainClassifier().to(device)\n",
    "\n",
    "feature_extractor = load_model(feature_extractor, ex_name)\n",
    "label_predictor = load_model(label_predictor, pr_name)\n",
    "domain_classifier = DomainClassifier().to(device)\n",
    "base = 500\n",
    "epochs = 300\n",
    "\n",
    "for epoch in range(base+1, base+epochs+1):\n",
    "    st = time.time()\n",
    "    train_D_loss, train_F_loss, train_acc = train_epoch(source_loader, target_loader, lamb=0.1)\n",
    "    ed = time.time()\n",
    "    if epoch % 50 == 0:\n",
    "        save_model(feature_extractor,  f'extractor_model_{epoch}.bin')\n",
    "        save_model(label_predictor, f'predictor_model_{epoch}.bin')\n",
    "        save_model(domain_classifier, f'domain_classifier_{epoch}.bin')\n",
    "\n",
    "    print('{:4.2f}s, epoch [{:0>3d}\\{}]: train D loss: {:6.4f}, train F loss: {:6.6f}, acc {:6.6f}'.format(ed-st, epoch, epochs,  train_D_loss, train_F_loss, train_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pXAGSdgvST3v"
   },
   "source": [
    "## Self-training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EUO54JfpUagS"
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dd0XJV6FSTdX"
   },
   "outputs": [],
   "source": [
    "class DrawDataset(data.Dataset):\n",
    "    def __init__(self, data, labels, transform):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = self.transform(Image.open(self.data[i]))\n",
    "        y = self.labels[i]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class AddDataset(data.Dataset):\n",
    "    def __init__(self, data, labels, s_tf, t_tf):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.s_tf = s_tf\n",
    "        self.t_tf = t_tf\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        isadd = True if i >= 5000 else False\n",
    "        tf = self.t_tf if isadd else self.s_tf\n",
    "        x = tf(Image.open(self.data[i]))\n",
    "        y = self.labels[i]\n",
    "        return x, (y, isadd)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IcIJmy4sWJT0"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_data(data_path, name):\n",
    "    data_path = os.path.join(data_path, name)\n",
    "    paths = []\n",
    "    labels = []\n",
    "    for dir_ in os.listdir(data_path):\n",
    "        dir_path = os.path.join(data_path, dir_)\n",
    "        label = int(dir_)\n",
    "        for file in  os.listdir(dir_path):\n",
    "            file_path = os.path.join(dir_path, file)\n",
    "            paths.append(file_path)\n",
    "            labels.append(label)\n",
    "    paths = np.array(paths)\n",
    "    labels = np.array(labels)\n",
    "    return paths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uzhC7-Zn5XuQ"
   },
   "outputs": [],
   "source": [
    "def train_epoch(source_dataloader, target_dataloader, lamb):\n",
    "    '''\n",
    "      Args:\n",
    "        source_dataloader: source data的dataloader\n",
    "        target_dataloader: target data的dataloader\n",
    "        lamb: 調控adversarial的loss係數。\n",
    "    '''\n",
    "\n",
    "    # D loss: Domain Classifier的loss\n",
    "    # F loss: Feature Extrator & Label Predictor的loss\n",
    "    # total_hit: 計算目前對了幾筆 total_num: 目前經過了幾筆\n",
    "    set_seed(0)\n",
    "    lamb = 0.1\n",
    "    running_D_loss, running_F_loss = 0.0, 0.0\n",
    "    total_hit, total_num = 0.0, 0.0\n",
    "    d_num = 0\n",
    "    mix_num = 0\n",
    "    # 验证混合标签是否正确区分\n",
    "    t_num = 0\n",
    "    # 实际上只用了部分 target在训练， 但是由于shuffle，每次用的target都不太一样\n",
    "    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_dataloader, target_dataloader)):\n",
    "        # 对于混合数据集，需要标注来源，否则D的训练会被影响\n",
    "        source_label, isadd = source_label\n",
    "\n",
    "        source_data = source_data.to(device)\n",
    "        source_label = source_label.to(device, dtype = torch.long)\n",
    "        target_data = target_data.to(device)\n",
    "\n",
    "        # 我們把source data和target data混在一起，否則batch_norm可能會算錯 (兩邊的data的mean/var不太一樣)\n",
    "        # batch normlization layer\n",
    "        mixed_data = torch.cat([source_data, target_data], dim=0)\n",
    "        domain_label = torch.zeros([source_data.shape[0] + target_data.shape[0], 1]).to(device, dtype = torch.long)\n",
    "        # 設定source data的label為1, 需要把其中target混合部分正确标注为0\n",
    "        domain_label[:source_data.shape[0]] = 1\n",
    "        domain_label[:source_data.shape[0]][isadd] = 0\n",
    "        t_num += np.sum((domain_label == 1).cpu().numpy())\n",
    "        # Step 1 : 訓練Domain Classifier\n",
    "        feature = feature_extractor(mixed_data)\n",
    "        # 因為我們在Step 1不需要訓練Feature Extractor，所以把feature detach避免loss backprop上去。\n",
    "        domain_logits = domain_classifier(feature.detach())\n",
    "        loss = domain_criterion(domain_logits, domain_label.float())\n",
    "        running_D_loss+= loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Step 2 : 訓練Feature Extractor和Predictor\n",
    "        class_logits = label_predictor(feature[:source_data.shape[0]])\n",
    "        domain_logits = domain_classifier(feature)\n",
    "        # 计算预测结果\n",
    "        # domain \n",
    "        d_preds = nn.functional.sigmoid(domain_logits)\n",
    "        d_preds[d_preds>0.5] = 1\n",
    "        d_preds[d_preds<=0.5] = 0\n",
    "        d_num += np.sum((d_preds ==domain_label).cpu().numpy())\n",
    "        mix_num += len(domain_label)\n",
    "\n",
    "        # loss為原本的class CE - lamb * domain BCE，相減的原因同GAN中的Discriminator中的G loss\n",
    "        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label.float())\n",
    "        running_F_loss+= loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_F.step()\n",
    "        optimizer_C.step()\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        optimizer_F.zero_grad()\n",
    "        optimizer_C.zero_grad()\n",
    "\n",
    "        total_hit += torch.sum(torch.argmax(class_logits, dim=1) == source_label).item()\n",
    "        total_num += source_data.shape[0]\n",
    "        print(i, end='\\r')\n",
    "    print('D_acc: {:4.2f}'.format(d_num /mix_num), end=' ')\n",
    "    return running_D_loss / (i+1), running_F_loss / (i+1), total_hit / total_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zz-rfaEXY2j7"
   },
   "outputs": [],
   "source": [
    "data_path = './real_or_drawing'\n",
    "source_x_paths, source_y = get_data(data_path, 'train_data')\n",
    "target_x_paths, target_y = get_data(data_path, 'test_data')\n",
    "source_dataset = DrawDataset(source_x_paths, source_y, source_transform)\n",
    "target_dataset = DrawDataset(target_x_paths, target_y, target_transform)\n",
    "test_dataset = DrawDataset(target_x_paths, target_y, test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SVYoPlQFZXpf"
   },
   "outputs": [],
   "source": [
    "source_loader = get_data_loader(source_dataset, mode = 'train')\n",
    "target_loader = get_data_loader(target_dataset, mode = 'train')\n",
    "test_loader = get_data_loader(test_dataset, mode = 'eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dnzjeUcK5Xua"
   },
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "epochs_num = 1000\n",
    "ex_name = f'extractor_model_{epochs_num}.bin'\n",
    "pr_name = f'predictor_model_{epochs_num}.bin'\n",
    "dc_name = f'domain_classifier_{epochs_num}.bin'\n",
    "# data\n",
    "test_loader = get_data_loader(test_dataset, batch_size=512, mode = 'eval')\n",
    "# load model\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "label_predictr = LabelPredictor().to(device)\n",
    "domain_classifier = DomainClassifier().to(device)\n",
    "\n",
    "feature_extractor = load_model(feature_extractor, ex_name)\n",
    "label_predictor = load_model(label_predictor, pr_name)\n",
    "domain_classifier = DomainClassifier().to(device)\n",
    "\n",
    "class_criterion = nn.CrossEntropyLoss()\n",
    "domain_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_F = optim.Adam(feature_extractor.parameters())\n",
    "optimizer_C = optim.Adam(label_predictor.parameters())\n",
    "optimizer_D = optim.Adam(domain_classifier.parameters())\n",
    "\n",
    "\n",
    "total_epochs = 1000\n",
    "threshold = 0.9\n",
    "mix_num = 5000 # pesudo labeled data 的数量\n",
    "sl_epochs = 10  # self  training time\n",
    "epochs = 200  # training  epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OjglEAp8lDi8",
    "outputId": "9b6ed19a-ab5b-48ea-82bc-8d4d86099b1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add : 5000 new source size: 10000\n",
      "D_acc: 0.99 7.99s, epoch [1001\\200]: train D loss: 0.0902, train F loss: 12.576127, acc 0.746500\n",
      "D_acc: 1.00 7.99s, epoch [1002\\200]: train D loss: 0.0220, train F loss: 1.087104, acc 0.824100\n",
      "D_acc: 1.00 8.00s, epoch [1003\\200]: train D loss: 0.0145, train F loss: 0.624270, acc 0.847500\n",
      "D_acc: 1.00 7.99s, epoch [1004\\200]: train D loss: 0.0111, train F loss: 0.484679, acc 0.865400\n",
      "D_acc: 1.00 7.98s, epoch [1005\\200]: train D loss: 0.0083, train F loss: 0.405506, acc 0.880700\n",
      "D_acc: 1.00 8.12s, epoch [1006\\200]: train D loss: 0.0166, train F loss: 0.352356, acc 0.891900\n",
      "D_acc: 1.00 8.03s, epoch [1007\\200]: train D loss: 0.0101, train F loss: 0.314021, acc 0.901500\n",
      "D_acc: 1.00 8.00s, epoch [1008\\200]: train D loss: 0.0086, train F loss: 0.280252, acc 0.910400\n",
      "D_acc: 1.00 7.97s, epoch [1009\\200]: train D loss: 0.0074, train F loss: 0.249906, acc 0.920000\n",
      "D_acc: 1.00 8.01s, epoch [1010\\200]: train D loss: 0.0103, train F loss: 0.225107, acc 0.928100\n",
      "D_acc: 1.00 8.24s, epoch [1011\\200]: train D loss: 0.0036, train F loss: 0.203521, acc 0.934500\n",
      "D_acc: 1.00 8.00s, epoch [1012\\200]: train D loss: 0.0018, train F loss: 0.184785, acc 0.940800\n",
      "D_acc: 1.00 8.01s, epoch [1013\\200]: train D loss: 0.0006, train F loss: 0.167375, acc 0.945200\n",
      "D_acc: 1.00 7.98s, epoch [1014\\200]: train D loss: 0.0004, train F loss: 0.152193, acc 0.949500\n",
      "D_acc: 1.00 8.02s, epoch [1015\\200]: train D loss: 0.0003, train F loss: 0.138865, acc 0.952900\n",
      "D_acc: 1.00 8.10s, epoch [1016\\200]: train D loss: 0.0002, train F loss: 0.126907, acc 0.956500\n",
      "D_acc: 1.00 8.00s, epoch [1017\\200]: train D loss: 0.0002, train F loss: 0.115620, acc 0.959700\n",
      "D_acc: 1.00 8.00s, epoch [1018\\200]: train D loss: 0.0002, train F loss: 0.105603, acc 0.963300\n",
      "D_acc: 1.00 7.99s, epoch [1019\\200]: train D loss: 0.0001, train F loss: 0.096454, acc 0.966200\n",
      "D_acc: 1.00 7.96s, epoch [1020\\200]: train D loss: 0.0001, train F loss: 0.087733, acc 0.969100\n",
      "D_acc: 1.00 7.98s, epoch [1021\\200]: train D loss: 0.0001, train F loss: 0.079649, acc 0.972200\n",
      "D_acc: 1.00 7.99s, epoch [1022\\200]: train D loss: 0.0001, train F loss: 0.073790, acc 0.974300\n",
      "D_acc: 1.00 7.95s, epoch [1023\\200]: train D loss: 0.0001, train F loss: 0.066854, acc 0.977000\n",
      "D_acc: 1.00 8.01s, epoch [1024\\200]: train D loss: 0.0001, train F loss: 0.061342, acc 0.978800\n",
      "D_acc: 1.00 7.97s, epoch [1025\\200]: train D loss: 0.0001, train F loss: 0.056081, acc 0.981000\n",
      "D_acc: 1.00 7.94s, epoch [1026\\200]: train D loss: 0.0001, train F loss: 0.052366, acc 0.981700\n",
      "D_acc: 1.00 8.00s, epoch [1027\\200]: train D loss: 0.0001, train F loss: 0.048069, acc 0.983700\n",
      "D_acc: 1.00 8.02s, epoch [1028\\200]: train D loss: 0.0001, train F loss: 0.043642, acc 0.986300\n",
      "D_acc: 1.00 8.01s, epoch [1029\\200]: train D loss: 0.0001, train F loss: 0.038838, acc 0.988000\n",
      "D_acc: 1.00 7.98s, epoch [1030\\200]: train D loss: 0.0001, train F loss: 0.035095, acc 0.989200\n",
      "D_acc: 1.00 8.00s, epoch [1031\\200]: train D loss: 0.0001, train F loss: 0.031795, acc 0.990100\n",
      "D_acc: 1.00 8.00s, epoch [1032\\200]: train D loss: 0.0001, train F loss: 0.028313, acc 0.992000\n",
      "D_acc: 1.00 8.01s, epoch [1033\\200]: train D loss: 0.0000, train F loss: 0.025602, acc 0.992500\n",
      "D_acc: 1.00 8.01s, epoch [1034\\200]: train D loss: 0.0000, train F loss: 0.023019, acc 0.993400\n",
      "D_acc: 1.00 7.99s, epoch [1035\\200]: train D loss: 0.0000, train F loss: 0.020113, acc 0.994200\n",
      "D_acc: 1.00 8.04s, epoch [1036\\200]: train D loss: 0.0000, train F loss: 0.018520, acc 0.994500\n",
      "D_acc: 1.00 8.03s, epoch [1037\\200]: train D loss: 0.0000, train F loss: 0.017504, acc 0.995000\n",
      "D_acc: 1.00 8.00s, epoch [1038\\200]: train D loss: 0.0000, train F loss: 0.016107, acc 0.995600\n",
      "D_acc: 1.00 8.02s, epoch [1039\\200]: train D loss: 0.0000, train F loss: 0.015124, acc 0.995800\n",
      "D_acc: 1.00 8.00s, epoch [1040\\200]: train D loss: 0.0000, train F loss: 0.014082, acc 0.995800\n",
      "D_acc: 1.00 8.00s, epoch [1041\\200]: train D loss: 0.0000, train F loss: 0.011877, acc 0.996600\n",
      "D_acc: 1.00 7.97s, epoch [1042\\200]: train D loss: 0.0000, train F loss: 0.010711, acc 0.997200\n",
      "D_acc: 1.00 8.01s, epoch [1043\\200]: train D loss: 0.0000, train F loss: 0.009350, acc 0.997800\n",
      "D_acc: 1.00 8.00s, epoch [1044\\200]: train D loss: 0.0000, train F loss: 0.008313, acc 0.998300\n",
      "D_acc: 1.00 8.05s, epoch [1045\\200]: train D loss: 0.0000, train F loss: 0.007575, acc 0.998100\n",
      "D_acc: 1.00 8.09s, epoch [1046\\200]: train D loss: 0.0000, train F loss: 0.007211, acc 0.998200\n",
      "D_acc: 1.00 8.10s, epoch [1047\\200]: train D loss: 0.0000, train F loss: 0.006584, acc 0.998400\n",
      "D_acc: 1.00 8.08s, epoch [1048\\200]: train D loss: 0.0000, train F loss: 0.006164, acc 0.998500\n",
      "D_acc: 1.00 8.01s, epoch [1049\\200]: train D loss: 0.0000, train F loss: 0.005790, acc 0.998400\n",
      "D_acc: 1.00 8.05s, epoch [1050\\200]: train D loss: 0.0000, train F loss: 0.005636, acc 0.998400\n",
      "D_acc: 1.00 8.28s, epoch [1051\\200]: train D loss: 0.0000, train F loss: 0.005263, acc 0.998500\n",
      "D_acc: 1.00 8.01s, epoch [1052\\200]: train D loss: 0.0000, train F loss: 0.005135, acc 0.998500\n",
      "D_acc: 1.00 8.07s, epoch [1053\\200]: train D loss: 0.0000, train F loss: 0.004898, acc 0.998500\n",
      "D_acc: 1.00 8.30s, epoch [1054\\200]: train D loss: 0.0000, train F loss: 0.004748, acc 0.998500\n",
      "D_acc: 1.00 8.21s, epoch [1055\\200]: train D loss: 0.0000, train F loss: 0.004528, acc 0.998600\n",
      "D_acc: 1.00 8.33s, epoch [1056\\200]: train D loss: 0.0000, train F loss: 0.004487, acc 0.998500\n",
      "D_acc: 1.00 8.29s, epoch [1057\\200]: train D loss: 0.0000, train F loss: 0.004465, acc 0.998600\n",
      "D_acc: 1.00 8.22s, epoch [1058\\200]: train D loss: 0.0000, train F loss: 0.004513, acc 0.998500\n",
      "D_acc: 1.00 8.00s, epoch [1059\\200]: train D loss: 0.0000, train F loss: 0.004205, acc 0.998600\n",
      "D_acc: 1.00 7.99s, epoch [1060\\200]: train D loss: 0.0000, train F loss: 0.004122, acc 0.998600\n",
      "D_acc: 1.00 7.99s, epoch [1061\\200]: train D loss: 0.0000, train F loss: 0.003999, acc 0.998500\n",
      "D_acc: 1.00 7.97s, epoch [1062\\200]: train D loss: 0.0000, train F loss: 0.003940, acc 0.998500\n",
      "D_acc: 1.00 7.99s, epoch [1063\\200]: train D loss: 0.0000, train F loss: 0.003784, acc 0.998500\n",
      "D_acc: 1.00 7.99s, epoch [1064\\200]: train D loss: 0.0000, train F loss: 0.003761, acc 0.998600\n",
      "D_acc: 1.00 7.98s, epoch [1065\\200]: train D loss: 0.0000, train F loss: 0.003656, acc 0.998400\n",
      "D_acc: 1.00 8.00s, epoch [1066\\200]: train D loss: 0.0000, train F loss: 0.003675, acc 0.998600\n",
      "D_acc: 1.00 8.00s, epoch [1067\\200]: train D loss: 0.0000, train F loss: 0.003864, acc 0.998300\n",
      "D_acc: 1.00 7.98s, epoch [1068\\200]: train D loss: 0.0000, train F loss: 0.003509, acc 0.998700\n",
      "D_acc: 1.00 7.96s, epoch [1069\\200]: train D loss: 0.0000, train F loss: 0.003389, acc 0.998700\n",
      "D_acc: 1.00 7.99s, epoch [1070\\200]: train D loss: 0.0000, train F loss: 0.003544, acc 0.998500\n",
      "D_acc: 1.00 7.98s, epoch [1071\\200]: train D loss: 0.0000, train F loss: 0.003466, acc 0.998500\n",
      "D_acc: 1.00 7.98s, epoch [1072\\200]: train D loss: 0.0000, train F loss: 0.003245, acc 0.998700\n",
      "D_acc: 1.00 7.99s, epoch [1073\\200]: train D loss: 0.0000, train F loss: 0.003338, acc 0.998600\n",
      "D_acc: 1.00 8.01s, epoch [1074\\200]: train D loss: 0.0000, train F loss: 0.003180, acc 0.998600\n",
      "D_acc: 1.00 8.01s, epoch [1075\\200]: train D loss: 0.0000, train F loss: 0.003217, acc 0.998600\n",
      "D_acc: 1.00 7.98s, epoch [1076\\200]: train D loss: 0.0000, train F loss: 0.003172, acc 0.998800\n",
      "D_acc: 1.00 8.03s, epoch [1077\\200]: train D loss: 0.0000, train F loss: 0.003179, acc 0.998700\n",
      "D_acc: 1.00 8.04s, epoch [1078\\200]: train D loss: 0.0000, train F loss: 0.003107, acc 0.998900\n",
      "D_acc: 1.00 8.01s, epoch [1079\\200]: train D loss: 0.0000, train F loss: 0.003125, acc 0.998900\n",
      "D_acc: 1.00 8.01s, epoch [1080\\200]: train D loss: 0.0000, train F loss: 0.003094, acc 0.998800\n",
      "D_acc: 1.00 8.02s, epoch [1081\\200]: train D loss: 0.0000, train F loss: 0.003011, acc 0.999000\n",
      "D_acc: 1.00 8.01s, epoch [1082\\200]: train D loss: 0.0000, train F loss: 0.003210, acc 0.998800\n",
      "D_acc: 1.00 8.00s, epoch [1083\\200]: train D loss: 0.0000, train F loss: 0.002923, acc 0.998800\n",
      "D_acc: 1.00 8.02s, epoch [1084\\200]: train D loss: 0.0000, train F loss: 0.002972, acc 0.998800\n",
      "D_acc: 1.00 8.01s, epoch [1085\\200]: train D loss: 0.0000, train F loss: 0.002896, acc 0.998800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 1.00 8.00s, epoch [1086\\200]: train D loss: 0.0000, train F loss: 0.002948, acc 0.998900\n",
      "D_acc: 1.00 8.01s, epoch [1087\\200]: train D loss: 0.0000, train F loss: 0.002909, acc 0.999000\n",
      "D_acc: 1.00 7.99s, epoch [1088\\200]: train D loss: 0.0000, train F loss: 0.002913, acc 0.998900\n",
      "D_acc: 1.00 7.96s, epoch [1089\\200]: train D loss: 0.0000, train F loss: 0.002844, acc 0.999000\n",
      "D_acc: 1.00 8.01s, epoch [1090\\200]: train D loss: 0.0000, train F loss: 0.002823, acc 0.998700\n",
      "D_acc: 1.00 8.20s, epoch [1091\\200]: train D loss: 0.0000, train F loss: 0.002870, acc 0.998800\n",
      "D_acc: 1.00 7.99s, epoch [1092\\200]: train D loss: 0.0000, train F loss: 0.002829, acc 0.998900\n",
      "D_acc: 1.00 7.97s, epoch [1093\\200]: train D loss: 0.0000, train F loss: 0.002780, acc 0.998900\n",
      "D_acc: 1.00 8.02s, epoch [1094\\200]: train D loss: 0.0000, train F loss: 0.002834, acc 0.999000\n",
      "D_acc: 1.00 8.00s, epoch [1095\\200]: train D loss: 0.0000, train F loss: 0.002744, acc 0.999000\n",
      "D_acc: 1.00 8.01s, epoch [1096\\200]: train D loss: 0.0000, train F loss: 0.002749, acc 0.998700\n",
      "D_acc: 1.00 8.01s, epoch [1097\\200]: train D loss: 0.0000, train F loss: 0.002764, acc 0.998800\n",
      "D_acc: 1.00 7.97s, epoch [1098\\200]: train D loss: 0.0000, train F loss: 0.002754, acc 0.999000\n",
      "D_acc: 1.00 8.01s, epoch [1099\\200]: train D loss: 0.0000, train F loss: 0.002713, acc 0.998900\n",
      "D_acc: 1.00 8.00s, epoch [1100\\200]: train D loss: 0.0000, train F loss: 0.002749, acc 0.998800\n",
      "D_acc: 1.00 8.00s, epoch [1101\\200]: train D loss: 0.0000, train F loss: 0.002754, acc 0.998800\n",
      "D_acc: 1.00 8.03s, epoch [1102\\200]: train D loss: 0.0000, train F loss: 0.002786, acc 0.998700\n",
      "D_acc: 1.00 8.01s, epoch [1103\\200]: train D loss: 0.0204, train F loss: 0.070058, acc 0.984600\n",
      "D_acc: 0.99 7.96s, epoch [1104\\200]: train D loss: 0.0501, train F loss: 0.261280, acc 0.930200\n",
      "D_acc: 0.99 8.01s, epoch [1105\\200]: train D loss: 0.0349, train F loss: 0.099288, acc 0.965500\n",
      "D_acc: 0.99 8.00s, epoch [1106\\200]: train D loss: 0.0271, train F loss: 0.039564, acc 0.988300\n",
      "D_acc: 1.00 8.00s, epoch [1107\\200]: train D loss: 0.0206, train F loss: 0.018537, acc 0.994300\n",
      "D_acc: 1.00 7.96s, epoch [1108\\200]: train D loss: 0.0166, train F loss: 0.009994, acc 0.997100\n",
      "D_acc: 1.00 8.00s, epoch [1109\\200]: train D loss: 0.0156, train F loss: 0.007582, acc 0.997600\n",
      "D_acc: 1.00 7.99s, epoch [1110\\200]: train D loss: 0.0148, train F loss: 0.008302, acc 0.997700\n",
      "D_acc: 1.00 8.04s, epoch [1111\\200]: train D loss: 0.0084, train F loss: 0.006147, acc 0.997900\n",
      "D_acc: 1.00 8.01s, epoch [1112\\200]: train D loss: 0.0148, train F loss: 0.005496, acc 0.998000\n",
      "D_acc: 1.00 7.97s, epoch [1113\\200]: train D loss: 0.0149, train F loss: 0.003702, acc 0.998400\n",
      "D_acc: 1.00 8.03s, epoch [1114\\200]: train D loss: 0.0170, train F loss: 0.003873, acc 0.998200\n",
      "D_acc: 0.99 8.01s, epoch [1115\\200]: train D loss: 0.0283, train F loss: 0.003989, acc 0.998000\n",
      "D_acc: 0.98 8.01s, epoch [1116\\200]: train D loss: 0.0668, train F loss: 0.001955, acc 0.997900\n",
      "D_acc: 0.97 7.99s, epoch [1117\\200]: train D loss: 0.1019, train F loss: -0.003357, acc 0.998600\n",
      "D_acc: 0.96 7.97s, epoch [1118\\200]: train D loss: 0.1372, train F loss: -0.006452, acc 0.998600\n",
      "D_acc: 0.93 8.01s, epoch [1119\\200]: train D loss: 0.2059, train F loss: -0.013252, acc 0.998700\n",
      "D_acc: 0.95 8.02s, epoch [1120\\200]: train D loss: 0.1699, train F loss: -0.010312, acc 0.998900\n",
      "D_acc: 0.96 8.01s, epoch [1121\\200]: train D loss: 0.1402, train F loss: -0.008329, acc 0.998700\n",
      "D_acc: 0.95 7.98s, epoch [1122\\200]: train D loss: 0.1606, train F loss: -0.010164, acc 0.998700\n",
      "D_acc: 0.96 8.01s, epoch [1123\\200]: train D loss: 0.1417, train F loss: -0.008330, acc 0.998700\n",
      "D_acc: 0.96 8.01s, epoch [1124\\200]: train D loss: 0.1337, train F loss: -0.007512, acc 0.998800\n",
      "D_acc: 0.97 8.01s, epoch [1125\\200]: train D loss: 0.1048, train F loss: -0.004867, acc 0.998800\n",
      "D_acc: 0.98 7.99s, epoch [1126\\200]: train D loss: 0.0961, train F loss: -0.004026, acc 0.998800\n",
      "D_acc: 0.98 7.96s, epoch [1127\\200]: train D loss: 0.0906, train F loss: -0.003421, acc 0.998900\n",
      "D_acc: 0.98 7.98s, epoch [1128\\200]: train D loss: 0.0905, train F loss: -0.003327, acc 0.998800\n",
      "D_acc: 0.98 7.99s, epoch [1129\\200]: train D loss: 0.0859, train F loss: -0.002827, acc 0.998700\n",
      "D_acc: 0.98 7.95s, epoch [1130\\200]: train D loss: 0.0844, train F loss: -0.002541, acc 0.999000\n",
      "D_acc: 0.98 8.21s, epoch [1131\\200]: train D loss: 0.0896, train F loss: -0.001089, acc 0.998100\n",
      "D_acc: 0.98 8.01s, epoch [1132\\200]: train D loss: 0.0851, train F loss: 0.019623, acc 0.992500\n",
      "D_acc: 0.98 8.01s, epoch [1133\\200]: train D loss: 0.0825, train F loss: 0.043712, acc 0.986500\n",
      "D_acc: 0.99 7.95s, epoch [1134\\200]: train D loss: 0.0596, train F loss: 0.032749, acc 0.988100\n",
      "D_acc: 0.99 8.00s, epoch [1135\\200]: train D loss: 0.0450, train F loss: 0.023851, acc 0.991500\n",
      "D_acc: 0.99 8.01s, epoch [1136\\200]: train D loss: 0.0379, train F loss: 0.017444, acc 0.994400\n",
      "D_acc: 1.00 8.04s, epoch [1137\\200]: train D loss: 0.0292, train F loss: 0.009533, acc 0.996500\n",
      "D_acc: 1.00 8.03s, epoch [1138\\200]: train D loss: 0.0274, train F loss: 0.003856, acc 0.998300\n",
      "D_acc: 1.00 8.03s, epoch [1139\\200]: train D loss: 0.0275, train F loss: 0.002849, acc 0.998500\n",
      "D_acc: 1.00 8.03s, epoch [1140\\200]: train D loss: 0.0305, train F loss: 0.001535, acc 0.998700\n",
      "D_acc: 1.00 8.02s, epoch [1141\\200]: train D loss: 0.0301, train F loss: 0.001318, acc 0.998800\n",
      "D_acc: 1.00 8.00s, epoch [1142\\200]: train D loss: 0.0264, train F loss: 0.001531, acc 0.998900\n",
      "D_acc: 1.00 8.03s, epoch [1143\\200]: train D loss: 0.0344, train F loss: 0.001144, acc 0.998800\n",
      "D_acc: 0.99 8.03s, epoch [1144\\200]: train D loss: 0.0418, train F loss: 0.000771, acc 0.998800\n",
      "D_acc: 0.99 8.04s, epoch [1145\\200]: train D loss: 0.0415, train F loss: 0.000514, acc 0.998800\n",
      "D_acc: 1.00 8.04s, epoch [1146\\200]: train D loss: 0.0358, train F loss: 0.000963, acc 0.998900\n",
      "D_acc: 1.00 8.03s, epoch [1147\\200]: train D loss: 0.0315, train F loss: 0.001297, acc 0.998800\n",
      "D_acc: 1.00 8.03s, epoch [1148\\200]: train D loss: 0.0293, train F loss: 0.001366, acc 0.998800\n",
      "D_acc: 0.99 8.02s, epoch [1149\\200]: train D loss: 0.0436, train F loss: 0.000634, acc 0.998800\n",
      "D_acc: 0.99 8.03s, epoch [1150\\200]: train D loss: 0.0456, train F loss: 0.000443, acc 0.998900\n",
      "D_acc: 0.99 8.05s, epoch [1151\\200]: train D loss: 0.0411, train F loss: 0.000445, acc 0.998800\n",
      "D_acc: 0.99 8.03s, epoch [1152\\200]: train D loss: 0.0401, train F loss: 0.000604, acc 0.998800\n",
      "D_acc: 0.99 8.03s, epoch [1153\\200]: train D loss: 0.0416, train F loss: 0.000484, acc 0.998800\n",
      "D_acc: 0.99 8.03s, epoch [1154\\200]: train D loss: 0.0433, train F loss: 0.000516, acc 0.999000\n",
      "D_acc: 0.99 8.01s, epoch [1155\\200]: train D loss: 0.0480, train F loss: 0.000218, acc 0.998800\n",
      "D_acc: 0.99 8.02s, epoch [1156\\200]: train D loss: 0.0471, train F loss: 0.000196, acc 0.998800\n",
      "D_acc: 0.99 7.78s, epoch [1157\\200]: train D loss: 0.0518, train F loss: -0.000097, acc 0.998800\n",
      "D_acc: 0.99 7.79s, epoch [1158\\200]: train D loss: 0.0616, train F loss: -0.000715, acc 0.998900\n",
      "D_acc: 0.99 7.79s, epoch [1159\\200]: train D loss: 0.0543, train F loss: -0.000186, acc 0.998800\n",
      "D_acc: 0.99 7.77s, epoch [1160\\200]: train D loss: 0.0563, train F loss: -0.000247, acc 0.998800\n",
      "D_acc: 0.99 7.77s, epoch [1161\\200]: train D loss: 0.0726, train F loss: -0.001183, acc 0.998800\n",
      "D_acc: 0.98 7.79s, epoch [1162\\200]: train D loss: 0.0899, train F loss: -0.002629, acc 0.998900\n",
      "D_acc: 0.98 7.77s, epoch [1163\\200]: train D loss: 0.0871, train F loss: -0.002589, acc 0.998900\n",
      "D_acc: 0.98 7.76s, epoch [1164\\200]: train D loss: 0.1079, train F loss: 0.017432, acc 0.993400\n",
      "D_acc: 0.97 7.76s, epoch [1165\\200]: train D loss: 0.1142, train F loss: 0.057807, acc 0.981400\n",
      "D_acc: 0.97 7.74s, epoch [1166\\200]: train D loss: 0.1072, train F loss: 0.026995, acc 0.988200\n",
      "D_acc: 0.98 7.77s, epoch [1167\\200]: train D loss: 0.0823, train F loss: 0.015519, acc 0.992800\n",
      "D_acc: 0.98 7.77s, epoch [1168\\200]: train D loss: 0.0835, train F loss: 0.010038, acc 0.995600\n",
      "D_acc: 0.99 7.78s, epoch [1169\\200]: train D loss: 0.0740, train F loss: 0.006478, acc 0.996600\n",
      "D_acc: 0.98 7.79s, epoch [1170\\200]: train D loss: 0.0849, train F loss: 0.003684, acc 0.997800\n",
      "D_acc: 0.98 7.81s, epoch [1171\\200]: train D loss: 0.0790, train F loss: 0.007148, acc 0.997300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.98 7.93s, epoch [1172\\200]: train D loss: 0.0942, train F loss: -0.001123, acc 0.998400\n",
      "D_acc: 0.99 7.77s, epoch [1173\\200]: train D loss: 0.0585, train F loss: -0.000522, acc 0.998800\n",
      "D_acc: 0.99 7.72s, epoch [1174\\200]: train D loss: 0.0537, train F loss: -0.000166, acc 0.999000\n",
      "D_acc: 0.99 7.77s, epoch [1175\\200]: train D loss: 0.0689, train F loss: -0.001222, acc 0.998800\n",
      "D_acc: 0.99 7.76s, epoch [1176\\200]: train D loss: 0.0614, train F loss: -0.000708, acc 0.999100\n",
      "D_acc: 0.99 7.77s, epoch [1177\\200]: train D loss: 0.0606, train F loss: -0.000616, acc 0.998900\n",
      "D_acc: 0.99 7.77s, epoch [1178\\200]: train D loss: 0.0561, train F loss: -0.000391, acc 0.999000\n",
      "D_acc: 0.99 7.77s, epoch [1179\\200]: train D loss: 0.0707, train F loss: -0.001222, acc 0.998700\n",
      "D_acc: 0.99 7.76s, epoch [1180\\200]: train D loss: 0.0810, train F loss: -0.002051, acc 0.998800\n",
      "D_acc: 0.98 7.73s, epoch [1181\\200]: train D loss: 0.0889, train F loss: -0.002572, acc 0.999000\n",
      "D_acc: 0.98 7.77s, epoch [1182\\200]: train D loss: 0.0901, train F loss: -0.002878, acc 0.998900\n",
      "D_acc: 0.98 7.76s, epoch [1183\\200]: train D loss: 0.0974, train F loss: -0.003238, acc 0.998800\n",
      "D_acc: 0.98 7.77s, epoch [1184\\200]: train D loss: 0.0954, train F loss: -0.003209, acc 0.998800\n",
      "D_acc: 0.98 7.79s, epoch [1185\\200]: train D loss: 0.1037, train F loss: -0.003929, acc 0.998900\n",
      "D_acc: 0.98 7.74s, epoch [1186\\200]: train D loss: 0.1187, train F loss: -0.004734, acc 0.999100\n",
      "D_acc: 0.97 7.77s, epoch [1187\\200]: train D loss: 0.1225, train F loss: -0.005400, acc 0.998800\n",
      "D_acc: 0.97 7.76s, epoch [1188\\200]: train D loss: 0.1310, train F loss: -0.006129, acc 0.999000\n",
      "D_acc: 0.96 7.76s, epoch [1189\\200]: train D loss: 0.1521, train F loss: -0.000014, acc 0.998400\n",
      "D_acc: 0.94 7.82s, epoch [1190\\200]: train D loss: 0.1906, train F loss: 0.085004, acc 0.974500\n",
      "D_acc: 0.95 7.75s, epoch [1191\\200]: train D loss: 0.1726, train F loss: 0.020059, acc 0.989300\n",
      "D_acc: 0.96 7.77s, epoch [1192\\200]: train D loss: 0.1371, train F loss: 0.002194, acc 0.996300\n",
      "D_acc: 0.98 7.79s, epoch [1193\\200]: train D loss: 0.0841, train F loss: -0.001821, acc 0.998700\n",
      "D_acc: 0.99 7.76s, epoch [1194\\200]: train D loss: 0.0681, train F loss: -0.000294, acc 0.998700\n",
      "D_acc: 0.99 7.77s, epoch [1195\\200]: train D loss: 0.0879, train F loss: -0.002056, acc 0.998800\n",
      "D_acc: 0.98 7.76s, epoch [1196\\200]: train D loss: 0.0894, train F loss: -0.002672, acc 0.999100\n",
      "D_acc: 0.99 7.76s, epoch [1197\\200]: train D loss: 0.0840, train F loss: -0.002454, acc 0.998900\n",
      "D_acc: 0.99 7.75s, epoch [1198\\200]: train D loss: 0.0827, train F loss: -0.002221, acc 0.999000\n",
      "D_acc: 0.98 7.76s, epoch [1199\\200]: train D loss: 0.0915, train F loss: -0.003006, acc 0.999000\n",
      "D_acc: 0.98 7.77s, epoch [1200\\200]: train D loss: 0.0941, train F loss: -0.003292, acc 0.999000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'total_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-0452ae04413c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{:4.2f}s, epoch [{:0>3d}\\{}]: train D loss: {:6.4f}, train F loss: {:6.6f}, acc {:6.6f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0med\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mtrain_D_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_F_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mtotal_epoch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'total_epoch' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for sl_epoch in range(1, sl_epochs+1):\n",
    "    # 构造新的source data\n",
    "    test_loader = get_data_loader(test_dataset, mode = 'eval')\n",
    "    pred_labels, pred_probs,_ = testing(feature_extractor, label_predictor, test_loader)\n",
    "    idx = (pred_probs.max(axis=1) > threshold) & (pred_labels != 3)\n",
    "    print(idx)\n",
    "    np.random.shuffle(idx)\n",
    "    add_data, add_labels = test_loader.dataset.data[idx][:mix_num], pred_labels[idx][:mix_num]\n",
    "    source_dataset = AddDataset(np.concatenate([source_x_paths, add_data]), np.concatenate([source_y, add_labels]), source_transform, target_transform)\n",
    "    print(f'add : {len(add_data)} new source size: {len(source_dataset)}')\n",
    "    source_loader = get_data_loader(source_dataset, batch_size=256, mode = 'train')\n",
    "    target_loader = get_data_loader(target_dataset, batch_size=256, mode = 'train')\n",
    "    test_loader = get_data_loader(test_dataset, batch_size=512, mode = 'eval')\n",
    "    ## 画出分布\n",
    "    labels_set, counts = np.unique(pred_labels,return_counts=True)\n",
    "    fig = plot_distribution(pred_labels)\n",
    "    fig.savefig(os.path.join(path, f'Ditribution after {total_epochs}.png'), dpi = 300)\n",
    "    fig.show()\n",
    "    ## test\n",
    "    df = pd.DataFrame({'id': np.arange(0,len(pred_labels)), 'label': pred_labels})\n",
    "    df.to_csv(f'DaNN_submission_{total_epochs}.csv',index=False)\n",
    "    # train\n",
    "    feature_extractor.train()\n",
    "    label_predictor.train()\n",
    "    domain_classifier.train()\n",
    "    for epoch in range(total_epochs+1,total_epochs+ epochs+1):\n",
    "        st = time.time()\n",
    "        train_D_loss, train_F_loss, train_acc = train_epoch(source_loader, target_loader, lamb=0.1)\n",
    "        ed = time.time()\n",
    "        if epoch % 100 == 0:\n",
    "            save_model(feature_extractor,  f'extractor_model_{epoch}.bin')\n",
    "            save_model(label_predictor, f'predictor_model_{epoch}.bin')\n",
    "            save_model(domain_classifier, f'domain_classifier_{epoch}.bin')\n",
    "\n",
    "            \n",
    "        print('{:4.2f}s, epoch [{:0>3d}\\{}]: train D loss: {:6.4f}, train F loss: {:6.6f}, acc {:6.6f}'.format(ed-st, epoch, epochs,  train_D_loss, train_F_loss, train_acc))\n",
    "    total_epochs += epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "h-cLCwhS5Xuf",
    "outputId": "9243dadf-3b92-4dd6-b95d-8bd26295b79a"
   },
   "outputs": [],
   "source": [
    "# 构造新的source data\n",
    "test_loader = get_data_loader(test_dataset, mode = 'eval')\n",
    "pred_labels, pred_probs,_ = testing(feature_extractor, label_predictor, test_loader)\n",
    "idx = (pred_probs.max(axis=1) > threshold)\n",
    "np.random.shuffle(idx)\n",
    "add_data, add_labels = test_loader.dataset.data[idx][:mix_num], pred_labels[idx][:mix_num]\n",
    "source_dataset = AddDataset(np.concatenate([source_x_paths, add_data]), np.concatenate([source_y, add_labels]), source_transform, target_transform)\n",
    "print(f'add : {len(add_data)} new source size: {len(source_dataset)}')\n",
    "source_loader = get_data_loader(source_dataset, mode = 'train')\n",
    "target_loader = get_data_loader(target_dataset, mode = 'train')\n",
    "test_loader = get_data_loader(test_dataset, mode = 'eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "yzpHbjz65Xun",
    "outputId": "c6318f7c-adb0-4e66-fe22-8166b3953256"
   },
   "outputs": [],
   "source": [
    "## 画出分布\n",
    "labels_set, counts = np.unique(pred_labels,return_counts=True)\n",
    "fig = plot_distribution(pred_labels)\n",
    "fig.savefig(os.path.join(path, f'Ditribution after {total_epochs}.png'), dpi = 300)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 801
    },
    "colab_type": "code",
    "id": "cnZsdKZK5Xus",
    "outputId": "289797a3-0784-4c0a-d666-19bfea64894a"
   },
   "outputs": [],
   "source": [
    "# train\n",
    "feature_extractor.train()\n",
    "label_predictor.train()\n",
    "domain_classifier.train()\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(total_epochs+1,total_epochs+ epochs+1):\n",
    "    st = time.time()\n",
    "    train_D_loss, train_F_loss, train_acc = train_epoch(source_loader, target_loader, lamb=0.1)\n",
    "    ed = time.time()\n",
    "    if epoch % 50 == 0:\n",
    "        save_model(feature_extractor,  f'extractor_model_{epoch}.bin')\n",
    "        save_model(label_predictor, f'predictor_model_{epoch}.bin')\n",
    "        save_model(domain_classifier, f'domain_classifier_{epoch}.bin')\n",
    "\n",
    "    print('source size {} {:4.2f}s, epoch [{:0>3d}\\{}]: train D loss: {:6.4f}, train F loss: {:6.6f}, acc {:6.6f}'.format(len(source_loader.dataset), ed-st, epoch, epochs,  train_D_loss, train_F_loss, train_acc))\n",
    "total_epoch += epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gnz5qZ4o5Xuy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#predict\n",
    "pred_labels, pred_probs, latents = testing(feature_extractor, label_predictor, test_loader)\n",
    "# save\n",
    "df = pd.DataFrame({'id': np.arange(0,len(pred_labels)), 'label': pred_labels})\n",
    "df.to_csv(f'DaNN_submission_{ex_name}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTYqUpv3m5bY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw12.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
