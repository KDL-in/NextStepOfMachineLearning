{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "TiUTLduK6R2P",
    "outputId": "97262d9a-bd56-4441-dd61-5418d45e138a"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "id": "Vd6bakGmQUEm",
    "outputId": "e74b373f-7c8e-4bfc-a5e2-83683d22c464"
   },
   "outputs": [],
   "source": [
    "# !cp -Rf /content/drive/My\\ Drive/hw12/real_or_drawing.zip ./\n",
    "# !unzip -qq real_or_drawing.zip\n",
    "# !pip install pytorch-model-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7JsoxYtmO1v9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pytorch_model_summary import summary as summary\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "      torch.cuda.manual_seed(seed)\n",
    "      torch.cuda.manual_seed_all(seed)\n",
    "set_seed(0)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "path = '/content/drive/Shared drives/shared drive/hw12'\n",
    "path = 'e:/p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j2PjWdhnIKMJ"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IZd8vC3oIeWU"
   },
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "2-9c0yKUmfyX",
    "outputId": "5efd6823-001b-485c-b90a-c4c59cfa3e2a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/4AAAB3CAYAAAC6y5tAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOy9aZRlx1kluuPc+d68OQ81V1lVUmm2ZEuWZWRZbWOMB8DtZ+MGDJgGHrymn5/fA5qGRTN0022a1UzdQMOjWfiZybjttjEzno2RbVmW5UFjSTWPWTnfvPM9J96P+OJ8+1berFJVplRWOvZatSoyzrlniOGLOBH725+x1iIgICAgICAgICAgICAgIGBrIrraDxAQEBAQEBAQEBAQEBAQEPDsIXz4BwQEBAQEBAQEBAQEBARsYYQP/4CAgICAgICAgICAgICALYzw4R8QEBAQEBAQEBAQEBAQsIURPvwDAgICAgICAgICAgICArYwwod/QEBAQEBAQEBAQEBAQMAWxtfth78x5qgx5puv9nMEXBqbVVfGmHcbY35pM54pQGGMebsx5jMbvMZ9xpiTm/VMARuDMWafMcYaY7JX+1kCAp5tBPtz9WCM+aQx5oc24TqPGGPuu8Q5e4wxq8aYzEbvF9CPML967vBMy1rG8APP8JrrnmuM+R5jzD9c7nMGXBoXs1vP13Hp6/bDPyAgICAg4PmCsFgd8HzB1Wir1tqbrLWfvMQ5x621Q9ba+Dl6rICA5z2stX9irf2Wq/0cWxHPxG4937ClP/zDblhAQEBAQEBAQEBAQEDA1wOu5vfp1/uH/23GmK8YY5aNMX9ujCkCgDHmh40xTxljFowxHzbG7PA/EDrMjxljDgE4ZBx+3RgzK9f5ijHmZjm3YIz5L8aY48aYc8aY3zXGlK7Suz7fcacx5lFjzKIx5g+prt5gjHnYGLNkjLnfGHOr/4Ex5nZjzEPGmJox5s8BFK/a028RGGN2G2P+lzHmvDFm3hjzWwPOeZkx5gvSH75gjHkZHRuX+jstdfmhde7zDqnvXc/m+2wFGGP+rTHmaWnnjxpj/rnkv90Y80/GmP8mdfG4MeZV9LtPGmPeZYx5QI7/hTFmfJ17jBhj/sAYc8YYc8oY80uBLnvlGNSPjDH7jTEfl7/njDF/YowZlfP/CMAeAH8pVOV/c3Xf4PkJ2Yn+6UFjyQXnDexTcuztxpjPyNi+aIw5Yox5LR3/hu4rg9qqMealMj4vGWO+bC5CyTfG/EtjzGNStn9vjNkr+b9rjPkvF5z7F8aY/0fSKcvAGPMSY8yDxpgVmXv9muT3uTAZY3YYN8dbMG7O98N07V8wxrzPGPMeaQePGGPu2OTiet7iYvMrc/E59LcYY56QMed3jDGfMpvg6rGVcaVlfcE13i196CNynU/5vkX4ZmPMIel7v22MMfLbPndO6UM/Oujcb2QYY35KbH5N2virxI6837hvzJrU4wvpN2y3SlJPi8aYRwHcecH1dxhjPmDcvOGIMeYddMzf54+NMSsA3v4cvfZaWGu/Lv8BOArgAQA7AIwDeAzAjwJ4JYA5AC8CUADw3wB8mn5nAXxEflMC8BoAXwQwCsAAuAHAdjn3NwB8WM6tAvhLAO+62u/+fPsndfU1ALulLP8JwC9JHc0CuAtABsD3y7kFAHkAxwD83wByAN4MoAvgl672+zxf/0kZfxnArwOowA0+98AZmM/IOeMAFgF8L4AsgO+Svyfk+F8D+HMAY1Ivr5D8+wCclPS/A/AQgKmr/c7Ph38A3iJ2LALwVgB1ANulXnrUB94KYBnAuPzukwBOAbhZ6vMDAP5Yju0TW5eVvz8E4PfkvGmxnT9ytd/9+fjvIv3oAIBXi/2aAvBpAL9BvzsK4Juv9vM/n/9dZCxJ7Y+cN7BPybG3y1jyw1KX/weA0wCMHP+G7yvcVgHsBDAP4HVSnq+Wv6fk+CcB/JCk3wjgKbh5VBbAzwK4X47dC+AElfMYgCaAHQPu+VkA3yvpIQAvlfSFdu1TAH5H+uBtAM4DeJUc+wUALXnuDIB3Afjc1S7br4d/uMj8CheZQwOYBLAC4E1Sv/+X/O6HrvY7fb3+u9Kylt9aAAck/W4ANelHBQC/CZm30bl/Bfcts0f6wrfKsbc/03O/Uf8BOCj2ydujfQD2ix3pSr3lAPwEgCMAcnIe261fBvCPcGPTbrixys+LI7hvzZ+TNnENgMMAXiPH/X3eKOeWrlpZXO3KuEglHQXwNvr7VwD8LoA/APArlD8khblP/rYAXknHXwngSQAvBRBRvoGbLOynvLsBHLna7/58+yd19aP09+sAPA3gvwP4Dxec+wSAV4hxSydjcux+hA//jdTD3WLgsxfkp4MC3Af/Axcc/6ycsx1AAmBswLXvg/sI/TUAnwEwcrXf9/n6D8DDAL5DyvzCPvAAdEL8SQC/TMduBNCBm+TuE1uXBTADoM0DCdyCzieu9rs+H/+t148GnPdGAF+iv48ifPhvtOzXG0vuA334D/jdwwC+Q9JvB/AUHStLX9kW+kpfOfvJ7E8B+KMLjv89gO+X9CehH/5/C+AH6bwIQAPAXrg51XEA98qxHwbw8XXu+WkAvwhg8oL7sl3bDSAGUKXj7wLwbkn/AoCP0rEbATSvdtl+PfzDReZXuMgcGsD3AfgsHTNwH0vhw3+Ty1r+vvDD/70XnBsD2E3n3kPH3wfg30r67Vj74T/w3G/Uf3AL97MAvhnyUS/5vwBaMBSbdgbAy+VvtluHQQsoAP536If/XQCOX3DPnwbwh3SfT2/W+2zk39c71f8spRtwHWEH3OoaAMBauwq3Or2Tzj1Bxz8O4LcA/DaAc8aY/9cYMwy3Y1MG8EWhty0B+DvJD7h8nKD0Mbh62gvgx335ShnvlmM7AJyy0iPodwFXjt0Ajllrexc5p6//CI7B9Z/dABastYvr/HYUztC9y1q7vNGH/UaBMeb7jLq7LMHt4E/K4UF9gKmAF/arHP3WY6/kn6F7/B7cbmbA5WNgPzLGTBtj3itUwRUAf4y1dRGwcQwaS/pwiT4F0NzBWtuQ5BBCXxmEvQDecsE4fQ/cQvCgc3+TzluA+zjcKXbsvXALKQDw3QD+ZJ17/iCA6wA8bpy72RsGnLMDbjyqUZ4fqzwunCMWTdB2Ai4+v7rYHHoH+ufPFsDzTrX8OcaVlvUgnLjg3AX0279B30Tr4XLO3fKw1j4F4J1wH+CzMpb7suVyT+Da/CCXjL7+gf659F4AOy6woz8Dt9jswb+9avh6//AfhNNwBQwAMMZUAEzA7UZ6cAeEtfa/WmtfDOAmuMHmJ+HoN00AN1lrR+XfiLX2G7pzbAC7Kb0Hrp5OAPiPVL6j1tqytfbP4FbUdl7gd7TnOXzerYgTAPZcYuLT138Ee+D6zwkA40b8lgdgEcAbAPyhMeabNvqw3wgQH73fB/Cv4dwpRuHoYb7dD+oDp+nvC/tVF852MU7A7WJOUj8bttbetImv8o2E9frRu+DGlluttcMA3gatR+CCcSfgijFoLEnxDPrUxRD6igO31RNwO/48Tlestb884Hcn4Nwi+NyStfZ+Of5nAN4sdXQXnHvS2ptbe8ha+11wCy7/GcD7ZS7HOA03HlUpz49VARfHxeZXF5tDnwGwi44Z/jtgIK60rAdhN507BEcpP73OuQGXCWvtn1pr74GrEwtne4D+co/g2vygcj+DteOTxwk4xjjbxqq19nX8CJvxHhvF8/HD/08B/IAx5jZjTAHAfwLweWvt0UEnG2PuNMbcZYzJwVH7WwBiWdX5fQC/boyZlnN3GmNe85y8xdbDjxljdhknPvYzcH7ivw/gR6X8jTGmYox5vQzkn4Xzb36HMSZrjHkTgJdcvcffEngAzjD9spR1ccAH+t8AuM4Y891S7m+Fo0j+lbX2DByV83eMMWPGmJwx5l7+sXVhTb4HwAeNMXc962/0/EcFztifBwBjzA/A7U56TMP1gZwx5i1wvrN/Q8ffZoy50RhTBvDvAbzfXhDqSurtHwD8qjFm2BgTGSdE94pn77W2NNbrR1UAqwCWjDE74RaQGefg/PoCNoZBYwnjUn1qXYS+koLb6h8D+DZjzGuMMRlp7/eZwcKtvwvgp40xNwGpUOJb/EFr7Zfg6uV/APh7a+3SoJsbY95mjJmSeZg/50K7dgKOMv0ueaZb4ZgC67EIAhQXm19dbA791wBuMca8URY+fwzORSZgfVxpWQ/C64wx9xhj8gD+g5z7dbFL/HyHMeagMeaVUg8tuI1fb3NebIx5k7T5d8ItDn9uwGXeB2f/xsQ+/p907AEAK8YJCJbElt5sjLlzwHWuKp53H/7W2o/BiYt9AG5yth/Av7jIT4bhPkAX4WgZ8wC88uxPwQnVfM446uZH4QQgAi4ffwo3oTos/37JWvsgnJ/fb8GV/1MQJUtrbQdOQObtcuytAP7Xc/3QWwnyQfhtcL5Mx+HoSm+94Jx5uF37H4frC/8GwBustX4X+XvhdpUfh/OHeueA+3wEwA8A+LAx5sXPystsEVhrHwXwq3CTg3MAboETLPP4PIBr4Xbx/yOAN0sdefwRnO/fWTiBq3dgML4PTlDmUbj+9H4MpuoGXAIX6Ue/CCfStAw3Qb7QXr0LwM8Kze8nnrsn3nJYM5bwwWfQpy6F0FeorcK17e+AW2Q5D7dz9ZMYMD+01n4QbpfsvTJn+hqA115w2p/B+dH+6UXu/60AHjHGrMKJmP0La21rwHnfBed7fhrABwH8vIw/ARfBxeZXF5tDyzzgLXCaWvNwmwIPwn0IBQzAlZb1OvhTAD8PR/F/MdwmS8DmoAAnzjcHN5+ahrN5APAXcPXmha/fZK3tDrjGL8J9Rx6BG6P+yB+gecNtcnwObgF05Fl4lw3Bq68GBAQEBDyHMMa8HU406Z51jn8STsX/fzyXzxUQcLVgjDkK1yc+erWfJSDgGx1Cez4J4HustZ+42s+zlWGMeTecUNzPXu1n+UaCMeYX4AQW33a1n+W5wvNuxz8gICAgICAgICAgYHMhLh+jQon+GTjtjEG054CAgOchwod/QEBAQEBAQEBAQMDdcCE05+Coy2+01jav7iMFBARsFgLVPyAgICAgICAgICAgICBgCyPs+AcEBAQEBAQEBAQEBAQEbGGED/+AgICAgICAgICAgICAgC2M7OWcPDoxYXfs2g0AMJFbM7BQVwFjjZ4syUt6Ehj6/TM/9cIjA/Iux4XBPKP7973eOpe3csDGiZ4b0fpK4vJNJgMAOHX8OBbm5y5164vCGGON6b8Eu3BceGyzMKgITF8p2jXnDX6Stb+5clzOu65/L2strLUbKriJiUm7Z89e91RypYTqhfuGTyeJhjJeXDqXprudHgBgZGQqzcsX8msvsA64PdTrKwCAVrue5o2PTafpjLRNmwyuuViesdGo0fW1vUeRMyvz8wtpXjajpiabJbMjBdPtdNKsZlPdCRPpL/5/a5MN18vk5KTdu3evf/IL/r8AV9QcB/8ozb0c9yruu5f4mZ568f60fn9ca7+faX86duw45uY2ZsfGR4ftru2uHfqm1+310uPdnvYNY1wbzWRzaV6pXErTuZzLb7c0QliH2lhE7RXwbYyuT0cjsd8R1YUdUK4x2Xy+Vy+O/UPT83M6uujxvnvZvv9cuq9C+8ey+YUlrK7WN1QvIyPDdmZmBgDQ6bn3igyNaX1jjU/T8w8YOJ9JF0htVl+zNGuPM8za431j4cA2Phj+UoPs9LroO5dsvRzgNnTi+Ik5a60a9CvA5OSk3bdvX9+9ub3wM6S2lNr+wCKk9HrtLJY23aP+6a/f/8tnbut4XPBj0GXNXfq6if7h+yVfv1gspukL73D06NEN27JiIWeHKgV3fXkH22dzBkFvyXYtn3fX8TYNAJJYbRWXu46Xery/n6ytj/4iNuvmXeRHl8g2A1L6h1mvI0p2t+veZXm5jkaztaF6iUqjNhrZLs/qH+Dil+xrg2bAXuk6HSaC1kHOyLyf6i2mMSLxY0dE9ovmTDaSOZmMe+5WPDewax7FcBuIe/5l9HhW54/2EmPb4G7sMuPls0iaSxuql2qlZCfGXdQ7P97adYyP70dxrLan773k3B71tyz1J34Zm7b3weOKt0PZSMu9rz34MYauHnN/XPt4sFQv/nswE1Fd07XYVg+yI3bAcd8K5hdqWK03B9bLZX3479i1G3/yDy7KTrbgJlkxNe5MooWTpIPm4A8dPwEwEU8a6LhdcyryVF59DXxAx70c7QL/+0sNMgkdjhI2ZjThk/extYaeW6JBpu7yCyOukX/7K1/+jJ9zPRhj0kFh4GRnwITyco4z+ocQf67+PuJFDjk7oQ/IKFpbxoMWC9yz+JkMN269vr9v/5MOntjpPbjTU4NKM931u51BIYUvD3v27MWnPnW/u7//wKWJUps+ZLoSMbTZWknz/ucHfyNNnz51HgDwhtf9SJqnH680cACw3vBQvXa7Gob3Cw98DADwxFOfT/O+883/Kk2Pjky433QojKnVcq+tLgMAHvzSp9K8Xk/Lq1wcAwD88Xs0jPMYLVhMTusiQ5Rxz3jm9Mk076tf+Vqarq+uAgDabXf9Vkv71ZVi7969+NznXL0kEqrVWq0XM8D2MAZ+iNn+MwYmratvGw8KD8sTEepDNCD4fsSDAX+AZTJR/3UwuB/3LdZicN9NFyv5WWjSoddyuPul37Tm2OVi1/Zp/NW7fxUAUG+7sjo7v5QePze3qM9aHAYAjEzuSPNeeNsL0/TMjGtvTz31VJp38vjRNF3sajsy0nYbDe17eRoZKyU3QSrQxDtO1o5ry9JWAeDYiRNpemnF9RdDHx5ZulY278bSiCZiEZ3LEwl/3x71d16fs9Je/ETil3/lt7FRzMzM4L/+9q8BAM7MnwYAFHM0prWpPWZdWUYZPd5qUbvJxvIevIgzeNz1iycRTbo47Sd+vJgaZbRcO2JU+eOUFyC1afOiZbQmTXN1dDqDP6gycjFLdcX2syt9v0AT0Hf+q3ccwwaxb98+PPjgg+45e+7evb5xRZ+hJWNanWxoHK+th/55rZaH/wADgKUl1y/n5ubSvL4F2zQENk3OCb7suLwnJibS9NiYG0P4Y3fQ73luYWlOxnVeq7kF6qkpHYMOXncwTfsW5d//jjvuGHjPy8FQpYDXv9rZo0LO3aHT0THYUpvzJjhjtG0Oj21P03tfcC0AYNs2zVtdUVvYaujY22i4xfz6qtqybqwfmFbqw9DAlsvQgkO6yMl9jseFjM/U5+chwv+exxWzTlo+qvqvz5Nsl3/2nNtA+MP3/B02imhkO4a/7z0AgIzYApMl+zRgLsyLMJEswvQ99zrjbcnqxshM1qWLNR3Plo+eTtPNmqu3uKRjQHdkTNOlcQBAJzec5iVUR0nXtacc2dVcU+/VW553j5pXu5yZ0LGzm9P7xjIX6l/w5EUrv4vl7rX4Rz+IjWJifAT/7p0uml6lUnXP1BmwcAGg03H2a2llPs3L0LiQk2ed7ep4PDG5TW9GtiGOXN3yJhrP/UeH3XfaaHkkzStEeq5fYOMxeKGh9rVl3PFcXp+v19ANMdt0NmGooLaJTC6aHb1WJm/k8dXO8nw4tZXWtev//JsfwHq4rA//KIpQlElKvlAGACRGG0SmR6tRa7+zBi8aPfMFeGT6Tli7qt33Adu3M9D//4U3S23NJR6AP2kj6gddPhC7RvPUY19Is/becF2aPv3YEQDA9Xe91F1nY5uXVwwuim6XBme7dvDv2/3itLd7GV4NW3uvyKz3ISSDN+8m83FvxNb5qPJ2r28Due9jnl/imTbIjTIO+q/ldxv8gkWnQ4tENJFKbZHVl7lmH33ITLl2NT01k+bFMU96aALnF0TIWPcZxswQAOArX/1Smre0+itp+o5b7wYAFDP6gb68rINIq+0MT6ulE5l2SyeYsXUf8ZWhoTSvWNQBc2n+fJr+xCc+4Z4Vipkd42l6bMK978x2Z7gfuP8BbBwWML5tyXPzhz9Z8UEf/n3LVIl8fCT6+/7FTrqA1FHc04kYBizARfRxAkr7CT3vLOd4UiJpS72Ud4MGLwrqrfhjNOcnAhm26bwi4ld2sWmwMOj5hTcZ6Dv08VSnD+us1NHiEW2Xiwtn0vStt9wOgN4DQIE+RjN27SpylNX2HGWpDuUjmsc6Sx80C0vuuY4e14lct6d1MDTk+lE3UYZN38eqGFK/cAMAmRx9oFI/N2nd004uzxSkvQxaBLpSrDZWcf+DbqHs5Lnj7vpG20qhoe9127SbtB2raVmeSLTvW/n4aXX0N7k87TxRufiP1mKxnObxh+qgcb/b0zqqywSMP35519fvAvFcoVzRe/lH8WwrAKjVBn80+zrkBdiYJpg9qa/iOh+yV4pGo4mHvvhlAMDRo24doUUsF/4A7omNi7LaXgoF7RN+FzxLbY93otjy+et2u2rL2C5pW6adrL4FmuiCK/aXbatJNlIwcGebGQ3x4AWcdtu1Od7lf7ZhAXTF3mdkQSJP7bxLY4Bn1rHdrw7pB97UjGPZ7jpwU5rXAS2A0di/sjALADj11FfTvMXzahc7MnZzwfcR+/zcgTdY6LD/oO/beLs0V7bv7LU3HrwJNWgXe+MwF9/ppzy/25uhhYFkwNieoY+BYqR2bW9L5zm3G9e2o45+9D18VDc4FufcR2ynRPahqvOnTNEtilWm9qR5nZldaXq56M6NZVMGADpHv5ym87KIuufWF6V5zRFlyM3R4q2fg/bPY3jDzf2vZbHxGkriBM1VaZvSjyMaD2Pu51IvhaI+/6kzp9L0nuooAF3EAoDVttqWURpPmi13T7Z5oxVdcOnJhtN8U+syE+l4lsjHeLutH+O1ho59mZJ8L9MYVKZ5VFts8onao2lelOX5nd7LdITFSAusvZ5eN2+dfYkSma8kgxddgeDjHxAQEBAQEBAQEBAQEBCwpXFZO/5uL8mtMETwdBBa9TbJml8Y2qbvX82T/54BJd9vitv1di8G7dj3bcvJThetGPfRYS+5K+KPr+NrQCtL+bbb/YlOPJ7mHT79sJ5adfQaU/KUqU1ezxzktjDAzy9Hq2l3HFSayfZxt0JVyOnxHO8KMs1YQrvGMVOAybcu3W2gcqczY+N3pchPkJ47llXGFi3+N3q6Inpy0f3u2GyNfkM3uORCNK80y/tu4lKYtdq+vX9yi+hLvGPvd4/inubdcvOr0rSn3sVd3c1h/+U8rVh6dwpeiV9Z0TLatdvtILzudW9O8z7x6b9P04ePPAEAuP2mnWne9DQzDeK+dwKAmNrF7r3u+jfceH2a9/nP3p+mP/6xT6Tpl91zLwDgnlfck+Y9ceShNP34Y48BAG6+ze12fPVh3cnYCIz3udXlaz14KV/MvpVwsYNEpxy0y+7OEXZAl3ez1u78stsBaGfH7wxliOLHLCj/LuzCYtgfzO8Mse8f+xQSVTbx9+13OKM/ogv+3ziSxKLedu/ekp3+Oq1unyD6fCLPvXf/C9K8ubPqLvIXTx4CAOzfr5TeYkF3CLpNpcL2unIPKvfhIVrVN/I7KutmXfvTU4cdi6vd0XrZsfNAmvaltlJnRgCxCzw9lsYRZuj076iJ1gVXDG/WRJ4+K7/fhB3/XtzDQs1Ri1dFFyRDU4eRjLbn20ZFC4B2QB5fIdqx7IaUhtSOmyy3QaLPy85OkiF3wtxaX0ve4SbCFGxGxjqyjcyQSRJPndR71pq6M9SRneJWU+uKeZg9op/mZTd5aER36VZXlKHiad4m2dy9ltOnTuHnfu7nAQDLy263j1kNfTRqGfMzeWL2ZJnl437X54YywO+e05nM2vpw+cJeIvvS9ywD3ArYbsaDmC3UNjzjIO6jAGs7PHhQ+/3111+/5v5RHzt082FhEXsfaLHB5YLuNCa06+eZCty2203duT076+xGrzyZ5mWnyb4MVdJ0POMoyUN5zZuk3cZE7GmHWDqmp20+EbenTpfoxG1tx23ZOY173CfYZ9nvpq8zEevLH7CjP8DNbn1trytDygBe+3R947Wvt4jHS3qYrLT9A2U9vrutLhgVsve7JtxubGZSWSeHytoeO2IPD+xVd45CUfvhiRNnAQDzh3XneVtJn7wlGk1HD+m3Ru/EkTS981bHHr3m4DV6/wYxGVpax34uPJDej74ZCzYTfk62KtpRvKPf5+LVcW0zT3U1UtZybcLZgfEhpeevJOTyRG6pidiPHM2zqiW14cvCXGsSO/eJp76iz1V0v58Y1b7J3ys5edaJUe2PbbJZy3XXXjJkj6rQ77GJirooeNeUVqLjaaOn6ajnjufEHSVr1meXhR3/gICAgICAgICAgICAgIAtjPDhHxAQEBAQEBAQEBAQEBCwhXFZVH8Dg6yoIHoaQYZEQLIsCPJM+TnrhQW5yF9rMIAS1KcO6tXnzVrKpByQ3zP9a+3zRwPCFQJAhpQVj3/mMwCA5ccfS/PGr1d6TaXqxI/gVdYvGeJlE0Dv7WllpSEVmrn3VqVx55qOEpOh8mOBLRilwHp9q1p7MJ3PKxkzBTCXU8pMR6g6IyVyFSBKT8WH5+I8Eow733Zt8Pf+Uqnhq0T7HOxFMdgF4lnQKoO1KuDXjd0NWEixn9oYr8nrE4wzaxWbz59VKlmOXDPGxhztiMVRFkhQb3zcCcW8/tU/kOYdPXI4TT/+xNMAgFtuUleDapVpV0IpImVvFlI5cewoAOA9/98fpHmPPvKIPt+kUrAgKuCHDqm4Sael73hgv+s7XtXfJpvUX6Sio1QIcb3TLq6K7+lwHD6prw45fI+nXLKwE13Xt8woyzRvpWtlMkIXjDiPROC8neMyGhCLzAygW7oD9F5pWLiL20RsojjpwuIi/vz9HwSgQlh1ol4//pi6T1UlKsoNN9yY5g2Xq2m623RiP1/7soqsLi8rLe7M2dk0vbTk6Hblgpbl+JjamZlp1192bVfRSVBkhiRx9TGzfXeaNzau1L+uCPAw1b+PFp2K+2m9Z/tE4Mg+pq5CZLs4sp7YSn/NSwtvXRqZTAZDIjR1ftmLlKptGhvWZ5027l33VXR8yS8orbgl7hQZEi3K5pkurnXQbLo+n9BYze3d28wOi2VSYeSK7oGt+64AACAASURBVBmy69DR2xI5gt2Uum2tV08d7xPAJN+amPuGuCtYqqpWW+nQtVVHX+1ssrhft9fD2bOOBsyq+h7cpiJ/byqjQVEM+lXW9Vp9gr9eiHQAfd/90fffulhXCPUS5+p4yXZX2+Q0RY5JRU0v8SybC5OqrnfERaFDFF8eL30fjWlsZ/eV2qIThCtUVKRvrKhjaJmmZEMTTphsbL+KwKGjba655FwIVuf1WrmOuhX4aUS2z62DXQadCN3crLpVnTujlPLYt3l2V2O3hr7xxo8xTB5fK758ORG6Lg2burx5DwVWx+dJo58rJ+tElbpG3GK/bT/Ng07qey/W1JZcI+6PPF5OT2sUi4KIH+/ZruPGtmkVmdsl6c899ESaN9HVeqvmnRjkSkNdDZbItW1sr3MNqWf1WZeaSu/v9ZX7WuHMPrF0v188KLThlcJYJDJ2NGSuVycXlbHSaJouZR1tPiaB2GqB3F3kuYuRutb0KeGTm57vGbal5daskcBs3ZVxL6b59Yh2uLzY1HJELu8g1zNxmeEgYay5N2JdG8hR8WcifZd6k34oQtQcijiX1zpu+rmSH4suYvHCjn9AQEBAQEBAQEBAQEBAwBZG+PAPCAgICAgICAgICAgICNjCuExVf1Xuz3gFf1LyzxJlZpB4LSv2XooKxrSxQSHYL4U+itqAnxNDLKVjR/SjPNGb0lx6/ixRq48/oRTUB/7+bwAAwxTbc3qbqjTWhV5z5in3m26bqBybgEGxjQcVHL9rj6j8p487WllklabISvtRpBTOrFy3zar+dN1YeI+GqPoR0b5ioT21lLEDQ+0pk/MUPf3N5CipYY9KPgcVuBTWoY3ZNYmNI056WBY17KaPJVzQ+LzJAFX//rjvRCsVGlStPpfmPf7kP6XpfF4pXNcduNNdi54lX1JKULHkqERLtbNpXo/osiPVKXkmpUZ/9BN/m6bnFp3bwEvuuDfNqxbUXeSBBz7rrjOiVLW3vPW70/RKWynPTz7tFFKPHFdXgJtvfnGafsOr3goAmJpyEQY++lefwmYgLecBse0xgNLaR0kd4KIRD4gxfWHat2NWlO67lqSZesw0S0/1Z/Q5NKXhiddT0vY3ZUo//35t1JV11ZnlGun7bQIdc2Wlhr/7yMcAaJSK0VGltLaIylwRWn+nqbZreFKp/pPjjhoYd9VVYGVZbe38wnyaPnbcRQvIkO0pFUjld8j1nf17d6R5Bw+oqvauXS6f23uOlNO7EvGB474zZXSQQnqOKOEcN9gmvp9SG2GbC0/XHhDd5QphTIRi3tEmex13305Ly32IqI8mdmW8jdxVZui9apH7Xdxjdxi16azMrn2O4jHXdLDguO0enOXroMeRAqg/JEKt5vGvQ/R8r9pfIPcCHmqyRAtuC5ez1dHf97ncyDPE3fVjK18JoihCpeLsuVfl53bGlHEfZSdDSv4RmNocr3nGypDSZQtFtT9lGU8y2bWuRu6PtZGUmDrs7UaP6LydvrRrB70elVefK+ZaV6R13cD8nKiPuvzsw9t5747QIjeSoYqWa0nG7kJe82649vY0vWPnfgDAyJi6LxiyL0MUgaQsbjMFmv8mRS2XFWERF9oU+SFWtyZ1d2X3GL3YzIybvxQL+huep5w84Vxce13tB1myBRlDY5iPkLFOZVw4Rm/K5CxJAIndnka0YXcYflZxlbT0XTOa14f9pu1uvHnJdi3/Ywt6q/y0jke7XiB1F+vvD1y7K00vLCwBAErk9hpTjPaKRG4YH9Mxhj1ki+J6tnevRrkBjXe9knuWw+eX0ryVVR0bOWCZ75s850l4ouD9meza864USRKj3nYU/FzOlWe3S7aD2nNe3KU7DervXR0XMvI87VV9V0NuNpbaZrHi2nMhovYMnUsb4861HZ17XHfNTWm6B1eG87PH0rwoom86UdjvkCtBJatz8ULR1WevxdFsyH2e5n/dprOJ3b4pmfbj8pArl8TP5S9SLWHHPyAgICAgICAgICAgICBgC+Myxf1sKr7hFyJ4USHLi0LRWtEsjhltLvh/zcX6dqIu+WBrMGgRisON1ld0VabTXRtPNGP05ErJrVJWSISoSBdbPqmrPXPn3W5oaZfG4zx9TBkB9ZYr8v3XuNiym7AhA2tt386iz/MYtOvD8b35Xf0KU6fLK7NrdyUBoGe9OJKupnUpznE+7941yujqNAvY5CQ/ph2ZQoaeO5E6IsUkQzvTsY9Dmgxe2urjPCRrd3b7dzjl+CZuBSwszOJP3vtbAIBez134ta/5zvT46KjG6GzLThPv0PKzdmXno93W8t82c32anpxQ4TFf3rxrODGuQjJz80747L//wX9K844eU3G/17zynwMAdm67Ns1b2a9CMMnTTkzx9CkV+MkZXdGcmnG7rbe/SHctKhVlOiwtKWvhzhc51sDiiooP9j33hGMflMpuxZvLZzOgZczx7gfviKd5fTHUxXYQeyNaTxRHdg76BY4omVnbBlnszMg6bYZ2IKL+PX/5X/tYu6v9xbN8OK45M3DAZet3q2i3xwwwynagIb8yWBALSHYq8wW1o7ff9fI0XZA2MndORaomx7WNlWV3skSxkCdIsG/3Dt05sR23as9jRp4YXTnZ5Rke0euPjqnYUF7ukeUY8lbLvbbi+kaPdrlzBYpjL0yOiBgdLH6XJxFNLzJkIxIJpfX7dMfO19smDDBJnGB12e2otBvuvXL0rJWMPl8s/WiURCm3kxjlyaKfP2heQrsxcYfGF79zTfXC4qh+JzFfIHWzPsEpv+M6eCfYMyX4Fywu5o+zaDHv/vOo05a45myzczTuFXOuvBLaxdsspI8njBULvUeprLbimn2OpcKsiRMn1IZ7lsiLbn9RmvfSl96VpkdGlH0zPu7Gm1xe20FM9Virud22BgmIddraJ3z/7tIu/8qqim4tiKDd3Jwyc86e1TFiacmx0dp9goY8d1ibZlv57O/4m9TedmVsaJOqV4XEL/2O/s033ZHmXbNPx/ZSQXbyeAeWbHy2pGJgRdnZLRd1txkUw7wt86/WgtbL/JKWq2cODlXV1lUp7Z+hVFH7t2OPzhOaTVeHs2eOpnl1qtfKEI8nMh7SeGYGzM82VZTRJrBt12ZSJirtsNoePZ8fJ+m7ZduY9v9bdrj+vX1C7U9mn87puj0dY7btcv0lIibFrbdfl6ZPn3Lsy4REY9t17S/zi64MR0a1Xos0ni2LQO1wWfOmC3r/Jdn9n41JeI7aY5Y3z6WN9DOWNGn9HN9/FzB9+goRmQxKefdukXyD5LPMONBymZ0/BwCI28pYKNKcJi+77AkJ0PaICpbjsVXKKz+kbbxIQoKjE/sAAO1Y62IlUVHFekOeoczigvSNI2N3lu7ZLaydU/Xqasf8zj4AlHJqJ6yMffSJhS4xGssFN8fvJpm+aw9C2PEPCAgICAgICAgICAgICNjCCB/+AQEBAQEBAQEBAQEBAQFbGJcn7mcNbNetFfTMWgJOTIJ2UUHivPbFf1Sk7LS+y3BMV80doEnVL/7nNUD6LsXCaS7dqBFdY1EpEnmJSx4RJSoxem5FaE1FvkNd6XKVnopFNIRGd75F9PdZVfywbUfduEEELMxmxsK8DPSJH1Ir6Ai9vkWx1EtEYzEk6uJ1UIpMPSFafhauDDqrKhLHInOVIUcdtLT+VCR/kUTiLPcaWtaWxJ9iiX1q1xHtGeTusRliV88UuVwJu7a7OONnzzlBu/NzGj98bHRnmu4KJZuprBmiimakXIbKSrdsD2m7O3XyXJr2lM2REaUstZtahvWaq49yVun/u/cp1eiaA466NzmlYmY3E+Vo/wtuBgCUKtoWHvrSZzX9RZfOkxjU3QduTtNjJFJUKTtBoyee+nKa95VHHkjTS0uO6ulZZ0vLSv3cEFKO+lqR0r6QtpLuszd0GW97Mn1tjdow5fa8nWKxSxZykTLO5Ij6mCUqrYjS1ClWryEhLy8cVKtrfzs9qzTO8UnX3nbu1HjzLChn7IAX56xBXSfa5OjYYpiGh107v/U2dRe55qYXpumHREBysakCPjt3qsCkr0+OEd0hIdUMiZdOTziKYbczWGAnk5NrURthqq0vwi7FCu6taj/udPxYw64SZHSjtS5RxrDIlOYXikJfb3MbY0FV+b1cczPGlzhJsNoQQSx5By8oBwB5ukUkrggF4iPuG9Zzj8fuuZokWNgiEaqI6KcZ6XMZ7i8siCoU04SplX3+FvIb6rvdHrulicCc0XGff56VsYy6I0yi40+7yW4yMt+JaayieMxD4vaWq2y2uJ9BsejopHWJTz0xoXb9X/7g96fpu++8GwDw5+99X5r3gQ98IE3fdKMbq174QrXVN910Q5p+6Utfmqb9mJshtywecM+dce5kn/3cZ9K8mNyO9l+zDwCwfbu68tRpnF+WOUOtppT0E8dPpenPfOZzAIDHHnsszcut8yyp4DGeOxioa5X/35II3siIxmvfs/sgAODa625L8/IFtfueer1KYmzzczqnLBbU1WZy0rnGDdOcDeTKacQNZGFRx9H3ve+DaXpWXFV37Vbhube85X9L02Pjzi6XaGwfGdV3uWb/rQCAiD4tTh5XV9fVmtKkSzIXLNDzs6hgKkA9YCy6UliY1IZZsQvE5O9zffNjB7vF7hjR+euOMffc3hUYAK69VsfWiOaypVL6xZPm3XCLCvGNTri5VG1Z3SKWzuk4HwmVfWybuhJ0cjr/+uJXnLtxdVjtarujNvjYgpsf9go6f8zyvi9PVETsHET15+mRgRchFlu2jqvtZUPsbUbGrhwZXnY7OC9uQO26tqXcmL5XR1zq8kXtA2PjOhculLTc8jL/TMhVIJPV9hhJ28xatS1jsZbbsNh4kycXjwp9o4jrRkJt6ExH52RnJW3sYBFi7luVvHtu09b+3GvRnGPI3SMrtuNi3zphxz8gICAgICAgICAgICAgYAsjfPgHBAQEBAQEBAQEBAQEBGxhXBbVP0kStBqOItlaFUV34t9US0qRKAs9kamkrKzYk99l6XhCAQobdYox6Sl/fZRHTWezPqargnUm02gCROOxpCZbkNjCBZK2HB7VmKoVyY9JobZXU4pptq0UtWbDnfPgw19N8175MqWo7hOqTnze0VVsb3Opf4MxwIWCCqtNsX67GUdluvZepfXtO6h0v+IQqVlLLNksUewMUcJbDUfXO09RD7bvVnpTNY17TXRj4hz1JO7tynmlxpx+WOO4z55TCpmH7VPu7SP+S+ZaJX85edNRrY7gvntfBwA4c87RFNsdbdf1htL1KhVHN66tal2sriqVKRJKdT6nVK4clXWZaPfVYadQWqb+yPGnjx9zccu/660/mOYVctofykOOXsT0pBKpBM+ede/CtLQxoi6+9rUuKkA3Ucr5sRNP6PNVNAJBvuBMkFdiBYBV6vvLNVdGE+OurdhNUJAdBHYXGURcS4jOlhC9Pkndg0itewDNFFDV/hzRJPm9s55nTHaqlajNabQcXe/84lNpXrOmlM2o7X5XJ0povaV1WB12jbzb1brs9fRZojZRZWVoyBL3OVvSZ41ErbbX17I2Dh/zetsORzU9eIPGzP3a49rfP3///QCA63ZPpXl1opH6MmzUtY22iErcJdqx2gai+meJ6i+v2CM3nJVlbdvDVWcTmbLa6xIFT1zBDHFKI6Lge9ecPrcLslPsdlCM3D2I8Z5GQgAA722XyV6a7vfMYZEIrTON5kPubRxvOopl3CT6/Y6qtqvcCefydDbWdpV0iNZMc4RY+KU9Ms7sOhh33D3a5MIxVNGyKgkFvjqsY1aTVODrLffcubw+S7un7cV7fnRojM4QXZuC2SCTd39Ui5o5Pa6q2tOivJ7NqqvXJ7BxGGNS9wYfveC6g6oW/i2v/pY0PSruM6MUnWLfvj1penjE2YVjx46kefe8/JvSNMdmX152fa1MLh89mlOdOuXGmEcffTTNO3xYI8fs3Oncjl7yEo0asG2HuuqklHJyDWTXNa+mffjw02keD+ccyWEz4oxfLqy1iLu+z7j6GR5VF4zrr9XICfslLniZFNm75HZ66qxTfP/SV3RO+clPqgvF/r170/Rdd7jrVkrapus0j/CRE/IZLZ9r9mob8JGYqsNa7nGi9Z4TG8XK6DGNyaWqe8eJGXUVqFM89SceOa7vOOT67TBFiyiUdN5tZc7jI0ltVi16VzsjETqiAbbeneDy8+THNzOkNr654spyLtKxZPeU9vkC1UHkr0FtcWpazy1KO5+fVXp/gcbe6d2uv5SG9TeHz2i9PnbYRUraf3BfmjdWV1v0yBeedO/HkdUyWscJRduCj7hC9doXbSH9npKyyGx8/zgyBpWcjIMyDud4Lk/3qMn9l5raR+wozV0kukinoba8WtU5T5vcA1drrryr00rVb9EY5D2XixSFpkVufBlplRH1h9IQu0O7OuxSZIvZUzoGPD3r6m1iSK/ZXNY530hG+0M9cTYvO6Q2Nzus98qJG3a56F0H16+XsOMfEBAQEBAQEBAQEBAQELCFcVk7/nHcRW3BrT7GDbfS0eV4vnt0N9dKLMEuxS5urOpqe9xzqxdRrKuJzUVdFakt6apHWVZ6C8O6MmhopSM/7FZFInobQ7sgPu52OUMrWLTDWWy7+1pawi/kVXgtbrlnPPPUIX3WsyfS9PljupuZk5X/HO2E7d6uu1IliQO6uujiT292XN9BOzz9K94unVC85A7t+B+42QnNfNMbvz3NG59UAZ5MVle+IlH3y2T0nrxDaEVsrH2LMgaqIxRvXq5lqa4MxRJP2SAk7nTuBq2XD/7BbwIAerEK2xlqBIYZHn3qJT7v2YWBTQXBOl23svj+D/9uejwhkZCbb3DCS9umrknzqlXdIehJ/NVjs7rbOzKix6emtY58G4hIeO3Qk7rz4tkBVepPTz/6xTQ9s8v16aX60TTv3Hlt70eOPAIAOH5UV++LZV09vvVWxxaZnVUxplNnlPXx4tt1F+n4CbdafvyUvte2Ke0vfqW84+3EJosz+p0hS7v4gzaILK0Ct4j5U19xO+4s+lWisuA41zmJN26KJNyWZxPs+gPv4CZW+2Ys9dmIdcX6+OxX0nRjxeXHZHNHSECyHbk20uho+VaKVNa0Dtyuu3es0bvyzsjQmNudy1Xl95uwq5bYBO22q+dFEfV67Am1uZ/61KfTdF3edWFO73vmjLa3RETimvT8jQa9CwvSSXpkpEp5tLMsO1492vldph3/YtExklptjimuaDRcHRZpR6+/HQtjbR02Eud7lk+XdjS7tP2fiDCTX+3fjO5iABSkPDyTr1dTVkm2SjuVYmdbtEs4Qsyj28ZdGS8dO5vmne/QWJ3Tc32c7QaxM8okHlvJuJ3rfFZ3sMco5rgXhxof1jFnrjOXpqOO7H5mtd7PEpOhZVx952nXJUc7Y3mj/XBiyp2zd9v+NG/XuI57M9L3F+YHt5Erh0mZIr6uh2l3i9HpuHeLaTf55Cm14b2eK+dXffOr0jzuPydOqA0vSp1mSWRybl7ZRydOuvFixw4ViF1a0vJ6+OGHAQAPfuHBNO873qRzjltuc7vgNRIG5l3mqakJeQ61r3Viy/GOP6efKyS9GPU5976TMjbf8WId9645sHZO1CZGUUxsu/kFN3c4dkSZGCeOKntitKBzsvkzbufy6NN6/Oyczo9qTddvYzIMt95xS5q+I/9iAEC5rP1wlJgWVZl3ZzLEkulonymI4PLwiO5Mj45P0XFtm3XZBbXEppskBpxHvFnicR5iWz27LCb7yUysRGxZidRLdxATYu6s2P0lLf9JYhcZEjsvyJjPczIqQlSrbhe306JdbGh78G0koT5wdlXLfXTUleuu3bpzPUZ9c9shZ2/rtHOdUF3YLonV+p/FzEbWtGdQWyN9L9oM1p+FEZaYn8fkIp47aXscKjkbn8sqgy9JiOk15GxxHA9mLBQLWof5spQH1Tv/DjKOxSTaOr+kdq4y5K5VHVK2UgxiUfrvIdLarE7qnKwozO/jp5S5VC0SW4cEgw1c3+3O6RhWKivTdjQvY1/iyiq5SL8JO/4BAQEBAQEBAQEBAQEBAVsY4cM/ICAgICAgICAgICAgIGAL4/LE/bpdrJx14jy26agfWaK1dogqmbOObrF85nSad+qpJ9P0uWNOqGnpnFLNLMUnzBBluyhiZRPblcZi8kr96Ao1NmLhM4oj3BUqUtJRKliZ2F85446XppT6fGtJKTG1BUfteOIBFVRpnlPq8/K8vmNZKD3fcp/S5Rp5vW6r6W48XfUxiLGpuJSQjT9eqCr3ZNttr07TN9ziqF5DVX1mTwEE+unxWaHSJyzaSAJAsYj9NGpKTeHf54RyY2j9yQygDSVM0yHRmOvvewsAYO8j70nzDj11VH+4ybTwy4UFxTsWmnTXqjvL8ZNKxxsW0UTb03bf7Wq5zsw4elC1qnVx5LDSoL1ICQBMzzh65dnT2i6XFvX4i/dfDwDo0fXPn1c6oBcPefBr/5jmNVeVgjY16Whn0zuVhpkl4U2v9Xjfy79Df99WanCzqc8ye8bR5c7PqeDJth3qtjBZmQYAfO5B9yws5LUZ8G5A1rIYFBkHT5vvkFjb0pk0vSwuDOS1geld2kYLRaX+QfpDl8UBS1qfPmZslKEY6Rn9fSZyBbttWvvY3JwKX55bdPXRJjtasuT6In0rT9TFfJli5XKcemmnnabS59vzSvtdmD0KAJjcdTsA7esbQWQM8tJ4jokI3Olzf50eX16mNl4VqjFR8s+fU/p4W9obx3jneh0aUvq28eJzVFb8O0+bj1ncb0X7cU5iAK+Q4Gsux35n7hlZuKrfJcu3QaLsr2PH/e/49/xePaFnRol3oxp4mctCFGVQFnplp+3KONPVNlSgGMf+SdpEDS2SoOlLZI5QeoG2uy+TbcqPKS047rmHf+zpo3rcKo2ylHG046lxtRfMn/WiiEldC6G5oPYjK8KXxRyJNdW0XgtZl1/Ma3+s5vS9q1W9136h2GZK+vwrGaW5nznt5gut2uYO+Maoe5xvE4Visf+EC5ItEiOenVVbNjPjaKOTk0rT/uAHP5Cm775bBX9f9/rXAwBWa2oTPv/5+9P0hz70YQDAS17ykjTPu/EAwHkR7D1Pwr2PP/5Ymt6xx9n9FRKQHSXXNu/G4QWGAaBh9PqXdnl8dmEt0PXtq+D6zi5yha2Mql1PpJ22yYbmCzqXve5a5345OaHvf+cdKg44Pab5ft585KTW64f/9m/T9KHjbrwqlNUWve5bv1Wv+yInRL1rp861LVG+vVsUu8WyILMvY0PzgWJF3W8mprVPnDzsvgGWF9VuDpHQX07asU1njRuvP2MMMjLGeBtr+/qI9m+fP0RC4pNlrZfJoiv31YbODY6c0LnujhntR1WZ15VKJOpK4mu+XKOcPktlRF2ohsacDaq3aQyr6rfX7j1if8gdr5pVW7l72j3rkXkaY9hORORiIKdERK+Prc5Ten68MVIWZuP7x9ZadOXbISs2lvsri/EOicDhEInclUiYvSLfpCskctyp61w1X9bfxSIM36ExytK8+Jy4trfJlfNDf6PuSTvELfX1r1E3npzV7+Bu1t0rprruGa2XHdsPuHchl9Aop+Ph8ora1/yojGfsDk2uI3HB5XfEnadvHnsBwo5/QEBAQEBAQEBAQEBAQMAWRvjwDwgICAgICAgICAgICAjYwrg8qn8SoyOx2XNeyTNSKtb8IY1LXhc17jOHVPF+5bTSRuvzjtqbTZQylCcVx5joI7mMu1dxjlQaac1iUdQ16w2leiV0vCP3iEmxt8iB20U5e2ibKiSaLlGTJT70yUe+nOaViWLRJaXf8ekZ+V8pf7WGvmNb4s9u3+HoQ5sTZ1kxkM5Gt/D0x7vu1vi5O6+/O01HZUfjXqnpO2WZfU9KpZGnGK5zr5T2BaW1LS+zWnVD/mfKE91MrsWuBBxbfu91zi3hTW9S6uJv/87vpek6xYP397Dr0MWeDa8Aay06Qt+bGndq/W/+tnemx2fn1M1lZsLFXs5FFb5CmoqFbjc9ReqhFBHi9ClVXF4U1xRjtCxfsF9jO3tqVpeoy8urSmWabLr73nfXG9O8VkvpbNWqayMd6k+lolIHfexSjtAwMqx0xNlZzZ+ecrSs73rLj9Dz6XMtiILpt792NwDgsYdU2XgjSEvWDqL6E21dlH97baVcNWtKT23VXVlniTqZIdvAXOu2xJFPckS3I8Vp03LlFscUD75H/UGKpQylYb5ojyphH5y+xz0ftXsSV0e1KxQ0ogs28kTlJ+XaVt29Y7upCrYdciFoN5z9yi/OyDNr+7lSWKsxm3ui3r5ACrYFKoqsSA9PTWq7qpB6fCJtsy8KCf2+QrHMy2VHLy0QpbtLLkvtjtQbK+lTWXiF8w65khWIJmkHpKI+qr64TFEbjNah/fuY2bbPNsRr0jWhrCcUi/lK0YstZlck2oIopw8TxTpHtF7vGRNHSonNUpSKvChR35jT3wxPaV00jZZhT5Sm923T8aM0pPTZp046ivDZ00oRL1R1DO/JuFEkSmu3pu3ZSnvvUdu9blKfKy91yFEyTFnLvTSqtOQJ62i5Ow++Is37wnm1I185+yUAwNxZdanaHBiaQ3iXFK3z+fPqQtWuOFs7OqplyFT8sTGXXyCa+b336vvsIhemYlHijpOS/8KCpqsS6SGf0zkdz3WMDO57du9L83hsW5x3c8kcuW92iQqfSDvPUuB1HsMjGnt8/+pT6362YQwgz16R8s6VlJqdyelzZ6Uv5ShCVoncgspCTWb3pNEx7TMVotLXVpztP3JGx6gFUn9flH48kuEIIxRJQ2jIHEGhTJTqRtP1mU6X3T+1T/RiV0fsslkZ0v67fceeND1/9qR7JnIhW6HID8Pi2mDTMRobhjERsgVRffcX7Gs3+kcv4/r9SJEil1GkqGrJjdc8zg5+lwAAIABJREFULn7lcY1OtLCoY9O+7S5dKmq7rFCEgKGKeyZ2/c1keQxx9+pRuedy+vtp+XbpkM9hlqIHbZtxdVCs6TyuSTY46eh1YykQS+2R3eAi7/qxia4zxhgUxDUh7rg2lNDYXSyrrfVUfXZpOkJurVlvv2g8bXW1XrvkcrQiczJu7636cppuR64fffIzOr9+jL5zzy+4a+3crWPQHbceTNPL4qq0THODczSXLEr0u9Ko9seI5nzLfREGJGIczR9bRud6Z5ddZICJnpvnJQlHZehH2PEPCAgICAgICAgICAgICNjCCB/+AQEBAQEBAQEBAQEBAQFbGJdF9S8WCjhw3T4AwLDQYIwhiidRsY4/6Sj+ebrDth2q6DlUdNSK1WWlTcyfV6rYaluvOy6qmqaoFysQxWvIM16IjtwmZcaUHkNKpFlSWawURAG5pVTXw1/9Upr20QAKILXsAik0DynV6qYXOwr97ptu0+NMVRd1yHLFuQIwDfHZQkyuCPv37wcA3HXXnWke08qitFy1rC2r7hNt07NI+yh8A1SE+/OIMpRS/Jlnb+i4RGvoE8hey8m/9xX3puknDz2dpj/0ob/Ud0hpSfwsz67Kr7UW3V4/3WZ6fD+lD6TpnpyXsOI7qel6ah1TkD01EwBy5I+xsOD61NT0DJ07nqa7bUc/miMl/6MntdyWV50i8J13arlOTGjf7cXuGXpdpS91SU3b11Epr+3q/LxS9D//xY+m6WbTXePG61+Y5h14wa1pOm65a1VHhN5V2KT+ktI/Xd9kSnQ/1d+lmy11aVpcVvr58opLD2WV1tejOuy0kjVpSy4WrWWi1wuNenVey7U1r2q0XVFrzSRERzZE2ZR3ajeV/lVbUNpaVyITFKfIdr5A6XzFKb0uIvcO7To9S4PKRajNY9Zfa+O+MhZAHLt3KFdcPXMUB5Ow/RZVb2r3hp6hXBH1+Iq2wfEJ7S9j40o/7UobKJSU0tpqaRlmrKvboWFV612tU73VXB3VligqDdmWrvA3e8TjjOlZfXZM1ERD6W5MisvStzL03lFGy8XbjCOHXX9jJfUrhbVJ6tZjhYrf7Wp/qFkt4zMt91yrZPZKFGGnIK/S6Wr5jQxrWcwQjbFUdW0zIopzj957fE9F7qn3qtMYvyhRLkp5otSO6/GlVXf8un3Tad7eUa3jlvSdelPnFeespjvLeq14wbWXz53XyD+zRT3enHPzoeaCUu83BQYwF4zZRZqb9Dpa/wvSpvftU3X5AwfUBcyPkVWK6HPrrTqPKZMadkfmZ9w/JygawL33OpXrTEYptIcOKQ16ZcXV/+7dOh6OVHWM+sjfuTEiovnC5KTWUySTgrhDfWYddXFP8e/11qe+bjZMZJCrSPuVeWtMlHh2s/Pzs3yWbSi7VEq0BnLBGDFqv/i1i2V3rxdcszvNe9nLdK43Ku4pq6va/+KG0usXZp374cgQUdr37kvTmY5EFSF3kh7N+2NP9Sf7l6c57vD41Jr08hJRp09rZJaCuFCpKdz4fM1GEaw8j2f1W6K5M9U+K26tO4e1DU6Pa98aHnP9bmq7jsHX79c22qAIId3E9ZPeqlbW3KyWeyxtc2lZx/vysPaH7LDrx8tEWT89p9dfFneOoWHtuwV6l7K4+ZTz+q7L9F2QUD9LfNujcSeiRubdONS2bFa0BddfOmJTMuT6zWXRFIX+bkff//Hj2m56kavf26/dmebVKPJCp6Vl4OcUy4sUASDS9miK7h1PndK6YveiRtMdP3JMo7zd83J1Zzkn7k9fPUJu7hkt92uH3Rx9ZVGfqdHRvlmk77BeTeaqFHGuSN/EKw03F42kLfUu4uYXdvwDAgICAgICAgICAgICArYwLmvHP8oYVKtu1bEsC0/ZFd1RskO6IlksuEtXKroCFce0gtNxqxHz51SUYZlikccURzluudUqFsbgXTMjK3NdUrJqNmhFU1ZALO3Ym4quQo7OOLGs4XHdtWvSSnZ91V2/3aOthYyucpUndTXqmhe5FfLRYV3ZBHjlxe9iu/tz3M3NxHqigdu2u3edIFEsXknOiuAHMyIyfQI6a593vR3/SMTlTLQ2j8/t+z2J8kTPMBYvi3x8z9u+N00vLuqK4Mc/7nYQsiSqw5daywfYOJy4n2t7/h2ZfdEnKCdLzZYExCyt+nv6RJYETzK0y14q63V3ys6M3/UE+pkEJ4653acvPvhPad7qovbDZt31039Yel+ad8vNt6fpmW2OqXBy9lCat2u7shd277gRAHD0pLJmvvbI59L0Mq1qzy0dBgCcOve1NO/kaRVSuW6/2/1vnHP9ubsJ8eJhLeDti/zPO6x9EdalkfRInKXdUnGWntiELNmDiEVlVvXc9op79g7tIPrdYgCoL7n2Wp+j68/pSnNz1aV7JCYUd/Rp2023atyiHf9uS5/bsxoyZEfLj5Jg1B7dRSpK3GDemY5ZiFD69OS0CPHEm8OesXK/jLT9HO0c8X6dZ9KsUvn1iCEzI0Jk5aqyseaX9NxdO1WozMdLnl/Rsu4Ru2DPTrd71qHd5LllFc8aF6GtHI1ZR0+ocKcXKTI5Ld8C7T5m/L2YUEHv3aQ6LkiyWNJdJku7GW1hvc2JqFuvu/FdzggJysb1v8mKG2N3jeuYUS/oWPtV2ZU4v6htpVbX3Yzt291uyO4xZSMdKOq4PZrVa2UzfudJn4XZOPtHXLnuHNd5RSuheMnTbgead3ozGWI+waUTYi6Zps5nslIhJYqx3KUdtzOz+qzNmttteWzpyTRvvqDvtdxwcxvb03LbDBiYlKXnGS+5HDEjaWz3Aq0zM1r209O6QzknQprbt29P8xLaAuV5gGeLLS9reRUK+TW/q61on1tYVKaUH3F37dJduWuvvTZNf/JTHwcAfOmhB9K8PDE3cjk35veNh31j+1rxy+dU3A86DvidURbj7RAToyOsnCyN55kBMd4Tsi8wNMY0dbyoLbt2Vq9pWe/ZqXZnuHK9eyayC9tndK6azbhnXF5S+7YyqjY03SUnW9viubZn99AgmiURuVJZ5yTTM67uVxZ1h/XwIRVJq9ZEfFNYZdwWrxiRQSLClP5dIhrXEhLy9MPk7gntQx3a3X7klNhdqtepirbByVG1NSuLbuf3+BPKejn5tO4SHz/m5jxzc8p6nt51TZoeP3iLu/+wipfWabyoSuz6fFXbUNwm4XQRmizntF3xVCrKkcCjsLuYYcfzo0Tm8Gkf2wzRRWh9FAqlNc+UIaaxFebSaFHr5aa9ascOn3RlWSFRxokhGhfovWKZ151b0DaYqeq7njzkymJ+TseIOENlAXePU6e1Dz7xpI797Za71wr1UUvjQq7tnqu3oNdcJJH6NlXyCw/uAwAYsg2rTZ7fufJoyDwvuYj4YtjxDwgICAgICAgICAgICAjYwggf/gEBAQEBAQEBAQEBAQEBWxiXRfWHtTDCD4nl/6imNB9LSn4tcQHoNZTm102UupEXykmHaHacLtC1MkLybDdWKY/iMgpdg2ONx0RFMkKL6qda0XuJqEqFYqPmOvqsxaqjva4uK7Xk3MmTaXpmu9JGcyVH/Wi3tVwS4nB69kWm0JG/ic69AQyiwA9CteqovRUS9GOKXl5cNDJEyWdKuRm0VtSnzWfWpNk9IButpfojWo/q79L8Znwr/zP+zTUH9qXpf/2OH0vTR446Wvrhw0q1KlAc1DhVKtw8wT9rNRa1jyuckNAj15lNaXwc413Tjbpr+yywZFlokc5NxUcor9fVvnHqyfsBAGeOflV/QwqKXrxvvqbUwieIHjUx4tr7gX0v1mehWNhLy44GGtO73njw7jR9nmJLl864PkUsUsQxUZc7jtY1MjIi522GM4ZNlSlTyiKL+1G5JSIS2mkp/arVIOE3+b9IMW9tk85dIlr/nEs3FtWO1ZfVHaW54vJbZFM7JP63uuLObZPwKbs+eMpom8WKiN7q22C2R3RAqqN4Vc8tDLlnMVkS46R3zIlrSXKdvOtm0DCtuhP492J6LteLf8eooM83RMJs2ZxrJ/mCNqxcXunxC/PaBq1Q+NnryhA9vCl9r0HjE/eXbN614Ty51hSJ0lpruN8ZGpMsuY4kcqk4puGY6YhURz0RM+uSu0ed2ktbqH+pzd2E7lLMRzi4240X2T37AACjRKOsUWzoc3XXHp6gPr5K75W72dmO6259UZrXOPO4nltX+mvJCEWaqJUFFgoUH4BMov2xSDbRD1VJVn+fobEmhqMg98g9ICZqfEfESXtdPd6pab2dWVF66OK8K4OFmlJ1T5FNjCVWe6aoc4xNgekXmwL6Xedisms+n6cKt92m4n01ESPLk2tDi9ySPnv//Wn6Yx/7CACgOqzziBtuVKFAI+PY3Jy2A6av3/fPnHDszTdfn+Y1ibY6PjYlv9H66PWI7uvdlsgmDXLRAoAk2Zw51uUgAlCUehFThLiP3k/x1FNbQmM4Cf356uxRe7JUr+yGZnsduafaj53T6m67bcLZQJ53shCfMVKu0GetkwC3b0NdGmNaRE3OiDuG6RP7pbl8TutzdMK5nExMqlvBqSPq5ldbcG4ivn8mvU1w1TAZoOjnUO69I+oPMb3XlAgc7pvScePInNbboyec3V1dVvtTgP5+26i2ze3Dri2ceEpF6B6+/wtpurbkXDSay3r9k7OaPjju3M0M9FkKfbHtXbrRoTZE40ZB+vQQjYdZFsak+rIiaNwn5E3tLZ3D+npdx634cmCtTd23M/KNkKdvlDa5k9QWz685fv0BFSw1sesPjz52OM278QZ1m9g5pmU4J+7AWXKdqeoUG42aG/uHRygz0nOzIkDI30VxS+dkQ+JSntAYVqayzkZuXGiTbVhZ1Hfdvk3ddEal3Y5OqrvHUlPd40/PeReDS8/Fwo5/QEBAQEBAQEBAQEBAQMAWRvjwDwgICAgICAgICAgICAjYwrgsqr+B0pYgtMeIKI/WqHrnwjkXC7x2VqlvO6+5KU23RDW/QRSOhOiNGVJrrUgMyoQUfbsdPbcnKpSdjlJjmOrlqUyWdKFzeVKxlPwu/b5cUUqeFWqtEsOBYaLzVkb03KxXzWwRrRNr44RGolRqN0MScwCY3lYgetVtt7p46aUhpZBkWD1TqCsRU+6ZSkiUGI2bvZaWBgBGOFR8rSy1kZSSuE5UAJNGBaCY1QPcAgwzHYmGc8NNSiP88Z/8KQDAz/3sz6V583Ok5OndErKbSfW3qaJ0SvVnZfS+c9fSEdnrwLvWJD2ij3IEAKJEZ4WenRA1kJV/h6Uhl3P6BC1yg8lKGUSJFux2il9bEDpggxTV2xR7tFJ217pm7836rHT9rz3yYJo+dvxpAMDB69Q2TIyrQuvZWRf/9LMPfgwAsEzK6xuBtw++3DnaApdb0nHP3aeO3hfr1/UX29bM+qyWS2uO1I9nHSWwTtEmGitaL526O96kSACrTS3XhrgYdIiOyFR4T49nFXO2g951JrFr+xgAGKs2LW64a7GSdyZLiuRj0vd6cnxTmP42fV7vzsDvxy5DPp59nlxfChVtox1pb42mlvVuUhAv07sszDo68vCIUifZF+zYYUcZ3LlX4/NOTagS9vx5N8a1KD7w5DaNoz3kKcwcRYT6Q9wRlybqz90utUGj6UzOp7WNxD1SMJc2kM9IJIFN4Prn8xns3eXGuNlzzoXv0SNKMZxd0nZjW+5d6nXqT3maFwidfJ5cVMoTqjJf71Jkn1V3LkdTYPqqpyW3rJZPKa/tIePbCytCU1n1uq4M86Q6zj5HmUTGBLKTozRj2jGkVN6H5t212lbzqhRveV76ZLaw8fpgGBMhJ/Gv/diZDApXAx2DuhSDfXRU5wG7d7s2e5LcGMfHlFb6sY99LE2/73+6iC833ngwzdu+Q+sxK34zJ08pdXtsTPvX9de737WJpr5ElPLxCReRYWhI+/QKuT3Z1G2Bxk1yA7naVP8kTlAX16zakusz7CJWKKx12fPufABAUx7kxRWT58ftJrub6fvlZewvs8sKz4ulDbBdt4YjDUm50TyvuarjmbfBnY723yZFjinIXiJN8/pcS5hSnhfq8siIRpiamdGIEkefdhEyFiXajXdl2xAMkMg3SxoNw2ifLeV0vLhuUtzZyMX4oSOafnLB2Z0mzw3IrufP6lxl94R712ysLmCzLZqzye/YdS0u6Lx8bMbNifITGinAGK1jH1WnSZT8oZK+l/+cKpKqfybmMDJkr/3/PLGmyah3t4zg779xm5YkFnUZJ8tDw/LMet3FJf2O7MjYUW9oJKqRqtqWIWn7UVbr5ZEnz6Tp7g6l+o+X3Ht1SD1/cV7fdVQmy2MjWhY9imAU5V1+tULREsjl/OySay+rbb1+Jav3/+oh194aSzqG7dypbWSIpiTe3SJL7k+VSNtIQeaiObGJF9vVDzv+AQEBAQEBAQEBAQEBAQFbGJcn7geg5wV1ZJc8T7vdPdpF74koTH3xXJpXX9EV4ZMn3EpwbVVXxQytMrK4yZiI7rVbusrZbpA4ko/FTatSnR4JjnidIzo+MkJCULLGZdp6zYjjJMvCUYt28U1eV13Gp3X3p+d3ZDleOy2veIZEupuNzYXfMGdBqAPX6e7TQYnVHZGIktViRSLCGLz7lJD6CbMXjJR7REIw/bHpvXAaiyvpvSL/sAmzM+hlZHU4SztGGdrRiWTlK8owi0DrxZS0jv/ZrW4H4Sd+4HVp3l/+7d+l6adPupW5+VUqjA3DauzdC3aY3VGFGZRJ53rWS4N27nkFvETtFb4+SLSqVdd+9tWn3Ar+7Kr2175d4Ka7b5Yq65FDuhO31HWiNKVR5cDsmNqXpqsVV9bHT2pM6zI93zV7VQRqbNy1x5h29ZZruvPzpYccO2D2nBN0aTY3q37664PrhWPSx6mIlJZPjthIHbEZzSV9rk6XdhXn9b06Egu2taB12K7rLlZL4rHXaednNdbrdntrd/R5NyuNV52sJ4LkjQPtLHf03Ay3PenmLOwZFbQ9GIl3HMuuid0EgR9A38f/z2whRkd2LRdIWAkk9LVtxu0isegOC2tGORIKlB2dbpaEj6hczy+59lgsa3sfGtOdl570nR6JWK2uat80MgBE1MbaVO6J7LgllNemHfGYd8zkHr2+nWtmCvTkXdeKhV4pGs0OHnrUjdfHT7hxo50QY4zG/byIGFVGtL8nJBa3tOB2Xr744D+meYd7Ou7uM1qf2yPZOaEdS7RojJdkjZiA1ZKWeyHjd/ZIXCxLApVe1JfmGn1iknJ9Lt+ERD73jI6l6ZN1VwaFlsZPH5nU3eolYZUUcpu71xJFBqVSUdLu2o26lmE/83FtHy0Wte5asnN7+LCKYuWv1+O7duk8olp1Y+vEhO7W8r2UNaB5M9t0nlSvu52uHs2TesRmm5527euVr3p5mre4qDvPK8uuz546rWJpLWLJ8LNwnT5XiDIRKkOujAoyV/TirABQJvFP/6xd2i2u0dg8ZKpyTWLokThgjvKz0gYqVb1+rq3jVU5sXKms8yQee/3Y0qVd7F6b2W7uWZNEn5WfJeNjsJPdtry/yAw0GTuKVf2GmNmtrKrZ047tNz/rdnvtJgnIGhk7ErELNq/PN9bSuUf90UcAAB96UN//VKLtvVl0/buV5TFG7UNE9XlOsodyan/aozvSdFlsXGlG623btSqAOjXlzs1U1K4ukdBoW+xSsahjlGFWh2dxkmg6SPQ0pt19K4JzMbEAWIA7GjCP2CgSm2BV5lKjE46Zt7qic85mXQUgc8LQymZYzFdt7fR2l745r3Or46fULi+uqB0ZEns8WiSGCxEdOiLsukTMzHPnl9N0ueL60fABbRcxiSZOy5wpM65jxZNH9VqLZ129375bv2vGhjR9bknn7fXI3bfeUPZDTN/MeS8UuOrOuxjTKez4BwQEBAQEBAQEBAQEBARsYYQP/4CAgICAgICAgICAgICALYzLovpbAEkswg5e9IqEcZICCYCJQEOXaECz51Rg4fgpl+4xDWtIqRt7X3RXmu6IYMnRJx9L83JEku/U3T3axGxosliX0JLyROVskCBJRWhnDYrrvkzU3ZY842KTrkn0wfHtSk8yCSvNOUQJiQsJVdP21sbU3QguFPYZrihV7PUH9SblUx8FAKyc0HqJmN4YeyFERY/WhyKihPd8eROlialOVqhAHL8W5ILhqf6GKEXsjuFdJMpES4soFndWBLpyRJ/iuN3ZvNZnvuza5it26+/vevudafqv/snRG3/nb1xM6e4m+WAohVzqu0/hS5NezItX4vpo1uIikiOqap7em8Uqu21HZaoTVer0aRVssjlXBhMz+vuIhC9bIpxZJHGZBgkT1YRezbFNc2QHnj7yMADgi1/9bJo3MqSiPbfd/LI0vWf7LQCAsTGlDp86eyhNz55ytKZtU3sBAEceV2rnlcLCIhF6owomkfEgETkvqBITtZHF9eorrizYjuXaSuXCil63s+jafmtJj9eJ6l8XUZgmUVY7RGP2Yo5MY+1LG/8/2RsQpG8ZrBXyAYBOspb6l+kTVtNzs4m014y/5sZhjEFW2pGndbIIFYuW1UVc9ex5pc+WKqqE05EOPDWpFDsDEjKk+5ZkfKmdVzpgoaRtf1iod0NE9bc9othJPOFaR+s1IXWrsh8X2c5lmYbp7s9inXw8w+5BYlMjaqOZPqqvu9b4uBNlYxtxpUiQQSNxZduNXHswRN/PEU0yX0jkvkSv53KT39XqSqldWdU+sEzjx1LFvdeNkyRmpNUC3+UyJKjH4qdtmQOsrOpY3yH3p0rR1dEwUTNzRIX1sdLbRHuOKWb5aE/nOzkp9w49P1pUL1JfuWjt/GAjiEyU0nsToePWyR2qz3NOnqFI8yym+n/poYcAAD0SbiuXyKVxQvvX7bc7keBqlWJiz2n/WRYqvqF+cPKkzv9uvMGJuTKl/PycUlh37NgGALjp+mv1BWjusSCCbx/5+KfTvCb1X4YXPX0uKf/ZbBZT064Pjoy6uXBCInpdsut+zsjulT0aY7qSzlGfYtHOmHyBjLSBHAk28/yoKMJnFRJFzZLbqo8n3iR3s/qq2lhvw3i8i2i8SenFPKekUSjpc0Nz7bFA9PQquc+MTrjymz3vbEUUbdzNzyYWiQitRSIEHS0fT4+vPv6pNP3wsps/tXfepheokFupCMdaKj9LbngsENuR2O71PNn1caX6RyVXt9u3qzv0jmtvTNMFcaGwVO/dPhFfEVVk8WsaF3pe55TGlYjcjmKaa3qxSVbNZnHmRMYjnRts/CMmk8libMy5ApWHnE05+6S6HLG7cEFcMyzZ0kJJ27NvQ/W2CovmdqrtOr+i3z5PL7g2NUP9YWJI661WcHX8wuu1rj5OLkd7ppwtvX6f9rciCYRXC+Jy2NYxrlbRejtwvRtD9k6QAC6JBPvxFgCakauDxZbaSUv1Xcy58mj7MfAiH5dhxz8gICAgICAgICAgICAgYAsjfPgHBAQEBAQEBAQEBAQEBGxhXJ6qfwLEIqXbWXJ0hxIr+hLVf+cLDgAA5p9+Is2rE73eSszB1Y5SFV5490vS9H3f/p1p2tPBdl6ncdkPPfZ4mj573NGYY4ppmK8qdbgt7gb/f3tf1mTJdZyXtd+19+6Znn0GAAEQCyGQMEVRpkTKcsgR0ovCL5Yj/O7f5FdLctgvCiskhWRZYYdoi4soLiIJEuusPUvvffda/XAyK79C3wY1M5cRcCu/h+kzVbeqTp29Tn755RjiK358oPSlGVNG+r6ez0ZK5+h2HA2js6pxoF98+yt1emlbj5cStx1opUjuE6KTULUWEWcZUbJbweuvXK+P3VxXmsn3vvkjIiI6HOtzY6BfHeUSU/r0PYmIQqD1HOWuXNpAB0bl6ogVJ2dIEQYl0IC5wxCukzJUHuafdtsQs7ZQOl9B7vkRKMiuqkAsJaDcXTGdDpXqr11Xes8mx8rVCAELiBlbYbz45l+iJg2zjicPdZFlmgdR20VqJirnzqBv/fTHjmr/sx8q1V5i4RIRzbgOWhDrd4r0UI6WUM20D4QBKpa7PH70oT5z7wmonu5/TEREuwOl5V+/on13fU3VnYesQPrggdLtkDr2xbd/k4iIOssuT3/zl39Lzw2oF6EelkBNLAqktQlNFJS9K+3RA44ucniiSq+9DGhfE6VwFSNHwcJ4zhilYMKqtlN4fkb4XKbVN6j8GHGj+feTqM+j4vUZEQCq+jzQ/4GqHkvc4TpkyhkPfQp4RJQwdU3iTYfgQuI1lKIdHdAHmmUO4VOmPE+dDLUso0h/u9TT9nzzVdc2H95RKjK6QLx+y42l4l5ARPR4X9V2D49delZoIVy+cqNOry87ymqJCuYF0MelXHEbHimbEL+buL4wWoEHlMyYqfQhuwQl0MefHR75nmvTaRZwNnSubHfBnYDnUozEUkBpjpnyOIHIFx7US+5rvYx4DoZQ8/QvtjWufK9w/SmF+WeQK214wK552UDzOoVoCwd8/GSo1MooOu1CMYH+kkHs7ZOx3veDxy560SDTuZYO9XzCyuWthdSHwvd9anfdukum6eFY81AClV7aRg9osY8eaZv/9t99i4iILl/W9cwMXDVfeeXFOr235+jPf/u3Oh5PYX3V6bj+ef++zgH7+0pR3Vh3lOZ1iEveamm+dp84mvUNqO8Q+oSo1xdnKL2jorVQ/T9N5XrRCMKIVnieu8TRELqgyO5D/805f+hiMZvo3Cp9OAK3J3ThQXcocbmLG24BWkYhuxX54DLo+egK4M4nOFaCO8dw4PpkCu5oOEZLGaPbAoZqSmfousYJX68PY/2G6K24tiGuKV6g7eeZUZZEE9c/wn3X9rN3/5fmb6LrpOjNrxMRUfXCr9THvLG6KOUf/9hlP9c5PErUjaYxt3JEMr/Qd+31tFy3L7t33V7XwW6tr3XQjl0dHkL5TcHdTNxA0JPIg2hcGY9hOaxdfFCEB69yFLKPAAAgAElEQVQAqkjcyfRY2TAR81p2gf3J90Pqth0tfndvx+UV3AsiH6JQcLEmiZYlznMeR/6awLTpldpel9taBp3E/XY51GcdH+mFVy85t4HhB1rvLVgLL3fdvTbBBy2sNC/3Bu78NNd2sXoJ52v3DhNwEQ88cBmcqLvuMGUX24neX/JPRLS07MabiCsrDPUb+ZMwi7/BYDAYDAaDwWAwGAznGE9n8fdUPEKsTrjfivGG+2tbREQUdXUHqzrW3bS47XbmX7yhAmu//bu/r+dhJ0M2N19444v1sRuvvFmnU94dxV3SCqwMGQuWTCe6Ey6xm4lUnMSHOMwPfvBt/e3Q7TSuXded8Cuvq+gMWuWCOYIKGJ+1KDiuI++5VAsQxnA3EjEst4P0zttfqk9FlQrgfPSRq4NHqe6srsFu15TzV4EQzZR0Z20JY1GzpQot6xPY3ZVdtFkG1im9FYkxAjasaQi7mH2+12CkeWmBdWfM2U7AEiiMFCKiLsQpJbbGobhMK9b3zi9yO2XBQPIWEy++1ioUy0SjfWhZJSKW1Ygnr3mdZ7nDMKqzVNv+wRMXa7saP6mP9VGZjUVnBrvAavAwL65CUmRnYDx1tjAez0BIC6x+qxedEMqNW1+rj33ly7+p51cu1unDI9e3yvK05ZqIqGCmw3Tgn/rdc+ET/bTCcgfhpYzj66bwrj5YKCq2UD7eV8vZkwP97UVfx7+Qm9R4pOU+mWo7y9gKjNZgHFskx/PicROpdb6s5u/E1ywjvByFR30ogzlMgAB+K5YdL4wkU3Of+TTwPY86sVgu2HLVVgtuCf0l5N3+zpKOYwOwdMYnrr8kLbWqRC2t82UY025ef5mIiPJ4qz62t6c7/Nu3nADl3fvKSpkd6PwhrI8K5o8K2siMrSwp1HWObB4WhPOg/D2wxlQoVMtjArJ9AjDzjHl+Lnkyxuc8M6qKSh5fWrG7f9ICwT0QgJsxvSuH9jOear1M+Pw4BdFG4MT1YJxL2Tr2jzs6tt3saRtoM1VsCOJjh8ge4zaULOs9l2Gc2t9jIbOxtotDYARIfzmBd9k9UGZPta/5en/fHZ+CsFMIfUKYMcWi1Hzlvr5PHRaXlHFhOAB2FwiMbayxdR3K6Jv/55t1+sMPPyQiohdefKE+lgNjYHVVhde2t51Yq1jTiYiGQxA15fXN4aFarPC3Bweuf3U6YOFdUgGue3fcHPYY+sT6qp6XMRLHKRwXUchPnludUfbVHCbV8yJOErp683NERLS27ixxGQyp2Sw/dU0BYm05pAvu/2hhDYEZ026ByCRbEHE+awg88ljheWjzQwaYe24Fc1AAzwp4vA+CCM6jAOupWzbWuGgFFxFauSdRU2iwxQyJiMX/FlE/XjGj6NAxErP3Xdsvgd3b/pyKitOm6wdFAYLRic7nnlCRnui8kOMyCYX8Svmr/TGGtaqXsjhoiuxZFG106TEwLbQ3ESXM5EBxPxTDFXbAFJodrh9LEHf2ub5wFYHLv4r79qKWYkRufdnruXb8+KEbh3KY9/xQ26CsPYBoQimMLTRxa7bhSMdnIOvR7p7Wd4uZa6MWzN0JMGBCZ6m/dl372O8tq1B1h+ejIxDDvHNH28O4ckyw1qquLVZXtF2kvHYI27pOaXkg4D3VQk599xKNdRASazjt+9Jfzrbrm8XfYDAYDAaDwWAwGAyGcwz78DcYDAaDwWAwGAwGg+Ec4+mo/kQ1hUcERXwQS/OBslMyZbpKlMKQZ0qfXN1wNJl/+Xu/Wx/rbCiNBqm1odDigaaC9KN2t9/4S9SMb+hzOgDyio+iWUyJqIB+NbmrscR/cs/Fg+wAmwRpFCXkVSj8oPPUjM8qBchUtLPoZ08LofX0+85F4saLKu5XvafPX2bm7DE8N/E03WVK0azS8s0LqONAf7vqufJC0Z0xCGzJu/XgmhiedSwiFBCrsgu/bXMbOwKaDroViE5KBPTKKdLT4V4c4pLGQNtKId8vfO5VIiLa/q4TFrkzRtriAjGf6V8zz1Hs0W/EfGbqLrYlcCG5+7GKaD6469prOoNyb+m91tcdbandBroeKCxKXHOMExuD6JXQjFMQset2lZI95Touc6VaTSfaR757W11P7u84WtfLn3ujPnZh/WadFsbmvXu3iaj5zs+OisqS78NCen4OtDrgZB5ybOq7tz+sjwWl5kHchA4GSmc+eKgiRKNAj6+kHGsXBocpxB2fML0yrXC8mEdvBOE3jKFce5PMH1Pgl3Wqgv5YYXvkvuGHejACESivxSJyTM1eBA0zCALqL7kxPGBxqRz8x0YTEGljCvsI4uNW4Ca0+9i5uQxOlPb88KG6j+08UlGxOHHUvW5Hqcztnr7r/R03Fty+86A+dufOB3XaZ7Gfg0MdMw6ABn79+ov8fjqQ+Z62IQkdH/pAbYQ68kBZSZiyJZR3BvUtc03OlN2iPE0pfmp4REEsMeDZFQOemU6AVs30TBQmnYFomceN1APROHRpKqFv8CMpgzXEh+CCMeuw0GALBLV6Oqal7IqWQrvIYf6YckzuMZTlELrO4b6rzzHQb3dAxHQ20HecspuHB/05XFLasni7Fd5i5nuBH/jUZ/E1WZMdAL1+BPT7CfefH/zwO/Wx4UDPv/mmc59EIdcrV9S98d2fv1und3bcPLkCyos+zKci2ofCwMfHKoh5546bo65duzb3vUZMnX338b362NXLGku7itx8g5R4b148eWq6GMzDIin+gjCMaP3CFqfdWFLCvDKXJg3zvRfMscnBNbg2yKfaDit2a8Vx20NfSl7XYTz4CD4DskxchDSvgY+ZdfmKwSWnDe4aWSk0cBAvhJct4SWkjhrCueAOUXDbKeo2tIB6SkdU3fmuS0/cWNK+pS7E5YauPWRdH+Y65lTgehawmFo11Pk+H2obx+8hUcfDNdUAxDCnPE8d7Or11UzLJWaxzAFw1is6LfCIbhW4VhylrmAnOdaF1jvWAfG4id84AbiuSXWWtTva89dLVqT0ZN9R5KvKvWMC7t7LK+rmI8KW2MdPjsGlaM/R69c6MK5Df6mWdd1ali693NdjQQhrMhZf7sDaoJfocw/2WYgQii/p63wkottlvlsfC8f6Xj63sSd7cB7KvQWLsh4LX2bw3hOYb1qFzL0ssPkpU41Z/A0Gg8FgMBgMBoPBYDjHsA9/g8FgMBgMBoPBYDAYzjGeiurvkcZBL5lKNB0p1XINFDlTprQI9ZyI6LitlKCXvuhU57cuq8L3GFT3A6QEMT0lArVDVMcVVpGH9KYcFF6ZGoHq+h7QaQt2G4g9LA7dE8n4Aag+jGqKGdCiKg6IiXEvkRYZcPlpTOrF0MxqWj3H9F1bUXrkBKhuvcDlNQTV6CTQdMhq/hhndtRwC9DftphK73tKX6oKoAMzrbHtIdUTXAE4jezHLpRbQOwOAWUUQF47ASv7Ah2mgHQF9KUJ85MKcFuYgGrolSuO7vvqK46m8/gRxpVfHJBVOK/mGzRtSJdCv8J2mSmd+MmOUtGHR46ejnGBk7ZSmfp9UU7X9jyrtN5mrHg+K/T+qBBLTOONobsMIGjq3pGjWc/uf1wfW1u9UKdPJkovFXeM1aX1+tjDBzt1+mc/d7FI1zZcPOcGJe0ZUVUVlYVQ/V25FlMtq/ugyvrtv3NuCY93btfHbl1XVdckcGOeDxKz0QWNPe33lCI2OXB0vvS+UgPTVKl7GStoI5UrgwYjlFSkpmJse4/Hp7MoXnJZAH249HEchb4jkU5CbSPJCqij33C0286qi0+ObelZ4Qd+TfUfjF25zEDFeGlZ6X4Zc0IHoKY7gbjqUq/3S63LVbj+EGjHxOP+C7c0Tnmrpf3lBz9ydOMP3/9hfSwCOuDKmmu7CVA7P/5AXcX2Hzuq6Muvq1r65W1V+e0kXG8FKMqXOFdpVkv1CdJj0F4knfBc2YjG8YyoiGjKjzgYu3Lzc1AexoDPlUR7gOgsHaXHSghlvwtK4dAG13t6fJXV+CuIIOAPldJ5b+bKaw9iY3sjKCzp2zDvh9BOB6wY/xhcSJ6A4vPhvjteQlQZSvVdR+D6ItiAWO2tSzqm7bArUBBHp655HvieX0d8Efo3urf8w/e/X6d//q6j6v/jj39aHxN6PxHRG284d6sgxL6s7/vwoUYuuX//PhE1qfoYT/6tt94mIqKPPrpdHxP3ACKiEa8bl5aW6mM5jO1J4vrfxz9T95oS6rG76mjWs9n8qBU4j9YRSH4JlP6z4Ps+tRK3FpOxsQBFd3zXqpSITPPHUInSgor4MbhjSOQZIqJpkfIz9foIVMoDdsENYMyogFpccF5wzeXBsypxy4HwTHJPIqKcacbosugRRF6Aeil4vkuBvp7B90Q65LSs0xbhFlsUVI1cBI7OLY4odkHH5QqjpbCbVEBab+UE3Px47i/WdD0QTLXv0bGOJV7IkTcwwgHQtIfsWroLUTimh+AyOHXnk3WdN3qwphI1f6y3DJZK4iIwBrdkL1RKugcudRn/Jghg3Q1jvMxHgScRIhbQr8qKCn73VXYf8iKdg7G9SU7bMEdPRlruRc7lDt8dIcynW31YuPK6ejTRujpO9V6dZbeW29q6Uh/bufNenR7sud+G4DabtDS9zO5oSaxz4CpEGgq67nxwotFi0CUKXWh7Prc3UPrP29peY3Z1X/K4j3/Kmsws/gaDwWAwGAwGg8FgMJxjPJXFvyKiincdxbqeww4VKpZ4HAt7eKK7VitXr9bpm2+85X4HYhMRCIJgHFbZxSpQngrjlIqYGOxietVp6z3GVkYRDLk/7sIGsJvUWXY7NF6AO47F/LSUAeygecg0kFeo5JrFiP3IDnebd5umEJt0OlbLbZfjMAeglBiDdSZma2AL8nWUabl3QRwpZotJCFb4GARJevxbH620sDsYc+zQVggCVJnWm895QdGgEC2cubQLaEMNUUfYxeS2FYZ6/QzbHrMHrt9wO3tJfNqa8/SoagtYbTnB2KwNcb85AXDxB7LRDu+HMZZTEMvqdNyOX9jR9t4H8RKJj0tttUhtLquIU6fj3n2aIpND2/jg6JE7Nn1UH5tOtY0tt1lcxAdLX6jMn5fefKdOzzInnpJO9L1bEBf80mWXrxdfesndB8R1nhVVVVHO7zPgmNff+7/fqs9/+5sqPvjogWMt9Nu6e3ppTXfK476rj5Vlfb8eiJReuKwimxk/656vluODu2oFI2bmeFDWBYwtIqrY2GEHMTSv3oGHPtJoZO4PspGQQUJwnR+5fpisqkXu0psv1+nPf+PXiIiofWG18fvnheg4HZ+48asLQjkpxJZ+/GSXf6e78zEwvtaWmYmADCI43+9rG9vbvU1ERPt7ylDpwE778ZHLSwqCn722jg8T3q1f21D22pVtYLA8clbqd3+kltZ0pFauN19/nYiILlxSYSmC+a2CiM15lfJpjPGMrA8HERyMk+fvL2VR0fjE5Xd64vprt6v9AQV+xTq2tarjzdWLyvST4yhghA1yNtJx7PjElffB8bg+9hjU98Zjd93+EMWYtFzF0p+CRRStozInZBB82/e1XcxCl1cvBsZcV8fUGTDpKm7/rXUQiFzRMSEYi8jTAsQWEZ5HAYuxyjyJwsh376o43m99/TeJiOjito71G5ubdTpi0axOV+sLxVRRpHg8dnVy66a2WRTR+/nPHFPrJz9VQcDtbRXne53bPMab3z84gLRbN6JQIRJLWmP3jmjx90DweR4L5izI3LtIRoDvB9TtuLFL2FNpqmN5I+44sxTRQlfCmkmYYHkE4oCw5qoa4p7MbEQBV2D+BTzABiC6SD6O3Xwcxavht7LkyOFYUyQuPTNPLo1CoK4vpjlYawtgEK+5MfzWC7eIiOi7P7tDzwuvs0zBF37H5XvtMucfmcLIGuP52MexDtoYM2O8lo77PrADvBTyy+20gPZOwDSigWv75VTX7Q8n+txs5srotS/9y/rY5obOQT5/A+UwFqcwru4POV79RC3LfqRzQxTqvcpcmINQb/gNw3Wodb0IFmZBxcyN9x1md1ah5u9gX63gTx66cvUrYLQCW/uYGa8ETGScblJgBeepMCW0rNsrsOa5cMM9/+QJPF9ZSD6vv3JgJp6kuhbOeW5ZgfkyjbW82kybXV/XNWO/r8/HsSuMWVAf1mxxC5i6MzcmD45dHeM39CdhFn+DwWAwGAwGg8FgMBjOMezD32AwGAwGg8FgMBgMhnOMp+NnVlUt/hGK8AJSY6YggsEiakOg77/+2ut1OmKxsQLoymdRE4SCNQEaUIzCBTMWgQN61MxXGojEPI5CECkBGosweSqg1XZADK1iSnk21bwGFeZV7xVyOoX4ySiOInQuYZUtilwmNODJxNFM/uIv/ro+90Vf6XAhC3ZEQIPHkLEtppAhOy5AcUIooyQUIUCkSsFvW+58EJxBOSldWaDcUY5UeHYlwJi3gXeaNpYD1TUDF4sQVbH4vlk1nyKbDhy1cJvjuEbhYvbE/qmOHCL6ctbv57MQoQ1jfYrrC9CPJlN4n8JRqZK+itJsbWpc8+Wuo7s+3lU3nf6KCsnMLjrK5vvv/rg+lo5VXDBnSvbgBPog5HX7srr8eOzO8fixUqlu31HK9SHTP4+PHOWzWABVtixLGnP86v/+J39KRER/9Wd/UZ+vQNzlykVXLmmm49zOo8d6M6aPt7pKEQ6AogaMTBKieLoOwpsnOs7k7H4TzKAuYUzzuVOG0N/8hvuTvABc45+m/TdiSOM4CG2+u+poytc/r4J3n//yl+r0xnXnElOJENhC9H0qmrLY4XjqaGsVDE6jiVLojo5dafo+ugbhvVwZXLyobXx9Tel0x3tKeTw6dG2vDXS8bqzXbSy5sjiBYL0pUNLj0LXzfKaU9AtbSvnusPDPEdCWd+78rE4v9Vx7uXRVhdKWltVVIAHXl4LnlSbVf178cncsjPTaZ4VHRDGX8xqLsQUR0PthLpaY4BmsBR7dB1G4j10dDmHZkGfYnkHIj+mnewMttwJcWwqmpI6A6p+BqK+I1C2DqGMQa99LJPY10EQxjrgfMz2/pfcPQfhuDdzB9pn6PoJxeLWjbipJ3OL3Wyx836MWix9W3DaSlo4/t3d0XP3ODx3t/gbQ85NlcKmMec12pC4tRaZrrh64AHzp7bfr5wseHikd9wELAcYgdNWGMfKY6/TFCzq+hImOhXfvuzH2BLT7Zgcg/jxwfW4G9Pl2ArHGYdyVdaXnnTG7/hI0/3zfp27X9RXps2OgdnszHcvE7RTH6qbIL7vOwUvl6AoLlO6yYiFBmCYrEDH22aUxioFGXui4Vbs7eNgnFSm79qIbYAbj4ozdChquujAf4dzT6bt+2YJ6n4KrTei5/jPN3PgbhM/vflnFbaquOTfjlMU5I1jTBiCwWqVcLjC+VSDSVvBa2oOyDFd1jgkCdfWpnnxERET5QPtIDkKhxGUQR/Bd42m+xnxdCWLA2M8Lpqqj0PUI3C/3eB01gXrLKz1Pvj5Lvg2QZo5tT9xQFkHxF3h+SHHXrbVmPIbv76gw7/6eupXKmDQYqFvE6gq4rV5yc/fuI71mMtI2TiDCO5m59rq8pqKJ1y7pmLTLboBPJuqGhO0wI7eOmM20/KYwR+X8PZR6+vwhuEXGvmvj/VWdl/owTvqetodp6t57DO5NAWl9jk64joeuXMri7LWyWfwNBoPBYDAYDAaDwWA4x7APf4PBYDAYDAaDwWAwGM4xno7q73m1PGLAlJcK6Ev5VNPLm44O/Nqvfq0+tnVJ1WRnI0czOYtS3VCUFBp0Q+Qc1c2ZKgX7GEhfF+pKBXEzkaaSlq4Y8hRivwLl6IRjd06AHvqTj5TuW4K7QpWfjsWNKt0l0836HClgilERngNCE3u85yh03/qOUlO+8A4q3EpMVFCQRXo+/y2APhZAwQdAAWsx1ckHd44usLFirtsOqGljHNeUaYI9iEVZtTC+rDvfh/v32kD1jNxvB0DvzHJt0i2gIaasrrk3hneFtveTd51S6H7laEI5qu4+M7zTSsGnRdYbJ6rGEYirPocOWGRK+ZmNlfYUMw0XlXd9UPntJuL6AjRwoEzPxJ0HKGxxrL8dHjrKUQvikVY9oNDm7rwP7x5UmtcI3XS4n21sqCvBE6D9Cw3w/j0XNzrNFtBfqpJypszt7zoF2Az6aB8orSm31zG4+dAhxE0m188kljYR0eYGRFDItV6yiXMhQGXvENTjE44kkgNNOh0rRaycuOMhUCvRi8afw2QVVwoijScdxEA162m+uxtKTV677NS++9tKk84L7ROjAzfOtHpMkVtAjGWPPPKZdi0K42Og6E2moODNz4NQw7SyojTL1151USCuXFHF5elYqcKPbuv4XXAkks0rSu/3Cm3vJ8eu3iYjpUa2gMK8xJEHfKCMnhzq/SXKwtqyZnYV4jEXpbv/kwOlxHdBHb4MkOLq0hXUK5a8uGvUDEEM6P2M8AOfutxOhuyGsryibSX0tb0/2XH9aQzzR9HTd5mxOnQOlNkUohYsLWl7azF1uwUU7SQGKjC77mWZlvvRsdaRHN+4sKHXgwrylMeSKajg471SHrMmwE+eoFo6uPNFkcvrKuR/94mOYxL1pAvRghYB3/ep03HjVYcjnsj/iYhGM83jt773fSIi+sFPVGkf+8zqqqunGFwYkgDLHuZhHu/THKnDOq4IoxjXET7Ee49bLo8/f09dxDxfn7t9xbm9vMzRn4iaLiXHrFwt8wIRUY6RlEgha70prE8nE3XnEtX6hOeaalFjWR1b3eUbKe050O/FLQndSCKMlCGS5JCtFNacSPsvmIZdQFk0XGh53AhwPkMXiDr40PzrJZ1jvUNaIkxgEeLjux1VLJcIPRjP/gjWokfs5yGeb4sglntlRcHY3Tdgd2AfXERQtZ+4TyPVP4K1QciU7bIR4x5chNc0igWVrv/jmrPwjyHNazIY61chgsjknosIMwa3pww9aCXKGkTVegLjj0THqCKlxHtYopmOmyVXXvMbDHuUewdciy4CBX9n3L/v+vTRrs6h46G6H7W77BaRqCvFAUQEKVJxA9J2OYP+kMO6ubfi5obL127Uxx49ugNpNz5VibZbPwfXD8/VUQRj3zL4Lqe8JkinEB2lp+U+GrpxqCohQkGibQzLeDhyZYCRITz4NmpxeUgkH+9T6scs/gaDwWAwGAwGg8FgMJxjPJXFv6yqOnZtyDuSCQgVYCzxhHeSL4Wfq48VoOhT8W57QWh1BMERsGqJpecJxHJ8vKc7PEcnzpKDO2AnQ7UUzdiqHoYo4gPxRHmXdAq7Sp9b0bzsjtz9f/p3f18f+9/vaixHtOjL9uYlsB5d3lCLx49+7GJ4v/7aK0RENBiqFepZ4ZHucGcs6IA7dLDZRSWX90Xd6Kd12FlcZksWimqtQx11Ey2XhE2MKCGxDLtRIYtcVNn8vFzmHb4Id4fhXhGb89oben0P2QN8fgtjdYM4SgL1Lc8dDPRhK2ua1+9/4CxVf3X7m0REdDTQtvA8EIu//D3LliD1VTU230G067TmDuUYLxT6nlhGUm/+vl4+dEIzKAzy+LGmV1fdrnC7rYJ/g6HuSB4du74X5Lqj7EMs3pzZA1OIe360t1unC7AcizXUh75fgIXtnS9/hYiIKrZK/Jc//s9z3+lp4Hk+tdjy941vfJWIiNrAOrn74Qd1WkQAYxAFo0p35Q/2XbkniZbf0hIoUnlaR1HgjqMVrQdCLt2e27EtoQ8NoB1KXnKwfGewqx3wmOmjMCf045BZCcmS7ih319Ry2wOBmWTJ7apPoV4P99S6FvecZXNt+wV3YAFWsqqsKJ26EWC55waoCTDKIhBhq9pseV7WnfhXXnm5Tq+x9bLf1fOhr+PcBgyASeJ2/Vcgfi/GJPdZmK2/Au2W0OLF1hxP77/a03sJk6MCesLW9pU6PSlc2/8Q2t36llqLkljrS+KWYxzwBCy0Yq0V9pv3T5YXPRueRxQz+2fG48xoBGNrBn2frX9BoHkeVyB82XH56XS0rIKZzoFJH6whY2el6q2qlSqFMS+t3Lx+8aqyOihWK5pY75Oelg82Uym3EPMCbcRjC/fJsbbB8Vitk1NggHi8lEJG22ik71X9AvHWZ4XneTXbQMS48LkIsR6hBXcMjKKdnZ1T1wRghfZhLJH5TKy2n7zvdOTaQQxz8MMdtVa22hzDHYSqQmjHnZ4bl1566aVT+SfSd8yAMXlyqOvDEq2VnPzgA+1fBag4tltu3L1yxYnOziBPzwyPyBOGFec7iMCyjGvRXNZsMF/DWFfw5D8D9kYJMcwziLEubQDX0h7EKA8DlwfPw/kW3/c0s7DdOi0Qiv2ohPla1thTEJbD8kQ2Sd224F5hS8eNkuPMiwhftQgVxqqgsnBjRM5MXprofRusC17fesAyoiyC37KoJrAUyhSYosBg8Zbd90Cz/wMbhpmPbbD2dtv6rNm9fyQiosN97aN7Ryq+7CXCyNL+cPe+ztdMFqSwq4wEZPriKt6T9UdDCBvGUG5bom++CG3MdDalex+9R0REh/tOlG8y0XYzGmu6zS+zAusVDwT7jln4NwSmhoj4ERF1V/R77PNvfIGIiO7eUSHBd3+qwrsimt3rgRotYX9wzwjhuyQHtvdM6gMW7jK2ERH5njuPIqXjCbBEYZ7vdN04lSTAAIRnxcySFIagWfwNBoPBYDAYDAaDwWD4Zwr78DcYDAaDwWAwGAwGg+Ec4+nE/aqKCo41LZTxDOjE1b7SeUMWZ4mBppiHSmfwOMB1Wig15f4DFXP46ftKvfjonju+s68UiKNjoBcyxQwFT1DcROhPEdCMGsIULC4yBYrctW98sU5f5rjjP9lTIZonB0oru7Gp1JHXXnIxIL/6G1+uj21DbM9ex1FDen1HU4kjjGL//BBCSQWc+QrKYv2aq/IOCPaNgfJzN3X5mwBjaZoB9fgYXDCYIjWcgbgePCtl+qeji6IAABy1SURBVEoK52eYLyb2n8USDpmzHAZarz60tzB0F8YRCA4mer7f17Jd43Lf7OmxrY7Sh6b33Qs/2XNtAMV3nhkeUP1ZvMUHwb0K6uCTGoDuB5iW90LBQs3/yUDbrs/uHij6MztRSqeIcHqhUi+D5P06fcxCJh4MD8BMpIzjqJapPrMEgSAeImqRQSKiCYgPDodKx+2tOppuWUEdozInpy9fcdToOH7+uOREFZXsbrDOgnavfF5jWy9BPPejfefWkAOdEilkZSHUSD3W76voDApStZmitQQKmK2Wnu8subx4cM3KqtLChD45BepkCpRK4vHPB+ol5ivm57d6Ss9tgbhgG6iFCbv0RKAYmKc65k5Grg496e8L4DBXVFHObVfG7HYbhBJbGN9WqMZ6bDgYnEqvrytNfBXG4RdeUhe0nMf/Q6AK5+BqJnNYnkIcbqDPJiwOhe39+s3rdfrCBSdc+RjmDC/Ucl9ruTpOd7Vf3L97r04XmRauCDYhBfDqVXUb6DIdUMbJhrDXMyLwA+qxG4qI7w2BIh6Du1+XXURKEDualTrH+zwBDAZKU43BfczraFpiV3c6KoRYgq0il3YOglnLK0r/HDEVHymPKDgqunO9jtYF0qljps92l0DMydd3KWEsT1ksbYg0e3iWiH+2Ifb2IuCo/iyEOZb2Ob8zilgXjkno/ihpPJ83VNqgnvjdZrm2A1xTxSze6MF8PQE6a1pMTz2rBF/NjMX7TonjfuL5W5sgkjk7TXkn0r6yv69trgJhs6W+c2nb3nbuNYsQ9yPPI59p4/X9UAQOykpo+di2Z1AWU3FvgWz5IMYbxUA/5zSWa9WYD9jtACjtIdSRuIbh9Viv4sqTgosFpofsLovz5VnlWbFqH7p7zIAqX7trnNEGngleRT734VDE/eD2MZSrrEPKSt+vABo2Zeze4+E6TusN7+u13bjk+SpijC6yZeXWGVhWGbYR7uN5oXNQBvmajVz64wcqEIvfSO0NJ6zugytTAdf74H4oS010uSR4r4rbS+XxGyygesqypOmEx06uAz/U5y8t6ZpJPEUHJ+AmBN8A9ZoM2vDqprpev/Tam3X6cN+tgT9+T9e/JYj3pcWM76kul1GkbVRErwNYD6CgqbxBr6frGGwvk7G7V1HA+tfHbxxoj9ygkrZ+U/eXdX1T8jfL4OiQ83z23G8Wf4PBYDAYDAaDwWAwGM4x7MPfYDAYDAaDwWAwGAyGc4yno/qDfLwo5nqhUjCEnkVE1J4O+RKlYuZAbf7osaO2fP8n79XH/v4fflynH+4qLXLM9MA2KGD32/rcm1cc3WsL1PNbkf62qJWG9XWRQirMixlQln7trRfq9PHD20REdG+sFI+Jr3Tef/tvfrtO37js8hJAuN4IqB3/+l99zZ1neqRQKBeFmupfgqIlUEX/5o7b6/nZI6W6nUyUZjLmes0blHS9fwaUlKr+DXJ9fhFn/Wkwb1+qGen+NLSOAoxDz2qmSPX8j6CMO2UXB1HG9M5QxH9WqEI/KrliGc9RfUYKWdl0GSAiysEFA+l2EdOyUBV1CHF7H0yEOqjXdDuaPjh2NOPVPqiog1K80D+Rdo+qq1EoZam0snhP3Qp2dx/V6dJz9XLvrsZO7fWV3r6x6eLJF0zdrBbAKS/LkmasFj9ham4L3mUbqNNb264/hxjrGPrAbOKorjOMMY8UX6B/h0zxL9a1z6NLUlTTY7XdBhCTu74GFKlR1ZokrjhUVgWUV6G+RQnEQo+1jgOglUkdRkDjDuC3dbznenx//v5SVVVNeROqbgzx3pH2Jj0F1e0xbrHEMsd+sburrmg47gr9ejjU36Iy+mjk2uvRkbbhF25eq9Ovv+bcRDrgtrF5UZXmty87CvGMtD88hvmt9Nw7XGVVcSKiCiiZg2N97oQjZZycKOU0AkVhiUU+m7k+PkNXkGdEEAZ1jPf+oXOXGKEvWIPm6OptDHHSA/BmKznSSzHVg+3ucp3OJtqOyplrW0fgYtLuoOuHo88eHgINE6i64pIzGUMkAKCDT7ntLIELyHCg1PUTVofGCENFI0QOUKi5z6OaeQcU71c2OFLKQlyVEF493lzcdm3u4gWgE+fo/ujGjfug9j0Clw3pc0g1rVKMUKLtTH6DrkQ4n6X1eaAr0+nrm2+i8OfQu+fFlkcaewLjGlL957l6hqACfvPmDSIiunXr1qn7PDOqqu4LMmfj8+MWUHt5HvdAyR/LStyOUNU+jKAftNW9RVXzcW2g9PCCXWtzoNcHUIcSIQJdCbCsZLxFajK66ZUcj7wCX1OMsDCF6EMSkQbHqOlE+7LP6xO/krwuIkJJRXHL3ddjKnkUaL0kifZZmTuh+1MKVPtS6g1cE/0Y3V203Foc2SUAavYUxNmnXBYZjvW46Gq7MTKAvK5BRJucj793W12k0TVua82NcYfwyVdhDC3ob1LK2IeKHPuTq4+C19oetLVnRVkUNDx2c1rCrlcpuB9EQNsvOIrOCMZ1cbcjIvL4O3My0nb5G7+urtdyPRHRD3/wD0TUbJc4xCcc8aPVnu8mHgTi6qnXhPCduxS6vrl1cQuuQZcplz7LWwLrUKLm+RN9196apnOe8/PUjelVdXa9mMXfYDAYDAaDwWAwGAyGcwz78DcYDAaDwWAwGAwGg+Ec4ylV/YmE8Rp4rB7aAr7KEFS+meofAN3tw3s7dfqP/vTPiYjozs5efWyppxTfq5eUbvviC45KubWp9MyNZU2/eM0pNvZAldoDqrpQXJsKibrnIfT1AqhWbU+pH73EvfTvX38V7qlUq0trShX0meLUoNgBdSQS2lcoiqC0UHhcQXmFVH6llvzwjjt+/xgiLIByt6qSzqfoBUBDJqHinKWC68v1cH84LZchfRtvVVPbGrR7zGt1Kn9Iv/SAyh4HTKOOtN1MMlDFz+RZi60Qebd5yrYNOiOnG79D+hW7bmBUgwgoZiGkC1Z3j0C5Fzxb6ucGQEnyQRl7dcVRyFoxRmsAehO7SKCa9yTX3wo9vNPWY/22tkcPohHcvv2RyzNE93jl1mt1Omk5yrbQzrwFSMh6pPTGXoejawAFdwaKxHWIghwUdMegHn/i7jMkpKGCMjhQ/4Tq6ftKzawqpI1JHoA+C7QwUVxutgtQEYb30/NQ7nwd1vVZrhPyXKTveui2wOVVcV4XoINNYRjSGo+lMlYLdd0dg7GH37JJuzutVo5jPp5PQTX76ETmLWjDibpYpEz5HoPi+537GoGm03f95doVpVjvHihldZq5ee8YXAkm4HpzPHBub/cfqCuCH6BSN8yxdf71Xu+/d1Cnc6biyvwzmYzpeZFlGe3sOHeHR49cHjOIBBMG2nemU5cvpNRv9LQsE6Yuph0t6/Utpb8eHum7BPzeM3AjyiEsjM+RA7KpPmvmAYWZ56rhCZYB9i1XxvuP1UURozlUrO5cQr374PLTgnopWVW6KPT6VoN6LvleRE9R+L5Xuy3m3Cb29rQdIW29jjAD40c4h3KPFFGcu3G+8vm4B7/1Qc1aXOUqXEfAs0o6rTjtoR2K79Wg90OkHRm3Gk6G8xYXpPT3dgvWbKzgT0T0+utvuPO8Vp3nhvC0qIgoF3cldofFOSYF9fqC5xZU4Q6AXh/xdR5Q8j2g4qcF3Ctlt4bGGK8QCj+WFS45ZFwsSnBdg7WF0PZR8B2mICq53nD9m2WoX6+YMXV5hq4IJbhqMhU/DMUVYhFUf6IwFDcUdwzXTkF4ei3qQ58OQIt/mjFlHBjVWC8B1LdETvA9cCvogMvfspv3BlOIzgTuEn4o6yBYD2B/5H7egvmyu6zfU0sbbo4ajDWz6FKJLgrST5Hqn07QvdGlpzy3LMIttqoqynjdFWbuHTvQXzGCSuBzucN3y3ii9P2Q3QN/9WvfqI9F4Hb6vW/+TZ2escuLF+uzlpY03eM6wjFhhlGV2JWw19VrOqC6H3F9tOA7WdxMiXRMw3mj29LrhwP97cGhmxv9SN38ZoW2tz67wdVD8qd0F7P4GwwGg8FgMBgMBoPBcI7xdBZ/0h1WEcQIII5yZ0l3sEa8c5X5uqv0wccam/j+/QdERPTF1z5fH/v6V3+tTq/AfS9uujirFKD1CvNUNf4SEcFGCJWl7FLiTjag3toDQTzQtImXnCWnA0KFYQm7mLBlKoYQCFlLAVj1Yo7f7POeyyIsmBXp5o48KYN3neRgdfREwEfzHKCYD+cHjMENKzOK+chDyzNEJOYZz8/aldfzGBNVngv7U7DrK9n2gYXgQVmjCJrEiu1Ck/fAgjAouT2LFXsRlv9qjmif1zhNnzzsNRt2DbFW4ukCdvr3j9SqNRq69HIfLQwg4MhiOiXsxMeRNviCHzwFpkiDVMEUlsCH3Xm0TPNO9fFQ738yVOG1G7+ieb1w2YmkoeW5C8wfEUaqd1wXUS+eV98v4V3llq87tnlDHI/jFoPokIdiRrxLnYMFZwaCWCiSFLClAa3YAYi4ecyiImAeVSG2h9Pv3hDEok8XiqnlJb3T70fU3OGv2QXIxoExVYSLJEuL4Mm0Wgm9/MpL7vk8zsQJWLtRZJT7btiIu4zsCGZ54TGw/iML64MPPyQijUFN1Nzh77PlJAaBH7SK7h85i3G3p0yOJNF7HR679hCA4GwQgjhXIM8EKxn01xIEDIX10AFrcwbjc8rjTcLv6i/AGpNlOT165PrvCb9rb0nftUTBVy6XqmEG1KSQNqIWWJ1jECZN9Lq+z2KYKK4HY9aE0wmKWIGo4tEhxzOGdr22tlane30WvDpWC8rensZ6D3nsqRoWQU13urre6fCYNRhovefAejg8cMxGf0XZgYuA73nUilyhzlj4cf+JslEa8dy5bQTwPvOsdY344zCfUoEW/znCvjA3C9PIg2VSECHbwpdMwZNPC9c2BMbAIi5MniAAVho8H0VwhfWwsaECW2+//U6dXl9Xxsni4NXrkooXU8gmyYBxJBZxbOcezs1sAUxg/ED2BAqYpsyIyRpMJ2QGCpMLGK8wvghbLQXh4BkIn0kbwroo5ghI5jm+n94f14pTFiND9hIK0/qRWysHkYjePv9YFscxXWE2cclW1glYYPFd8xkzMeD9cL4smLnYWPvAsypgQU5ZaK5CliUusiUefF/HlHyi1n/idWkO7X0KeUnZ4t3pquDfpUs6FmY8d5VTXXv5ID4YoKAlz70e9M3QA4Zw4vIQsoXZbzDxnhEeUcXz33Dgxm0UfQ1jtYKLaGYErJgA2Ihf/a3fISKi5RUVe/+z//bHdXpwpAwvmduXVnQ+Q4bxhNvmYKBzBK7Rl3gOaUG5Ly1rvqU/nJzoM30oV+m7+K1CIz3/5Iky4EbMsNi8sFEfC4GBMhq6PB4dDfjZJu5nMBgMBoPBYDAYDAbDP0vYh7/BYDAYDAaDwWAwGAznGE9F9fd8jyKmhwTMlSyBUlVCLMX00FHmxhOknSn14Fdee5GIiH7vt79SH7t5TQX9qhzpP47OkAO9CCkznlBNgCKLsUM9pstmSNmBfAccux1phB7Ey7x7+zYREd17qPHHv/jWG5o/+K3Qp5H6moNQU8QUK68+tnB1P/dMYHlMMqBRs2Ab0mJ9fw6dGA+h2I2H1/G7lGdR/YWejvT9hhwP3xLvD7+tBbww7iXSFN3xAPIf+vPLU9qOVyh9anem9xrMHMUq5mctZEfMIyjPOXGJ55QLlk+jXlgMCQX7UqDyYBW9fNNRgdZXtF1+8LFSWI9PhEYJcVIjfdaYxbSQJo0xhFOm8VVzqNdERFcurnJe9fkrq0q7ikKllT166Nx/+iCoUhUq1NJur3Be3f3PqN6nQkUVpSIuxeUpYqVEREGlVP2SKW4B0uJQbI3bLnrAIP0UhTOFah6B6E/g4xAs7gzQ3jF2/VyqP9Jv5dh81OJcQKcsi9NUYPcO0nfBLQCp/hLvWC5fQL1EUUTbl5xQq1B8m+KE2sjbIqBTYf7ht9whIig/HMc86EdJx9XHEMRp5yEI51CVieq4wRijfXUZ2jvTNL1Gu8B6E7cv6MTzGdC1S0yDXgr5kj4tl7RR1fMZ4fs+dbuOEnn5khNF27yglHkUqPR5Dk5nEJt7qvTZlKmmF2A86ICQYu9CB65zFOGVnhbA48fqMiQU4uW+0iw9INtGTI2W+PZERN2uUjrbHP8cacllg5bsyrIFIlNI78exuo7ZDpTNrevX6/R47N57cKLCoItAGIa0ueHGe6mb999/vz6PbhhCyQbp0gblW9LYtnwPKeM6BlYsttWF8sCylQY8hRjtSPkO5b7oQoaKcZXECEdRL+xzjce4Y0D1RyHQZabevvHGm/WxK1euwHULXoPxPUOe/6Q8C1j/onuiuOP44AqBQn5hcFqQD10gUF1PBB5R1BNFBUXoEOs4A9c0EQct0QUMRObC4PSaDhcCki8U98M2MB7rWDAVkTS4V6ujYuAhu0NFLRZg857aM/kUfN+nJRa1k/Z+MtT75vvgNsHvkAI9PgVh4vqeuP4Hv6Yc+p4sN6oCXaBOu8Yg/T6fwtzG5Y5U/zGMsRW3nQtbSgPf2lC3oicj/i30wRS+d4oTnftkrdcY36Cf+dyeZK29sO7Dc3nF/TiDsmqBOF/FAol5ru//pV//Wp2+eesWERH9yX/9w/rY8aFS9SNwa5A1eDqFPgD1Erdce+z3gL4P/S3jghmONC/dto6JIlgYw7EpuA3U/QTWNiN0xwAXBl/aA7oFwFpZXDNGo8GpfH4SZvE3GAwGg8FgMBgMBoPhHMM+/A0Gg8FgMBgMBoPBYDjHeCruTFURZaz2KZSVCuI7hi2l/xWpoxucHO3Vx7ZAqf/Sm452dWVTYx8jP70CCqeobzapkMgz5kOgPFkVSFU6He/dh3tJDPEI1esh/SLTwm5e0tivQQBuBRBHOGZKDsYx9iNwISglr5JYRGxSr6Zu1aqrQL95/0jf5ZjDDiC9Ht9VPCgwV0jzRlaPlCsqlzfD0J+m6s+jt6PbQZNCxvmDp84Lc9/Mk6ZjoAcJ5bUolJb1nSegGFxTRKWuFhNvWd69pk4jdRHVooVmiRRgSGchU/0xqAK08QjaY5dp0EiNvrSldNh+1/XDCbTRTqw3HnNICmSZp6mWx4TjWmMs8gxda+S5QL1c6imFLx0d1unWqqOjvfzyzfrYyhJECmHlWImJvIgYy54fUNhhuh9T63wIA+JBDOOKOXp+An04VLoiBRznNQY1XlApxrYfcHvEuOwBnFelZGwDSGWf8zJIwTt1dbO9SX9BZWVUZ8YHSBVi7G6Cfhq1HPVNx5Hn5/uFUUyb244efXzk2sj+nlK7c1CvfuFzrxJRU013BtRHqt2EwBWtREoqRGFg+uMJKLLPA7o0LS0p9U/69nCgNO67O/dPXY/ljm4VdQ6R6YxpjKXO7X9pSd/7ylWlLS9zvuRJHtTZs2J9bZ3+/b/7D+6+3Dc6XXB3IZxLmNYM7eroWBWNHz3aISKiXl/Hgxj6A9LqJbIBxq4egTuGqIHjnIEUZamXTkfdB1ABPeCoAZOJHsPmLr9FF5HLQBFPgDKacVQPpDgvgQuCxGsejTT/f/if/oieF3Ec040bN4iI6A/+4A+IiOhb3/pWff7Bgwd1emfHlf3R0VF9DGnYEp8a36HJFkU3O1dQy3MUrCVfROri8MlntTgSQ9ONrxHyx/2L7jtYz+z+CSz0xvl2W9vXF77wBSIievXVV+tj86IdLJLyX1UVuJC4+6LK9hRigUu5NN38cE3ljqP7KrqnFOAWmzO1GJ+VIWVb6qjpNwDPEjfA0+4DREo1x3mhgnFV6hsjpIxGOl+iu23Iav2tROfOdlv7jFD92x13TSOi1DMiTVP6+PZdIiKKuf8ifR/nGJmvKZ3v8li7QsL4AEL+1O5AbHgZC6CsfPyE4XqbQbmN4RtotufGsCjRYzFEhmmzK1agxUfLsI46YEp6CK5f2IZ8qBev7g96rxLKXujnMrUuotd4XkBR4jLfW3JjRwRuDz1wKfLZVfTa9RfrYxsXLtfp//Hnf05ERMNjHefWN9W1bAZ+C0esth8n+u2awHfq3oFbh2BEDgLXtrVVl+cujDcPH+maJeQoMJubGjnkBNYJ47Hr+2Gk/X080XFyMta2KW6LGM0A3ehq9x/vF3+7mMXfYDAYDAaDwWAwGAyGc4ynsviXVNFQrOq881fMdEd3UsEOE8enHsB20PZN3S3v9p0QzBSsnhnsHRWwC5myRQbj9c6zDKOYRu6jdUfEyHCLDSzy/Fy0EOPtQ955ykAMYwjWUrSqiXifDzs4aYEWRGYXlO5dygVZlr1PmL+xfD4caBnLrlDDkoc7zVIf/4TdbylOr2EJOJ0nFOUpUaDHO20tbAgL8f2bAl+nLebNrKL1B4RWhOkAojppCe8t1lC2GC2iVnzPpxbvKkeRxD3HHWOIr8vlEoZ4XtuQCJ0Evr5TBDGlMb8l10dWIAtAd3pF+Kga6u52G9prELlndSDW9t4hCN7xsJG0kbGgOUh4R3gGu6T3HuzW6Vuvaj/6xm98g4iIrt+8qu+Cu+I8jlQksVuff9ff90NqdVxMZ68SUS60Js2pfTjWXtKd8t4aiyHlIAoG1paqqT7FGZjft7S94jWYharxl6/Sn1YiLIrsi3nXo0gqCnNWp37rNfbztY2ESZ/zfCrHz4wwimiNGWATtoIV0BYCoKCIWGQMFn+0Lkr+GzGkIR1B34sjEbzS+6MVWvouWng3NlRESZ41m4EwFMwV9T1hpx6tV2LFx3rNoD0V+WlLKlryRyCY1WPLkpTVIqwxnU6PvvT2l11eCrGyo5UQWSUyJuuxkxOIgVyxkC5Yh5EJkc/0um6brclgoQkKKMMuj5lQFigALKKKyCJAAcdOx61BRkNtN5iv0chZ35CR9tL1l+r0hS1lK0oOGzbrJm3u9A8WACci597z61//OhERfeUrKpg8AOuS1MMxMDBOMM2/xWsGIPqFZSd9DdtsmmanzmOMdGQ71PGrMd47WK7Fot9gyZSnxzVkK/ZZtI2I6K233qrT77zzDhE1WQCIX4a4X1mWdfuRcR1jge8fqNiuWPxxTdbpgGhiT0QTNf/IysA2W4vE4bgPFu0Zlzdae4NI+0fB80lRwLgJwmeZiPfBWjtL5wnGwdog0neJEhRodccTsPiHYMUWpo1XnV4nPiuKoqzZCEE9x6C1G4QMub0hiyiCb4SI6ZcBrJ3iJRW4XN/crNN9Fr6cguhiDuUq7AAP2vAQyuLRzh13f2BOthIQHWVh9SjCuQLmdp7Okp7OYQTtphhrOmUrNArYhiB+HHJ91PNpYw3xbCirkqapGx+EuT0Yan8pYQ7YvurYoe0ltdL/z7/+yzq9+8ix7UJYH1+4CAw91MbjtVoMZTmA+WA4dONXt6dt9Cow7LbZko9j2/2dh3W623bldrCnfQTH0VpQNdY2dLR7WmyTiGh7yeWh39e8lFCHx8du3M5SZuV/CpvcLP4Gg8FgMBgMBoPBYDCcY9iHv8FgMBgMBoPBYDAYDOcY3qfRAU792PN2iejOLy87/yxxvaqqzV/8s7Nh9fJLgdXLZxNWL59NWL18NmH18tmF1c1nE1Yvn01YvXw2YfXy2cSZ9fJUH/4Gg8FgMBgMBoPBYDAY/v+CUf0NBoPBYDAYDAaDwWA4x7APf4PBYDAYDAaDwWAwGM4x7MPfYDAYDAaDwWAwGAyGcwz78DcYDAaDwWAwGAwGg+Ecwz78DQaDwWAwGAwGg8FgOMewD3+DwWAwGAwGg8FgMBjOMezD32AwGAwGg8FgMBgMhnMM+/A3GAwGg8FgMBgMBoPhHMM+/A0Gg8FgMBgMBoPBYDjH+H9OCtyfz506dwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1296x1296 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_distribution(pred_labels):\n",
    "    fig = plt.figure()\n",
    "    plt.bar(labels_set, counts)\n",
    "    plt.title(f'Ditribution after {total_epochs}')\n",
    "    return fig\n",
    "\n",
    "def no_axis_show(img, title='', cmap=None):\n",
    "  # imshow, 縮放模式為nearest。\n",
    "  fig = plt.imshow(img, interpolation='nearest', cmap=cmap)\n",
    "  # 不要顯示axis。\n",
    "  fig.axes.get_xaxis().set_visible(False)\n",
    "  fig.axes.get_yaxis().set_visible(False)\n",
    "  plt.title(title)\n",
    "\n",
    "titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
    "plt.figure(figsize=(18, 18))\n",
    "for i in range(10):\n",
    "  # 注意和subplots的区别，这个方法可以和平常的使用无缝对接\n",
    "  # 它应该类似于全局设置，而subplots则是单独设置\n",
    "  plt.subplot(1, 10, i+1)\n",
    "  fig = no_axis_show(plt.imread(f'real_or_drawing/train_data/{i}/{500*i}.bmp'), title=titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "DT4YwZI_nqVC",
    "outputId": "e0097fcf-ee1b-4892-c872-eced3ef7e81a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18, 18))\n",
    "for i in range(10):\n",
    "  plt.subplot(1, 10, i+1)\n",
    "  fig = no_axis_show(plt.imread(f'real_or_drawing/test_data/0/' + str(i).rjust(5, '0') + '.bmp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Id5yTHsZxulR"
   },
   "source": [
    "**Canny Edge Detection**\n",
    "\n",
    "图片素描风格化的库，有两个阈值，low和high，如果高于high，则为阈值，如果高于low低于high，则进一步做判断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqsJZAfdw06d"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "HgmUw6Yrnw1k",
    "outputId": "3fad8dd6-6efb-4ffa-b76e-28d74404d110"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
    "plt.figure(figsize=(18, 18))\n",
    "\n",
    "original_img = plt.imread(f'real_or_drawing/train_data/0/0.bmp')\n",
    "plt.subplot(1, 5, 1)\n",
    "no_axis_show(original_img, title='original')\n",
    "\n",
    "gray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
    "plt.subplot(1, 5, 2)\n",
    "no_axis_show(gray_img, title='gray scale', cmap='gray')\n",
    "\n",
    "canny_50100 = cv2.Canny(gray_img, 50, 100)\n",
    "plt.subplot(1, 5, 3)\n",
    "no_axis_show(canny_50100, title='Canny(50, 100)', cmap='gray')\n",
    "\n",
    "canny_150200 = cv2.Canny(gray_img, 150, 200)\n",
    "plt.subplot(1, 5, 4)\n",
    "no_axis_show(canny_150200, title='Canny(150, 200)', cmap='gray')\n",
    "\n",
    "canny_250300 = cv2.Canny(gray_img, 250, 300)\n",
    "plt.subplot(1, 5, 5)\n",
    "no_axis_show(canny_250300, title='Canny(250, 300)', cmap='gray')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "74FbJFNVHeUi",
    "outputId": "bacd0b60-28a5-4ad9-a892-d2918f165a5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(gray_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z7L0HfPGIlI4"
   },
   "source": [
    "## Data\n",
    "\n",
    "对于图像数据，torch有非常好的预处理的API。ImageFolder可以直接输入地址，文件夹是标签，需要提供合适的tensforms。\n",
    "\n",
    "对于source。\n",
    "- 转成灰度图像\n",
    "- 边缘风格化处理，用 transforms.Lambda映射cv2.Canny函数，处理原始图像\n",
    "- 转为图片，data augment，转tensor\n",
    "\n",
    "对于target\n",
    "- 转灰度图\n",
    "- resize到和source一样大小\n",
    "- 转为图片，data augment，转tensor\n",
    "\n",
    "但是对于target上做data augment，我先保留看法，我认为这个会让discriminator更容易辨认，所以extrator的反向操作的强度会加剧，所以应该会然discriminator的loss变大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pw98wf_PxMKP"
   },
   "outputs": [],
   "source": [
    "\n",
    "source_transform = transforms.Compose([\n",
    "    # 轉灰階: Canny 不吃 RGB。\n",
    "    transforms.Grayscale(),\n",
    "    # cv2 不吃 skimage.Image，因此轉成np.array後再做cv2.Canny\n",
    "    transforms.Lambda(lambda x: cv2.Canny(np.array(x), 170, 300)),\n",
    "    # cv2.Canny只接受ndarray，且不能是RGB\n",
    "    # 重新將np.array 轉回 skimage.Image\n",
    "    transforms.ToPILImage(),\n",
    "    # 水平翻轉 (Augmentation)\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # 旋轉15度內 (Augmentation)，旋轉後空的地方補0\n",
    "    transforms.RandomRotation(15, fill=(0,)),\n",
    "    # 最後轉成Tensor供model使用。\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "target_transform = transforms.Compose([\n",
    "    # 轉灰階: 將輸入3維壓成1維。\n",
    "    transforms.Grayscale(),\n",
    "    # 縮放: 因為source data是32x32，我們將target data的28x28放大成32x32。\n",
    "    transforms.Resize((32, 32)),\n",
    "    # 水平翻轉 (Augmentation)\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # 旋轉15度內 (Augmentation)，旋轉後空的地方補0\n",
    "    transforms.RandomRotation(15, fill=(0,)),\n",
    "    # 最後轉成Tensor供model使用。\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    # 轉灰階: 將輸入3維壓成1維。\n",
    "    transforms.Grayscale(),\n",
    "    # 縮放: 因為source data是32x32，我們將target data的28x28放大成32x32。\n",
    "    transforms.Resize((32, 32)),\n",
    "    # 最後轉成Tensor供model使用。\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def get_data_loader(dataset, with_aug = True, mode = 'train', batch_size = 64):\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    return DataLoader(dataset, batch_size = batch_size, shuffle = shuffle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FrDRqdsPOKGr"
   },
   "outputs": [],
   "source": [
    "source_dataset = ImageFolder('real_or_drawing/train_data', transform=source_transform)\n",
    "target_dataset = ImageFolder('real_or_drawing/test_data', transform=target_transform)\n",
    "test_dataset = ImageFolder('real_or_drawing/test_data', transform=test_transform)\n",
    "source_loader = get_data_loader(source_dataset, mode = 'train')\n",
    "target_loader = get_data_loader(target_dataset, mode = 'train')\n",
    "test_loader = get_data_loader(target_dataset, mode = 'eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BybA__Wlfav_"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOjSyUbfOiE3"
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1), # (64, 32, 32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), #（128，16， 16）\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # (256, 8, 8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(256, 256, 3, 1, 1), # (256, 256, 4, 4)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 1, 1), # (512, 2, 2)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                 # (512, 1, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x).squeeze()\n",
    "        return x\n",
    "\n",
    "class LabelPredictor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LabelPredictor, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        c = self.layer(h)\n",
    "        return c\n",
    "\n",
    "class DomainClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DomainClassifier, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        y = self.layer(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0dp-RfIoN7c"
   },
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    torch.save(model.state_dict(), os.path.join(path, name))\n",
    "\n",
    "def load_model(model, name):\n",
    "    model.load_state_dict(torch.load(os.path.join(path, name),map_location=device) )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eIDNavYxivOd"
   },
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor().to(device)\n",
    "label_predictor = LabelPredictor().to(device)\n",
    "domain_classifier = DomainClassifier().to(device)\n",
    "\n",
    "class_criterion = nn.CrossEntropyLoss()\n",
    "domain_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_F = optim.Adam(feature_extractor.parameters())\n",
    "optimizer_C = optim.Adam(label_predictor.parameters())\n",
    "optimizer_D = optim.Adam(domain_classifier.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QWR3G7eei58v",
    "outputId": "510dfb03-8efd-443a-ec82-184a37abec75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "      Layer (type)         Output Shape         Param #     Tr. Param #\n",
      "========================================================================\n",
      "          Conv2d-1      [1, 64, 32, 32]             640             640\n",
      "     BatchNorm2d-2      [1, 64, 32, 32]             128             128\n",
      "            ReLU-3      [1, 64, 32, 32]               0               0\n",
      "       MaxPool2d-4      [1, 64, 16, 16]               0               0\n",
      "          Conv2d-5     [1, 128, 16, 16]          73,856          73,856\n",
      "     BatchNorm2d-6     [1, 128, 16, 16]             256             256\n",
      "            ReLU-7     [1, 128, 16, 16]               0               0\n",
      "       MaxPool2d-8       [1, 128, 8, 8]               0               0\n",
      "          Conv2d-9       [1, 256, 8, 8]         295,168         295,168\n",
      "    BatchNorm2d-10       [1, 256, 8, 8]             512             512\n",
      "           ReLU-11       [1, 256, 8, 8]               0               0\n",
      "      MaxPool2d-12       [1, 256, 4, 4]               0               0\n",
      "         Conv2d-13       [1, 256, 4, 4]         590,080         590,080\n",
      "    BatchNorm2d-14       [1, 256, 4, 4]             512             512\n",
      "           ReLU-15       [1, 256, 4, 4]               0               0\n",
      "      MaxPool2d-16       [1, 256, 2, 2]               0               0\n",
      "         Conv2d-17       [1, 512, 2, 2]       1,180,160       1,180,160\n",
      "    BatchNorm2d-18       [1, 512, 2, 2]           1,024           1,024\n",
      "           ReLU-19       [1, 512, 2, 2]               0               0\n",
      "      MaxPool2d-20       [1, 512, 1, 1]               0               0\n",
      "========================================================================\n",
      "Total params: 2,142,336\n",
      "Trainable params: 2,142,336\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Linear-1            [1, 512]         262,656         262,656\n",
      "            ReLU-2            [1, 512]               0               0\n",
      "          Linear-3            [1, 512]         262,656         262,656\n",
      "            ReLU-4            [1, 512]               0               0\n",
      "          Linear-5             [1, 10]           5,130           5,130\n",
      "=======================================================================\n",
      "Total params: 530,442\n",
      "Trainable params: 530,442\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Linear-1            [1, 512]         262,656         262,656\n",
      "     BatchNorm1d-2            [1, 512]           1,024           1,024\n",
      "            ReLU-3            [1, 512]               0               0\n",
      "          Linear-4            [1, 512]         262,656         262,656\n",
      "     BatchNorm1d-5            [1, 512]           1,024           1,024\n",
      "            ReLU-6            [1, 512]               0               0\n",
      "          Linear-7            [1, 512]         262,656         262,656\n",
      "     BatchNorm1d-8            [1, 512]           1,024           1,024\n",
      "            ReLU-9            [1, 512]               0               0\n",
      "         Linear-10            [1, 512]         262,656         262,656\n",
      "    BatchNorm1d-11            [1, 512]           1,024           1,024\n",
      "           ReLU-12            [1, 512]               0               0\n",
      "         Linear-13              [1, 1]             513             513\n",
      "=======================================================================\n",
      "Total params: 1,055,233\n",
      "Trainable params: 1,055,233\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(summary(feature_extractor, torch.zeros(1, 1, 32, 32).to(device)))\n",
    "print(summary(label_predictor, torch.zeros(1, 512).to(device)))\n",
    "print(summary(domain_classifier, torch.zeros(1, 512).to(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hoJTsw6lFI1"
   },
   "source": [
    "## Training\n",
    "\n",
    "DaNN的训练的理解如下：\n",
    "- 两条线，第一是对于带标签数据的acc，包含extractor和predictor，另一条线是无标签的source/target二discriminator的acc\n",
    "- 我们的目标是使得extractor提取的features混合在一起，不能让discriminator简单的分辨出来，最好分辨不出来。\n",
    "- 于是我们同时训练二者，A step，训练d，让它尽可能分类准，B step，训练e和p，取d的负梯度，让features发生变化，这种变化使得d变得无法分辨\n",
    "- d会不断地挣扎，A step会让d尽力去从feature中区分source和target， 而B step则总是混淆features，让d准确率降低\n",
    "- 因为d无法看到feature之前的东西，feature之前的东西不受到d的控制，因而d最后一定会坏掉。\n",
    "\n",
    "DaNN重要的指标：\n",
    "- d的loss，或者二分类的acc变化，我们想让它越差越好\n",
    "\n",
    "先测试一下epoch的影响，由于要使用kaggle，我试试100，200， 300， 400，500三个参数下的大致表现，然后剩下两次用于测试，target data不进行aug，训练的时候aug，测试的时候不aug。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxPf5Va-ix-z"
   },
   "outputs": [],
   "source": [
    "def train_epoch(source_dataloader, target_dataloader, lamb):\n",
    "    '''\n",
    "      Args:\n",
    "        source_dataloader: source data的dataloader\n",
    "        target_dataloader: target data的dataloader\n",
    "        lamb: 調控adversarial的loss係數。\n",
    "    '''\n",
    "\n",
    "    # D loss: Domain Classifier的loss\n",
    "    # F loss: Feature Extrator & Label Predictor的loss\n",
    "    # total_hit: 計算目前對了幾筆 total_num: 目前經過了幾筆\n",
    "    running_D_loss, running_F_loss = 0.0, 0.0\n",
    "    total_hit, total_num = 0.0, 0.0\n",
    "\n",
    "    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_dataloader, target_dataloader)):\n",
    "\n",
    "        source_data = source_data.cuda()\n",
    "        source_label = source_label.cuda()\n",
    "        target_data = target_data.cuda()\n",
    "        \n",
    "        # 我們把source data和target data混在一起，否則batch_norm可能會算錯 (兩邊的data的mean/var不太一樣)\n",
    "        # batch normlization layer\n",
    "        mixed_data = torch.cat([source_data, target_data], dim=0)\n",
    "        domain_label = torch.zeros([source_data.shape[0] + target_data.shape[0], 1]).cuda()\n",
    "        # 設定source data的label為1\n",
    "        domain_label[:source_data.shape[0]] = 1\n",
    "\n",
    "        # Step 1 : 訓練Domain Classifier\n",
    "        feature = feature_extractor(mixed_data)\n",
    "        # 因為我們在Step 1不需要訓練Feature Extractor，所以把feature detach避免loss backprop上去。\n",
    "        domain_logits = domain_classifier(feature.detach())\n",
    "        loss = domain_criterion(domain_logits, domain_label)\n",
    "        running_D_loss+= loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Step 2 : 訓練Feature Extractor和Predictor\n",
    "        class_logits = label_predictor(feature[:source_data.shape[0]])\n",
    "        domain_logits = domain_classifier(feature)\n",
    "        # loss為原本的class CE - lamb * domain BCE，相減的原因同GAN中的Discriminator中的G loss\n",
    "        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label)\n",
    "        running_F_loss+= loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_F.step()\n",
    "        optimizer_C.step()\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        optimizer_F.zero_grad()\n",
    "        optimizer_C.zero_grad()\n",
    "\n",
    "        total_hit += torch.sum(torch.argmax(class_logits, dim=1) == source_label).item()\n",
    "        total_num += source_data.shape[0]\n",
    "        print(i, end='\\r')\n",
    "\n",
    "    return running_D_loss / (i+1), running_F_loss / (i+1), total_hit / total_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iEcqNJ_K4evp"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTGnuAk54hZ4"
   },
   "outputs": [],
   "source": [
    "def testing(extractor, predictor, loader, return_true_labels = False):\n",
    "    pred_labels = []\n",
    "    latents = []\n",
    "    labels = []\n",
    "    pred_probs = []\n",
    "    extractor.to(device)\n",
    "    predictor.to(device)\n",
    "    predictor.eval()\n",
    "    extractor.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (test_data, label) in enumerate(loader):\n",
    "            test_data = test_data.cuda()\n",
    "            latent = feature_extractor(test_data)\n",
    "            class_logits = label_predictor(latent)\n",
    "            pred_prob = torch.softmax(class_logits, dim = 1).cpu().detach().numpy()\n",
    "            pred_label = torch.argmax(class_logits, dim=1).cpu().detach().numpy()\n",
    "\n",
    "            pred_labels.append(pred_label)\n",
    "            pred_probs.append(pred_prob)\n",
    "            latents.append(latent.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "            \n",
    "    pred_labels = np.concatenate(pred_labels)\n",
    "    pred_probs = np.concatenate(pred_probs)\n",
    "    latents = np.concatenate(latents, axis = 0)\n",
    "    labels = np.concatenate(labels, axis = 0)\n",
    "    if return_true_labels == True:\n",
    "        return pred_labels, pred_probs, latents, labels\n",
    "    return pred_labels,pred_probs, latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phHgmqGARX_a",
    "outputId": "c8501c08-7db0-48f7-dd5b-754af7140f4a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# testing\n",
    "set_seed(0)\n",
    "# model name\n",
    "epochs_num = 500\n",
    "ex_name = f'extractor_model_{epochs_num}.bin'\n",
    "pr_name = f'predictor_model_{epochs_num}.bin'\n",
    "# data\n",
    "test_loader = get_data_loader(test_dataset, mode = 'eval')\n",
    "# load model\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "label_predictr = LabelPredictor().to(device)\n",
    "feature_extractor = load_model(feature_extractor, ex_name)\n",
    "label_predictor = load_model(label_predictor, pr_name)\n",
    "#predict\n",
    "pred_labels, pred_probs, latents = testing(feature_extractor, label_predictor, test_loader)\n",
    "# save\n",
    "df = pd.DataFrame({'id': np.arange(0,len(pred_labels)), 'label': pred_labels})\n",
    "df.to_csv(f'DaNN_submission_{ex_name}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "il0MYc1Ohg8X"
   },
   "source": [
    "## Ensemble\n",
    "\n",
    "这边使用soft voting，实现weighted voting的方式以及hard label的方式。\n",
    "\n",
    "结果是，直接从61掉到了59，看来前期的模型是太弱了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aB0w6RI01KvH"
   },
   "outputs": [],
   "source": [
    "def soft_voting(preds, weights = None):\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(preds))\n",
    "    weights = np.array(weights).reshape((-1, 1, 1))\n",
    "    # print(preds[0].shape)\n",
    "    preds = np.concatenate([p[np.newaxis,:] for p in preds],axis=0)\n",
    "    # print(preds.shape)\n",
    "    preds = np.sum(preds * weights, axis=0)\n",
    "    # print(preds)\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EAMrgjClxjTD"
   },
   "outputs": [],
   "source": [
    "models = [100, 200, 300, 400, 500]\n",
    "preds = []\n",
    "preds_hard = []\n",
    "for epochs_num in models:\n",
    "    ex_name = f'extractor_model_{epochs_num}.bin'\n",
    "    pr_name = f'predictor_model_{epochs_num}.bin'\n",
    "    # data\n",
    "    test_loader = get_data_loader(test_dataset, mode = 'eval')\n",
    "    # load model\n",
    "    feature_extractor = FeatureExtractor().to(device)\n",
    "    label_predictr = LabelPredictor().to(device)\n",
    "    feature_extractor = load_model(feature_extractor, ex_name)\n",
    "    label_predictor = load_model(label_predictor, pr_name)\n",
    "    #predict\n",
    "    pred_labels, pred_probs, latents = testing(feature_extractor, label_predictor, test_loader)\n",
    "    preds.append(pred_probs)\n",
    "    preds_hard.append(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOag1f1X10oc"
   },
   "outputs": [],
   "source": [
    "# 100, 0.51349\n",
    "# 200, 0.49530\n",
    "# 300, 0.53430\n",
    "# 400, 0.56480\n",
    "# 500, 0.59531\n",
    "weights = [0.51349, 0.49530,0.53430,0.56480, 0.59531]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHyPPUPayHy9"
   },
   "outputs": [],
   "source": [
    "preds_ens = soft_voting(preds, weights= weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OAWmrmNczkm7"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id': np.arange(0,len(pred_labels)), 'label': preds_ens})\n",
    "df.to_csv(f'DaNN_submission_{ex_name}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LHNmcBLidrYd"
   },
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZeyMf3a2RcB9"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "def cluster(vec, show_plot = False):\n",
    "  # 选择最佳的维度\n",
    "  d = 50\n",
    "  if show_plot:\n",
    "    pca = PCA()\n",
    "    pca.fit(vec)\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    # d = np.argmax(cumsum>0.99)+1\n",
    "    print(d, cumsum[d])\n",
    "    plt.plot(cumsum)\n",
    "    plt.show()\n",
    "  tfs = KernelPCA(n_components=d, kernel='rbf')\n",
    "  vec_1 = tfs.fit_transform(vec)\n",
    "  print('Fisrt Reduction Shape:{}'.format(vec_1.shape))\n",
    "  vec_2 = TSNE(n_components=2, random_state=0).fit_transform(vec_1)\n",
    "  # print(\"Second Reduction Shape:{}\".format(vec_2.shape))\n",
    "#   labels = MiniBatchKMeans(2, random_state=0).fit(vec_2).labels_\n",
    "  return vec_2\n",
    "\n",
    "def plot_scatter(feat, label, savefig=None):\n",
    "    \"\"\" Plot Scatter Image.\n",
    "    Args:\n",
    "      feat: the (x, y) coordinate of clustering result, shape: (9000, 2)\n",
    "      label: ground truth label of image (0/1), shape: (9000,)\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    X = feat[:, 0]\n",
    "    Y = feat[:, 1]\n",
    "    plt.scatter(X, Y, c = label, s =5, alpha=0.5)\n",
    "    plt.legend(loc='best')\n",
    "    if savefig is not None:\n",
    "        plt.savefig(savefig)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P1QfOd0mVeVe"
   },
   "outputs": [],
   "source": [
    "source_transform_noaug = transforms.Compose([\n",
    "    # 轉灰階: Canny 不吃 RGB。\n",
    "    transforms.Grayscale(),\n",
    "    # cv2 不吃 skimage.Image，因此轉成np.array後再做cv2.Canny\n",
    "    transforms.Lambda(lambda x: cv2.Canny(np.array(x), 170, 300)),\n",
    "    # cv2.Canny只接受ndarray，且不能是RGB\n",
    "    # 重新將np.array 轉回 skimage.Image\n",
    "    transforms.ToPILImage(),\n",
    "    # 最後轉成Tensor供model使用。\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "source_dataset_noaug = ImageFolder('real_or_drawing/train_data', transform=source_transform_noaug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "duCTrlW3SjpZ"
   },
   "outputs": [],
   "source": [
    "# plot cluster \n",
    "import pandas as pd\n",
    "# testing\n",
    "set_seed(0)\n",
    "# model name\n",
    "epochs_num = 100\n",
    "ex_name = f'extractor_model_{epochs_num}.bin'\n",
    "pr_name = f'predictor_model_{epochs_num}.bin'\n",
    "# data\n",
    "test_loader = get_data_loader(test_dataset, mode = 'eval')\n",
    "source_loader = get_data_loader(source_dataset_noaug, mode = 'eval')\n",
    "# load model\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "label_predictr = LabelPredictor().to(device)\n",
    "feature_extractor = load_model(feature_extractor, ex_name)\n",
    "label_predictor = load_model(label_predictor, pr_name)\n",
    "#predict\n",
    "pred_s, latents_s,labels_s = testing(feature_extractor, label_predictor, source_loader, return_true_labels=True)\n",
    "pred_t, latents_t = testing(feature_extractor, label_predictor, test_loader)\n",
    "# select idx\n",
    "select_idx = np.arange(len(latents_t))\n",
    "np.random.shuffle(select_idx)\n",
    "select_idx = select_idx[:len(latents_s)]\n",
    "pred_t, latents_t=pred_t[select_idx], latents_t[select_idx,:]\n",
    "# dimension reduction\n",
    "latent_tc = cluster(latents_t, show_plot=True)\n",
    "latent_sc = cluster(latents_s, show_plot=True)\n",
    "latent_all = np.concatenate([latent_tc, latent_sc], axis = 0)\n",
    "labels = np.zeros((len(latent_all),), dtype=np.int32)\n",
    "labels[len(latent_tc):]=1\n",
    "fig = plot_scatter(latent_all, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xc8K8BbfestS"
   },
   "outputs": [],
   "source": [
    "# plot cluster without adv\n",
    "import pandas as pd\n",
    "# testing\n",
    "set_seed(0)\n",
    "# model name\n",
    "epochs_num = 100\n",
    "ex_name = f'extractor_model_noadv_{epochs_num}.bin'\n",
    "pr_name = f'predictor_model_noadv_{epochs_num}.bin'\n",
    "# data\n",
    "test_loader = get_data_loader(test_dataset, mode = 'eval')\n",
    "source_loader = get_data_loader(source_dataset_noaug, mode = 'eval')\n",
    "# load model\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "label_predictr = LabelPredictor().to(device)\n",
    "feature_extractor = load_model(feature_extractor, ex_name)\n",
    "label_predictor = load_model(label_predictor, pr_name)\n",
    "#predict\n",
    "pred_s, latents_s,labels_s = testing(feature_extractor, label_predictor, source_loader, return_true_labels=True)\n",
    "pred_t, latents_t = testing(feature_extractor, label_predictor, test_loader)\n",
    "# select idx\n",
    "select_idx = np.arange(len(latents_t))\n",
    "np.random.shuffle(select_idx)\n",
    "select_idx = select_idx[:len(latents_s)]\n",
    "pred_t, latents_t=pred_t[select_idx], latents_t[select_idx,:]\n",
    "# dimension reduction\n",
    "latent_tc = cluster(latents_t, show_plot=True)\n",
    "latent_sc = cluster(latents_s, show_plot=True)\n",
    "latent_all = np.concatenate([latent_tc, latent_sc], axis = 0)\n",
    "labels = np.zeros((len(latent_all),), dtype=np.int32)\n",
    "labels[len(latent_tc):]=1\n",
    "fig = plot_scatter(latent_all, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qAEr6RqbQEgJ"
   },
   "source": [
    "## Tem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uHoituQrmWvb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.40s, epoch [001\\1500]: train D loss: 0.8902, train F loss: 1.962531, acc 0.243000\n",
      "5.38s, epoch [002\\1500]: train D loss: 0.6804, train F loss: 1.626444, acc 0.395000\n",
      "5.36s, epoch [003\\1500]: train D loss: 0.6368, train F loss: 1.478044, acc 0.452800\n",
      "5.39s, epoch [004\\1500]: train D loss: 0.6202, train F loss: 1.385732, acc 0.484200\n",
      "5.45s, epoch [005\\1500]: train D loss: 0.5201, train F loss: 1.318019, acc 0.515000\n",
      "5.33s, epoch [006\\1500]: train D loss: 0.5349, train F loss: 1.278779, acc 0.538600\n",
      "5.33s, epoch [007\\1500]: train D loss: 0.5456, train F loss: 1.230906, acc 0.544800\n",
      "5.33s, epoch [008\\1500]: train D loss: 0.5257, train F loss: 1.200576, acc 0.564800\n",
      "5.33s, epoch [009\\1500]: train D loss: 0.5452, train F loss: 1.137928, acc 0.580400\n",
      "5.34s, epoch [010\\1500]: train D loss: 0.5172, train F loss: 1.119278, acc 0.593000\n",
      "5.35s, epoch [011\\1500]: train D loss: 0.5367, train F loss: 1.094033, acc 0.600200\n",
      "5.32s, epoch [012\\1500]: train D loss: 0.5161, train F loss: 1.070617, acc 0.612800\n",
      "5.33s, epoch [013\\1500]: train D loss: 0.5020, train F loss: 1.040162, acc 0.624600\n",
      "5.33s, epoch [014\\1500]: train D loss: 0.5042, train F loss: 1.023980, acc 0.620400\n",
      "5.33s, epoch [015\\1500]: train D loss: 0.5235, train F loss: 0.978517, acc 0.641600\n",
      "5.33s, epoch [016\\1500]: train D loss: 0.5438, train F loss: 0.954967, acc 0.648400\n",
      "5.32s, epoch [017\\1500]: train D loss: 0.5229, train F loss: 0.901687, acc 0.670600\n",
      "5.32s, epoch [018\\1500]: train D loss: 0.5487, train F loss: 0.893503, acc 0.674000\n",
      "5.33s, epoch [019\\1500]: train D loss: 0.5096, train F loss: 0.854311, acc 0.688600\n",
      "5.33s, epoch [020\\1500]: train D loss: 0.5020, train F loss: 0.834327, acc 0.698200\n",
      "5.33s, epoch [021\\1500]: train D loss: 0.5272, train F loss: 0.835512, acc 0.692800\n",
      "5.33s, epoch [022\\1500]: train D loss: 0.4980, train F loss: 0.754811, acc 0.723800\n",
      "5.33s, epoch [023\\1500]: train D loss: 0.5119, train F loss: 0.718664, acc 0.731400\n",
      "5.33s, epoch [024\\1500]: train D loss: 0.4945, train F loss: 0.728684, acc 0.731200\n",
      "5.33s, epoch [025\\1500]: train D loss: 0.4848, train F loss: 0.689807, acc 0.741000\n",
      "5.33s, epoch [026\\1500]: train D loss: 0.4787, train F loss: 0.659215, acc 0.757600\n",
      "5.34s, epoch [027\\1500]: train D loss: 0.4689, train F loss: 0.634684, acc 0.760600\n",
      "5.34s, epoch [028\\1500]: train D loss: 0.4928, train F loss: 0.589240, acc 0.777600\n",
      "5.33s, epoch [029\\1500]: train D loss: 0.4747, train F loss: 0.565112, acc 0.796400\n",
      "5.34s, epoch [030\\1500]: train D loss: 0.4647, train F loss: 0.573343, acc 0.787000\n",
      "5.33s, epoch [031\\1500]: train D loss: 0.4830, train F loss: 0.501079, acc 0.802600\n",
      "5.33s, epoch [032\\1500]: train D loss: 0.4641, train F loss: 0.482578, acc 0.815600\n",
      "5.35s, epoch [033\\1500]: train D loss: 0.4687, train F loss: 0.480408, acc 0.823600\n",
      "5.34s, epoch [034\\1500]: train D loss: 0.4577, train F loss: 0.457132, acc 0.831800\n",
      "5.36s, epoch [035\\1500]: train D loss: 0.4634, train F loss: 0.407121, acc 0.848200\n",
      "5.34s, epoch [036\\1500]: train D loss: 0.4578, train F loss: 0.414800, acc 0.843400\n",
      "5.34s, epoch [037\\1500]: train D loss: 0.4217, train F loss: 0.396110, acc 0.847000\n",
      "5.34s, epoch [038\\1500]: train D loss: 0.4274, train F loss: 0.342264, acc 0.871000\n",
      "5.35s, epoch [039\\1500]: train D loss: 0.4081, train F loss: 0.365269, acc 0.870400\n",
      "5.35s, epoch [040\\1500]: train D loss: 0.4423, train F loss: 0.364649, acc 0.862800\n",
      "5.34s, epoch [041\\1500]: train D loss: 0.4143, train F loss: 0.364707, acc 0.861000\n",
      "5.34s, epoch [042\\1500]: train D loss: 0.4241, train F loss: 0.315488, acc 0.875600\n",
      "5.34s, epoch [043\\1500]: train D loss: 0.3935, train F loss: 0.298043, acc 0.889600\n",
      "5.34s, epoch [044\\1500]: train D loss: 0.4324, train F loss: 0.275709, acc 0.898200\n",
      "5.34s, epoch [045\\1500]: train D loss: 0.3934, train F loss: 0.300061, acc 0.889000\n",
      "5.35s, epoch [046\\1500]: train D loss: 0.3980, train F loss: 0.254233, acc 0.907800\n",
      "5.35s, epoch [047\\1500]: train D loss: 0.4088, train F loss: 0.239321, acc 0.906400\n",
      "5.34s, epoch [048\\1500]: train D loss: 0.3902, train F loss: 0.257001, acc 0.905000\n",
      "5.34s, epoch [049\\1500]: train D loss: 0.4003, train F loss: 0.217579, acc 0.913200\n",
      "5.35s, epoch [050\\1500]: train D loss: 0.4002, train F loss: 0.186510, acc 0.926200\n",
      "5.57s, epoch [051\\1500]: train D loss: 0.3897, train F loss: 0.255253, acc 0.906400\n",
      "5.36s, epoch [052\\1500]: train D loss: 0.3853, train F loss: 0.235783, acc 0.907200\n",
      "5.37s, epoch [053\\1500]: train D loss: 0.3973, train F loss: 0.193665, acc 0.917800\n",
      "5.35s, epoch [054\\1500]: train D loss: 0.3906, train F loss: 0.182081, acc 0.924600\n",
      "5.36s, epoch [055\\1500]: train D loss: 0.3903, train F loss: 0.185725, acc 0.927800\n",
      "5.37s, epoch [056\\1500]: train D loss: 0.3762, train F loss: 0.174531, acc 0.932400\n",
      "5.35s, epoch [057\\1500]: train D loss: 0.3801, train F loss: 0.155005, acc 0.934400\n",
      "5.37s, epoch [058\\1500]: train D loss: 0.4012, train F loss: 0.179332, acc 0.927000\n",
      "5.36s, epoch [059\\1500]: train D loss: 0.3907, train F loss: 0.143474, acc 0.941000\n",
      "5.35s, epoch [060\\1500]: train D loss: 0.3864, train F loss: 0.141609, acc 0.937800\n",
      "5.36s, epoch [061\\1500]: train D loss: 0.3850, train F loss: 0.130045, acc 0.941000\n",
      "5.36s, epoch [062\\1500]: train D loss: 0.3742, train F loss: 0.125584, acc 0.945400\n",
      "5.35s, epoch [063\\1500]: train D loss: 0.4034, train F loss: 0.156535, acc 0.931800\n",
      "5.36s, epoch [064\\1500]: train D loss: 0.3781, train F loss: 0.120344, acc 0.948400\n",
      "5.35s, epoch [065\\1500]: train D loss: 0.3922, train F loss: 0.129137, acc 0.941800\n",
      "5.36s, epoch [066\\1500]: train D loss: 0.3837, train F loss: 0.118818, acc 0.947600\n",
      "5.36s, epoch [067\\1500]: train D loss: 0.3870, train F loss: 0.112492, acc 0.955800\n",
      "5.35s, epoch [068\\1500]: train D loss: 0.3918, train F loss: 0.186265, acc 0.925800\n",
      "5.36s, epoch [069\\1500]: train D loss: 0.3858, train F loss: 0.123265, acc 0.944800\n",
      "5.36s, epoch [070\\1500]: train D loss: 0.3921, train F loss: 0.140388, acc 0.939400\n",
      "5.36s, epoch [071\\1500]: train D loss: 0.3657, train F loss: 0.138695, acc 0.940400\n",
      "5.36s, epoch [072\\1500]: train D loss: 0.3555, train F loss: 0.111290, acc 0.952200\n",
      "5.37s, epoch [073\\1500]: train D loss: 0.3713, train F loss: 0.091651, acc 0.955000\n",
      "5.36s, epoch [074\\1500]: train D loss: 0.3821, train F loss: 0.087728, acc 0.960000\n",
      "5.38s, epoch [075\\1500]: train D loss: 0.3735, train F loss: 0.091205, acc 0.961000\n",
      "5.40s, epoch [076\\1500]: train D loss: 0.3669, train F loss: 0.085534, acc 0.959800\n",
      "5.40s, epoch [077\\1500]: train D loss: 0.3861, train F loss: 0.085276, acc 0.959800\n",
      "5.43s, epoch [078\\1500]: train D loss: 0.3847, train F loss: 0.105480, acc 0.954600\n",
      "5.41s, epoch [079\\1500]: train D loss: 0.3694, train F loss: 0.089109, acc 0.958600\n",
      "5.41s, epoch [080\\1500]: train D loss: 0.3750, train F loss: 0.098098, acc 0.955600\n",
      "5.41s, epoch [081\\1500]: train D loss: 0.3736, train F loss: 0.130073, acc 0.946400\n",
      "5.41s, epoch [082\\1500]: train D loss: 0.3700, train F loss: 0.124535, acc 0.943800\n",
      "5.41s, epoch [083\\1500]: train D loss: 0.3632, train F loss: 0.095973, acc 0.959200\n",
      "5.38s, epoch [084\\1500]: train D loss: 0.3668, train F loss: 0.081967, acc 0.961200\n",
      "5.38s, epoch [085\\1500]: train D loss: 0.3731, train F loss: 0.070541, acc 0.965600\n",
      "5.38s, epoch [086\\1500]: train D loss: 0.3712, train F loss: 0.121655, acc 0.948200\n",
      "5.38s, epoch [087\\1500]: train D loss: 0.3778, train F loss: 0.091374, acc 0.958000\n",
      "5.38s, epoch [088\\1500]: train D loss: 0.3588, train F loss: 0.068846, acc 0.962800\n",
      "5.38s, epoch [089\\1500]: train D loss: 0.3545, train F loss: 0.061607, acc 0.969200\n",
      "5.38s, epoch [090\\1500]: train D loss: 0.3682, train F loss: 0.068226, acc 0.970600\n",
      "5.39s, epoch [091\\1500]: train D loss: 0.3715, train F loss: 0.105783, acc 0.955000\n",
      "5.39s, epoch [092\\1500]: train D loss: 0.3848, train F loss: 0.083859, acc 0.961200\n",
      "5.38s, epoch [093\\1500]: train D loss: 0.3663, train F loss: 0.067055, acc 0.965600\n",
      "5.38s, epoch [094\\1500]: train D loss: 0.3730, train F loss: 0.057596, acc 0.969400\n",
      "5.39s, epoch [095\\1500]: train D loss: 0.3684, train F loss: 0.047576, acc 0.974000\n",
      "5.38s, epoch [096\\1500]: train D loss: 0.3810, train F loss: 0.075439, acc 0.964600\n",
      "5.39s, epoch [097\\1500]: train D loss: 0.3757, train F loss: 0.106260, acc 0.953000\n",
      "5.38s, epoch [098\\1500]: train D loss: 0.3680, train F loss: 0.087656, acc 0.959800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.38s, epoch [099\\1500]: train D loss: 0.3644, train F loss: 0.057117, acc 0.969000\n",
      "5.40s, epoch [100\\1500]: train D loss: 0.3736, train F loss: 0.067013, acc 0.963600\n",
      "5.33s, epoch [101\\1500]: train D loss: 0.4257, train F loss: 0.175113, acc 0.928000\n",
      "5.33s, epoch [102\\1500]: train D loss: 0.4048, train F loss: 0.119121, acc 0.948200\n",
      "5.32s, epoch [103\\1500]: train D loss: 0.4150, train F loss: 0.163264, acc 0.933000\n",
      "5.32s, epoch [104\\1500]: train D loss: 0.4230, train F loss: 0.138692, acc 0.940600\n",
      "5.31s, epoch [105\\1500]: train D loss: 0.3856, train F loss: 0.107592, acc 0.951400\n",
      "5.32s, epoch [106\\1500]: train D loss: 0.3888, train F loss: 0.107203, acc 0.951600\n",
      "5.32s, epoch [107\\1500]: train D loss: 0.3813, train F loss: 0.058599, acc 0.970800\n",
      "5.32s, epoch [108\\1500]: train D loss: 0.3824, train F loss: 0.066186, acc 0.965200\n",
      "5.32s, epoch [109\\1500]: train D loss: 0.3890, train F loss: 0.119357, acc 0.950000\n",
      "5.32s, epoch [110\\1500]: train D loss: 0.3955, train F loss: 0.070211, acc 0.963800\n",
      "5.31s, epoch [111\\1500]: train D loss: 0.3947, train F loss: 0.058135, acc 0.963800\n",
      "5.32s, epoch [112\\1500]: train D loss: 0.3960, train F loss: 0.083782, acc 0.960200\n",
      "5.32s, epoch [113\\1500]: train D loss: 0.4054, train F loss: 0.054305, acc 0.968000\n",
      "5.32s, epoch [114\\1500]: train D loss: 0.3843, train F loss: 0.063996, acc 0.965400\n",
      "5.32s, epoch [115\\1500]: train D loss: 0.3871, train F loss: 0.054129, acc 0.969600\n",
      "5.32s, epoch [116\\1500]: train D loss: 0.3842, train F loss: 0.054664, acc 0.969800\n",
      "5.32s, epoch [117\\1500]: train D loss: 0.3744, train F loss: 0.059196, acc 0.967600\n",
      "5.33s, epoch [118\\1500]: train D loss: 0.3965, train F loss: 0.058368, acc 0.968000\n",
      "5.32s, epoch [119\\1500]: train D loss: 0.3916, train F loss: 0.090718, acc 0.956200\n",
      "5.33s, epoch [120\\1500]: train D loss: 0.3903, train F loss: 0.035936, acc 0.977200\n",
      "5.32s, epoch [121\\1500]: train D loss: 0.4014, train F loss: 0.057646, acc 0.965600\n",
      "5.31s, epoch [122\\1500]: train D loss: 0.3839, train F loss: 0.059483, acc 0.969200\n",
      "5.32s, epoch [123\\1500]: train D loss: 0.4064, train F loss: 0.092130, acc 0.956200\n",
      "5.32s, epoch [124\\1500]: train D loss: 0.3942, train F loss: 0.046050, acc 0.970400\n",
      "5.31s, epoch [125\\1500]: train D loss: 0.3885, train F loss: 0.043495, acc 0.969600\n",
      "5.32s, epoch [126\\1500]: train D loss: 0.4144, train F loss: 0.048059, acc 0.968800\n",
      "5.32s, epoch [127\\1500]: train D loss: 0.4055, train F loss: 0.051921, acc 0.970800\n",
      "5.32s, epoch [128\\1500]: train D loss: 0.4098, train F loss: 0.031498, acc 0.978200\n",
      "5.32s, epoch [129\\1500]: train D loss: 0.4058, train F loss: 0.063041, acc 0.968800\n",
      "5.31s, epoch [130\\1500]: train D loss: 0.4004, train F loss: 0.051196, acc 0.969600\n",
      "5.33s, epoch [131\\1500]: train D loss: 0.4060, train F loss: 0.027376, acc 0.976600\n",
      "5.32s, epoch [132\\1500]: train D loss: 0.4032, train F loss: 0.041608, acc 0.974400\n",
      "5.32s, epoch [133\\1500]: train D loss: 0.4069, train F loss: 0.030350, acc 0.976400\n",
      "5.32s, epoch [134\\1500]: train D loss: 0.4017, train F loss: 0.053099, acc 0.967800\n",
      "5.33s, epoch [135\\1500]: train D loss: 0.4077, train F loss: 0.048579, acc 0.970000\n",
      "5.32s, epoch [136\\1500]: train D loss: 0.3881, train F loss: 0.031979, acc 0.979200\n",
      "5.32s, epoch [137\\1500]: train D loss: 0.4063, train F loss: 0.030482, acc 0.977400\n",
      "5.32s, epoch [138\\1500]: train D loss: 0.4277, train F loss: 0.064192, acc 0.966400\n",
      "5.31s, epoch [139\\1500]: train D loss: 0.4185, train F loss: 0.084524, acc 0.960000\n",
      "5.33s, epoch [140\\1500]: train D loss: 0.4229, train F loss: 0.087385, acc 0.957600\n",
      "5.34s, epoch [141\\1500]: train D loss: 0.4079, train F loss: 0.042660, acc 0.973400\n",
      "5.31s, epoch [142\\1500]: train D loss: 0.4064, train F loss: 0.037185, acc 0.975200\n",
      "5.32s, epoch [143\\1500]: train D loss: 0.4119, train F loss: 0.031358, acc 0.975200\n",
      "5.31s, epoch [144\\1500]: train D loss: 0.3987, train F loss: 0.036391, acc 0.978000\n",
      "5.31s, epoch [145\\1500]: train D loss: 0.4221, train F loss: 0.063867, acc 0.963000\n",
      "5.32s, epoch [146\\1500]: train D loss: 0.3940, train F loss: 0.034403, acc 0.974000\n",
      "5.31s, epoch [147\\1500]: train D loss: 0.4124, train F loss: 0.037835, acc 0.973000\n",
      "5.31s, epoch [148\\1500]: train D loss: 0.3944, train F loss: 0.033554, acc 0.978800\n",
      "5.32s, epoch [149\\1500]: train D loss: 0.3953, train F loss: 0.027415, acc 0.978200\n",
      "5.32s, epoch [150\\1500]: train D loss: 0.4222, train F loss: 0.051123, acc 0.972200\n",
      "5.32s, epoch [151\\1500]: train D loss: 0.4322, train F loss: 0.041758, acc 0.972400\n",
      "5.31s, epoch [152\\1500]: train D loss: 0.4187, train F loss: 0.055559, acc 0.970000\n",
      "5.32s, epoch [153\\1500]: train D loss: 0.4277, train F loss: 0.056409, acc 0.968600\n",
      "5.32s, epoch [154\\1500]: train D loss: 0.4283, train F loss: 0.028437, acc 0.976800\n",
      "5.31s, epoch [155\\1500]: train D loss: 0.4170, train F loss: 0.040364, acc 0.970200\n",
      "5.32s, epoch [156\\1500]: train D loss: 0.4126, train F loss: 0.055981, acc 0.968800\n",
      "5.32s, epoch [157\\1500]: train D loss: 0.4221, train F loss: 0.032353, acc 0.975000\n",
      "5.31s, epoch [158\\1500]: train D loss: 0.3998, train F loss: 0.025898, acc 0.977600\n",
      "5.31s, epoch [159\\1500]: train D loss: 0.3832, train F loss: 0.017133, acc 0.982400\n",
      "5.32s, epoch [160\\1500]: train D loss: 0.4247, train F loss: 0.030239, acc 0.974800\n",
      "5.31s, epoch [161\\1500]: train D loss: 0.4152, train F loss: 0.037425, acc 0.975600\n",
      "5.31s, epoch [162\\1500]: train D loss: 0.4187, train F loss: 0.023179, acc 0.979600\n",
      "5.33s, epoch [163\\1500]: train D loss: 0.4107, train F loss: 0.028660, acc 0.976600\n",
      "5.32s, epoch [164\\1500]: train D loss: 0.4186, train F loss: 0.024786, acc 0.981800\n",
      "5.32s, epoch [165\\1500]: train D loss: 0.4320, train F loss: 0.056470, acc 0.967800\n",
      "5.31s, epoch [166\\1500]: train D loss: 0.4091, train F loss: 0.019398, acc 0.981000\n",
      "5.33s, epoch [167\\1500]: train D loss: 0.4030, train F loss: 0.025323, acc 0.978800\n",
      "5.32s, epoch [168\\1500]: train D loss: 0.4380, train F loss: 0.073456, acc 0.961800\n",
      "5.29s, epoch [169\\1500]: train D loss: 0.4238, train F loss: 0.035258, acc 0.978000\n",
      "5.28s, epoch [170\\1500]: train D loss: 0.3928, train F loss: 0.012485, acc 0.983800\n",
      "5.26s, epoch [171\\1500]: train D loss: 0.4235, train F loss: 0.016399, acc 0.981200\n",
      "5.26s, epoch [172\\1500]: train D loss: 0.4180, train F loss: 0.019604, acc 0.980600\n",
      "5.27s, epoch [173\\1500]: train D loss: 0.4161, train F loss: 0.021689, acc 0.980400\n",
      "5.26s, epoch [174\\1500]: train D loss: 0.4107, train F loss: 0.018570, acc 0.978400\n",
      "5.27s, epoch [175\\1500]: train D loss: 0.4163, train F loss: 0.027425, acc 0.977600\n",
      "5.27s, epoch [176\\1500]: train D loss: 0.4248, train F loss: 0.027876, acc 0.977800\n",
      "5.28s, epoch [177\\1500]: train D loss: 0.4201, train F loss: 0.024496, acc 0.977800\n",
      "5.26s, epoch [178\\1500]: train D loss: 0.4155, train F loss: 0.010184, acc 0.983800\n",
      "5.26s, epoch [179\\1500]: train D loss: 0.4231, train F loss: 0.021814, acc 0.982200\n",
      "5.27s, epoch [180\\1500]: train D loss: 0.4853, train F loss: 0.174304, acc 0.939800\n",
      "5.26s, epoch [181\\1500]: train D loss: 0.4295, train F loss: 0.046328, acc 0.973400\n",
      "5.28s, epoch [182\\1500]: train D loss: 0.4124, train F loss: 0.023050, acc 0.979200\n",
      "5.27s, epoch [183\\1500]: train D loss: 0.4112, train F loss: 0.019166, acc 0.979200\n",
      "5.26s, epoch [184\\1500]: train D loss: 0.4160, train F loss: 0.016393, acc 0.981400\n",
      "5.26s, epoch [185\\1500]: train D loss: 0.3961, train F loss: 0.020755, acc 0.979800\n",
      "5.28s, epoch [186\\1500]: train D loss: 0.4249, train F loss: 0.008930, acc 0.983800\n",
      "5.27s, epoch [187\\1500]: train D loss: 0.4085, train F loss: 0.019596, acc 0.982400\n",
      "5.27s, epoch [188\\1500]: train D loss: 0.4229, train F loss: 0.020323, acc 0.978800\n",
      "5.27s, epoch [189\\1500]: train D loss: 0.4255, train F loss: 0.009592, acc 0.982200\n",
      "5.33s, epoch [190\\1500]: train D loss: 0.4290, train F loss: 0.024927, acc 0.978800\n",
      "5.33s, epoch [191\\1500]: train D loss: 0.4227, train F loss: 0.016823, acc 0.980200\n",
      "5.33s, epoch [192\\1500]: train D loss: 0.4196, train F loss: 0.011874, acc 0.984400\n",
      "5.32s, epoch [193\\1500]: train D loss: 0.4426, train F loss: 0.023441, acc 0.977000\n",
      "5.33s, epoch [194\\1500]: train D loss: 0.4448, train F loss: 0.035268, acc 0.973000\n",
      "5.32s, epoch [195\\1500]: train D loss: 0.4301, train F loss: 0.007329, acc 0.982000\n",
      "5.32s, epoch [196\\1500]: train D loss: 0.4175, train F loss: 0.006818, acc 0.982400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.33s, epoch [197\\1500]: train D loss: 0.4457, train F loss: 0.011026, acc 0.982600\n",
      "5.32s, epoch [198\\1500]: train D loss: 0.4297, train F loss: 0.018499, acc 0.981400\n",
      "5.33s, epoch [199\\1500]: train D loss: 0.4233, train F loss: 0.012362, acc 0.982600\n",
      "5.32s, epoch [200\\1500]: train D loss: 0.4311, train F loss: 0.021253, acc 0.978800\n",
      "5.30s, epoch [201\\1500]: train D loss: 0.4321, train F loss: 0.035692, acc 0.975400\n",
      "5.28s, epoch [202\\1500]: train D loss: 0.4292, train F loss: 0.013662, acc 0.982400\n",
      "5.29s, epoch [203\\1500]: train D loss: 0.4297, train F loss: 0.006024, acc 0.984400\n",
      "5.30s, epoch [204\\1500]: train D loss: 0.4377, train F loss: 0.009543, acc 0.985000\n",
      "5.30s, epoch [205\\1500]: train D loss: 0.4525, train F loss: 0.021784, acc 0.978600\n",
      "5.32s, epoch [206\\1500]: train D loss: 0.4270, train F loss: 0.022732, acc 0.977800\n",
      "5.29s, epoch [207\\1500]: train D loss: 0.4395, train F loss: 0.006151, acc 0.983200\n",
      "5.30s, epoch [208\\1500]: train D loss: 0.4330, train F loss: 0.014550, acc 0.982200\n",
      "5.29s, epoch [209\\1500]: train D loss: 0.4260, train F loss: 0.003460, acc 0.984000\n",
      "5.29s, epoch [210\\1500]: train D loss: 0.4336, train F loss: 0.027734, acc 0.978800\n",
      "5.38s, epoch [211\\1500]: train D loss: 0.4170, train F loss: 0.002533, acc 0.986200\n",
      "5.29s, epoch [212\\1500]: train D loss: 0.4277, train F loss: -0.001085, acc 0.985600\n",
      "5.29s, epoch [213\\1500]: train D loss: 0.4433, train F loss: 0.009317, acc 0.982800\n",
      "5.29s, epoch [214\\1500]: train D loss: 0.4578, train F loss: 0.013844, acc 0.979400\n",
      "5.29s, epoch [215\\1500]: train D loss: 0.4429, train F loss: 0.006510, acc 0.983400\n",
      "5.30s, epoch [216\\1500]: train D loss: 0.4332, train F loss: 0.034580, acc 0.976600\n",
      "5.31s, epoch [217\\1500]: train D loss: 0.4301, train F loss: 0.013328, acc 0.982400\n",
      "5.32s, epoch [218\\1500]: train D loss: 0.4362, train F loss: 0.005005, acc 0.984200\n",
      "5.31s, epoch [219\\1500]: train D loss: 0.4347, train F loss: 0.004360, acc 0.985200\n",
      "5.32s, epoch [220\\1500]: train D loss: 0.4450, train F loss: 0.005363, acc 0.984200\n",
      "5.32s, epoch [221\\1500]: train D loss: 0.4290, train F loss: 0.011353, acc 0.982200\n",
      "5.33s, epoch [222\\1500]: train D loss: 0.4627, train F loss: 0.027844, acc 0.977600\n",
      "5.31s, epoch [223\\1500]: train D loss: 0.4475, train F loss: 0.015325, acc 0.981000\n",
      "5.31s, epoch [224\\1500]: train D loss: 0.4403, train F loss: 0.003983, acc 0.982600\n",
      "5.31s, epoch [225\\1500]: train D loss: 0.4388, train F loss: 0.009180, acc 0.983000\n",
      "5.32s, epoch [226\\1500]: train D loss: 0.4216, train F loss: 0.000767, acc 0.986200\n",
      "5.32s, epoch [227\\1500]: train D loss: 0.4204, train F loss: 0.012653, acc 0.983000\n",
      "5.32s, epoch [228\\1500]: train D loss: 0.4763, train F loss: 0.057696, acc 0.969000\n",
      "5.32s, epoch [229\\1500]: train D loss: 0.4403, train F loss: 0.010686, acc 0.986000\n",
      "5.32s, epoch [230\\1500]: train D loss: 0.4580, train F loss: 0.020396, acc 0.980600\n",
      "5.33s, epoch [231\\1500]: train D loss: 0.4513, train F loss: 0.028019, acc 0.977000\n",
      "5.32s, epoch [232\\1500]: train D loss: 0.4475, train F loss: 0.004754, acc 0.983200\n",
      "5.32s, epoch [233\\1500]: train D loss: 0.4542, train F loss: 0.005078, acc 0.984200\n",
      "5.32s, epoch [234\\1500]: train D loss: 0.4432, train F loss: 0.020843, acc 0.980400\n",
      "5.31s, epoch [235\\1500]: train D loss: 0.4296, train F loss: 0.013338, acc 0.981400\n",
      "5.31s, epoch [236\\1500]: train D loss: 0.4310, train F loss: 0.017963, acc 0.980800\n",
      "5.32s, epoch [237\\1500]: train D loss: 0.4266, train F loss: 0.001440, acc 0.987200\n",
      "5.33s, epoch [238\\1500]: train D loss: 0.4522, train F loss: 0.009513, acc 0.982800\n",
      "5.31s, epoch [239\\1500]: train D loss: 0.4368, train F loss: 0.007974, acc 0.982400\n",
      "5.32s, epoch [240\\1500]: train D loss: 0.4334, train F loss: 0.019407, acc 0.981800\n",
      "5.31s, epoch [241\\1500]: train D loss: 0.4255, train F loss: -0.003937, acc 0.986400\n",
      "5.32s, epoch [242\\1500]: train D loss: 0.4303, train F loss: -0.001384, acc 0.986400\n",
      "5.32s, epoch [243\\1500]: train D loss: 0.4608, train F loss: 0.049435, acc 0.971400\n",
      "5.31s, epoch [244\\1500]: train D loss: 0.4317, train F loss: 0.000041, acc 0.987200\n",
      "5.32s, epoch [245\\1500]: train D loss: 0.4375, train F loss: 0.013348, acc 0.981800\n",
      "5.32s, epoch [246\\1500]: train D loss: 0.4284, train F loss: -0.001302, acc 0.989600\n",
      "5.32s, epoch [247\\1500]: train D loss: 0.4519, train F loss: 0.003936, acc 0.983200\n",
      "5.32s, epoch [248\\1500]: train D loss: 0.4630, train F loss: 0.048154, acc 0.968800\n",
      "5.34s, epoch [249\\1500]: train D loss: 0.4516, train F loss: 0.005240, acc 0.983200\n",
      "5.33s, epoch [250\\1500]: train D loss: 0.4214, train F loss: 0.014209, acc 0.984600\n",
      "5.33s, epoch [251\\1500]: train D loss: 0.4678, train F loss: 0.004829, acc 0.983800\n",
      "5.32s, epoch [252\\1500]: train D loss: 0.4530, train F loss: 0.007472, acc 0.982400\n",
      "5.32s, epoch [253\\1500]: train D loss: 0.4419, train F loss: -0.008044, acc 0.988800\n",
      "5.32s, epoch [254\\1500]: train D loss: 0.4387, train F loss: -0.003758, acc 0.987800\n",
      "5.32s, epoch [255\\1500]: train D loss: 0.4370, train F loss: -0.002263, acc 0.986600\n",
      "5.33s, epoch [256\\1500]: train D loss: 0.4268, train F loss: 0.003635, acc 0.985600\n",
      "5.32s, epoch [257\\1500]: train D loss: 0.4601, train F loss: 0.001840, acc 0.985400\n",
      "5.33s, epoch [258\\1500]: train D loss: 0.4591, train F loss: 0.014197, acc 0.983200\n",
      "5.32s, epoch [259\\1500]: train D loss: 0.4541, train F loss: 0.007645, acc 0.984600\n",
      "5.33s, epoch [260\\1500]: train D loss: 0.4431, train F loss: 0.011857, acc 0.982000\n",
      "5.33s, epoch [261\\1500]: train D loss: 0.4454, train F loss: 0.012238, acc 0.981600\n",
      "5.32s, epoch [262\\1500]: train D loss: 0.4531, train F loss: 0.007116, acc 0.983000\n",
      "5.33s, epoch [263\\1500]: train D loss: 0.4427, train F loss: 0.011678, acc 0.982800\n",
      "5.32s, epoch [264\\1500]: train D loss: 0.4499, train F loss: 0.003612, acc 0.984400\n",
      "5.33s, epoch [265\\1500]: train D loss: 0.4386, train F loss: 0.001841, acc 0.986200\n",
      "5.32s, epoch [266\\1500]: train D loss: 0.4358, train F loss: 0.000878, acc 0.985200\n",
      "5.31s, epoch [267\\1500]: train D loss: 0.4432, train F loss: 0.003539, acc 0.983600\n",
      "5.34s, epoch [268\\1500]: train D loss: 0.4522, train F loss: 0.010639, acc 0.983800\n",
      "5.32s, epoch [269\\1500]: train D loss: 0.4193, train F loss: 0.003802, acc 0.986400\n",
      "5.32s, epoch [270\\1500]: train D loss: 0.4524, train F loss: 0.001562, acc 0.985600\n",
      "5.32s, epoch [271\\1500]: train D loss: 0.4587, train F loss: 0.005795, acc 0.985400\n",
      "5.33s, epoch [272\\1500]: train D loss: 0.4598, train F loss: 0.006148, acc 0.984200\n",
      "5.32s, epoch [273\\1500]: train D loss: 0.4585, train F loss: 0.003712, acc 0.983000\n",
      "5.32s, epoch [274\\1500]: train D loss: 0.4363, train F loss: -0.004467, acc 0.986400\n",
      "5.32s, epoch [275\\1500]: train D loss: 0.4565, train F loss: 0.004409, acc 0.984400\n",
      "5.31s, epoch [276\\1500]: train D loss: 0.4583, train F loss: 0.003659, acc 0.985200\n",
      "5.33s, epoch [277\\1500]: train D loss: 0.4433, train F loss: 0.000803, acc 0.985400\n",
      "5.32s, epoch [278\\1500]: train D loss: 0.4617, train F loss: 0.008407, acc 0.983600\n",
      "5.31s, epoch [279\\1500]: train D loss: 0.4644, train F loss: 0.009800, acc 0.982600\n",
      "5.33s, epoch [280\\1500]: train D loss: 0.4702, train F loss: 0.004736, acc 0.984000\n",
      "5.33s, epoch [281\\1500]: train D loss: 0.4665, train F loss: -0.000001, acc 0.984800\n",
      "5.32s, epoch [282\\1500]: train D loss: 0.4600, train F loss: 0.002565, acc 0.986400\n",
      "5.33s, epoch [283\\1500]: train D loss: 0.4446, train F loss: -0.006768, acc 0.988400\n",
      "5.32s, epoch [284\\1500]: train D loss: 0.4325, train F loss: -0.006912, acc 0.987600\n",
      "5.32s, epoch [285\\1500]: train D loss: 0.4628, train F loss: 0.009962, acc 0.981600\n",
      "5.32s, epoch [286\\1500]: train D loss: 0.4652, train F loss: 0.019627, acc 0.980200\n",
      "5.30s, epoch [287\\1500]: train D loss: 0.4457, train F loss: -0.005123, acc 0.985000\n",
      "5.31s, epoch [288\\1500]: train D loss: 0.4469, train F loss: -0.004645, acc 0.987800\n",
      "5.32s, epoch [289\\1500]: train D loss: 0.4480, train F loss: -0.009086, acc 0.987800\n",
      "5.31s, epoch [290\\1500]: train D loss: 0.4567, train F loss: -0.001355, acc 0.985000\n",
      "5.32s, epoch [291\\1500]: train D loss: 0.4468, train F loss: -0.000181, acc 0.987400\n",
      "5.32s, epoch [292\\1500]: train D loss: 0.4563, train F loss: 0.001019, acc 0.985200\n",
      "5.31s, epoch [293\\1500]: train D loss: 0.4604, train F loss: -0.008975, acc 0.988200\n",
      "5.33s, epoch [294\\1500]: train D loss: 0.4563, train F loss: 0.008049, acc 0.982800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.31s, epoch [295\\1500]: train D loss: 0.4570, train F loss: 0.003668, acc 0.984600\n",
      "5.32s, epoch [296\\1500]: train D loss: 0.4640, train F loss: -0.001218, acc 0.985000\n",
      "5.31s, epoch [297\\1500]: train D loss: 0.4384, train F loss: -0.008755, acc 0.989200\n",
      "5.32s, epoch [298\\1500]: train D loss: 0.4526, train F loss: -0.010881, acc 0.990200\n",
      "5.31s, epoch [299\\1500]: train D loss: 0.4591, train F loss: -0.006166, acc 0.988000\n",
      "5.41s, epoch [300\\1500]: train D loss: 0.4655, train F loss: 0.009886, acc 0.983000\n",
      "5.30s, epoch [301\\1500]: train D loss: 0.4524, train F loss: 0.001738, acc 0.985400\n",
      "5.31s, epoch [302\\1500]: train D loss: 0.4646, train F loss: 0.001997, acc 0.984800\n",
      "5.35s, epoch [303\\1500]: train D loss: 0.4472, train F loss: -0.004177, acc 0.986200\n",
      "5.37s, epoch [304\\1500]: train D loss: 0.4429, train F loss: -0.002749, acc 0.987200\n",
      "5.35s, epoch [305\\1500]: train D loss: 0.4435, train F loss: -0.002782, acc 0.986600\n",
      "5.38s, epoch [306\\1500]: train D loss: 0.4618, train F loss: -0.012583, acc 0.989800\n",
      "5.41s, epoch [307\\1500]: train D loss: 0.4659, train F loss: 0.001693, acc 0.985000\n",
      "5.33s, epoch [308\\1500]: train D loss: 0.4840, train F loss: 0.006402, acc 0.982800\n",
      "5.33s, epoch [309\\1500]: train D loss: 0.4889, train F loss: 0.016316, acc 0.981600\n",
      "5.33s, epoch [310\\1500]: train D loss: 0.4693, train F loss: -0.010789, acc 0.988600\n",
      "5.36s, epoch [311\\1500]: train D loss: 0.4570, train F loss: -0.002578, acc 0.986600\n",
      "5.26s, epoch [312\\1500]: train D loss: 0.4811, train F loss: 0.004622, acc 0.984800\n",
      "5.24s, epoch [313\\1500]: train D loss: 0.4625, train F loss: 0.006646, acc 0.982200\n",
      "5.24s, epoch [314\\1500]: train D loss: 0.4550, train F loss: -0.014122, acc 0.990600\n",
      "5.25s, epoch [315\\1500]: train D loss: 0.4496, train F loss: -0.005654, acc 0.987600\n",
      "5.24s, epoch [316\\1500]: train D loss: 0.4679, train F loss: -0.000712, acc 0.986800\n",
      "5.24s, epoch [317\\1500]: train D loss: 0.4659, train F loss: -0.008605, acc 0.986600\n",
      "5.23s, epoch [318\\1500]: train D loss: 0.4653, train F loss: 0.017458, acc 0.986000\n",
      "5.24s, epoch [319\\1500]: train D loss: 0.4931, train F loss: 0.054436, acc 0.968200\n",
      "5.24s, epoch [320\\1500]: train D loss: 0.4499, train F loss: 0.001642, acc 0.985800\n",
      "5.24s, epoch [321\\1500]: train D loss: 0.4593, train F loss: 0.000457, acc 0.986600\n",
      "5.23s, epoch [322\\1500]: train D loss: 0.4559, train F loss: 0.000525, acc 0.985400\n",
      "5.24s, epoch [323\\1500]: train D loss: 0.4585, train F loss: 0.000530, acc 0.985800\n",
      "5.23s, epoch [324\\1500]: train D loss: 0.4644, train F loss: -0.011369, acc 0.988800\n",
      "5.24s, epoch [325\\1500]: train D loss: 0.4423, train F loss: -0.004094, acc 0.987200\n",
      "5.23s, epoch [326\\1500]: train D loss: 0.4527, train F loss: -0.006416, acc 0.988600\n",
      "5.23s, epoch [327\\1500]: train D loss: 0.4576, train F loss: 0.002971, acc 0.981800\n",
      "5.24s, epoch [328\\1500]: train D loss: 0.4497, train F loss: -0.020824, acc 0.992600\n",
      "5.25s, epoch [329\\1500]: train D loss: 0.4472, train F loss: -0.015503, acc 0.991400\n",
      "5.23s, epoch [330\\1500]: train D loss: 0.4614, train F loss: 0.003118, acc 0.985600\n",
      "5.24s, epoch [331\\1500]: train D loss: 0.4444, train F loss: -0.012928, acc 0.991000\n",
      "5.23s, epoch [332\\1500]: train D loss: 0.4582, train F loss: 0.024415, acc 0.978200\n",
      "5.24s, epoch [333\\1500]: train D loss: 0.4385, train F loss: -0.012227, acc 0.991000\n",
      "5.25s, epoch [334\\1500]: train D loss: 0.4448, train F loss: -0.004316, acc 0.988000\n",
      "5.24s, epoch [335\\1500]: train D loss: 0.4537, train F loss: -0.008329, acc 0.987800\n",
      "5.25s, epoch [336\\1500]: train D loss: 0.4467, train F loss: 0.000111, acc 0.986000\n",
      "5.24s, epoch [337\\1500]: train D loss: 0.4522, train F loss: -0.000144, acc 0.986000\n",
      "5.24s, epoch [338\\1500]: train D loss: 0.4649, train F loss: 0.001678, acc 0.986600\n",
      "5.24s, epoch [339\\1500]: train D loss: 0.4442, train F loss: -0.015525, acc 0.990200\n",
      "5.23s, epoch [340\\1500]: train D loss: 0.4652, train F loss: -0.003866, acc 0.987600\n",
      "5.24s, epoch [341\\1500]: train D loss: 0.4576, train F loss: -0.008243, acc 0.988000\n",
      "5.24s, epoch [342\\1500]: train D loss: 0.4566, train F loss: -0.018974, acc 0.991400\n",
      "5.24s, epoch [343\\1500]: train D loss: 0.4828, train F loss: 0.007721, acc 0.980800\n",
      "5.24s, epoch [344\\1500]: train D loss: 0.4541, train F loss: -0.015711, acc 0.990800\n",
      "5.24s, epoch [345\\1500]: train D loss: 0.4752, train F loss: -0.016086, acc 0.990000\n",
      "5.23s, epoch [346\\1500]: train D loss: 0.4616, train F loss: -0.009331, acc 0.987800\n",
      "5.25s, epoch [347\\1500]: train D loss: 0.4830, train F loss: -0.001070, acc 0.983800\n",
      "5.25s, epoch [348\\1500]: train D loss: 0.4798, train F loss: -0.002663, acc 0.986000\n",
      "5.25s, epoch [349\\1500]: train D loss: 0.4833, train F loss: 0.001279, acc 0.985800\n",
      "5.24s, epoch [350\\1500]: train D loss: 0.4712, train F loss: 0.000796, acc 0.985200\n",
      "5.25s, epoch [351\\1500]: train D loss: 0.4765, train F loss: 0.002539, acc 0.984200\n",
      "5.24s, epoch [352\\1500]: train D loss: 0.4860, train F loss: -0.000447, acc 0.983600\n",
      "5.23s, epoch [353\\1500]: train D loss: 0.4514, train F loss: -0.011373, acc 0.987800\n",
      "5.25s, epoch [354\\1500]: train D loss: 0.4783, train F loss: -0.005707, acc 0.987600\n",
      "5.24s, epoch [355\\1500]: train D loss: 0.4713, train F loss: 0.013084, acc 0.981400\n",
      "5.27s, epoch [356\\1500]: train D loss: 0.4787, train F loss: 0.004094, acc 0.983200\n",
      "5.31s, epoch [357\\1500]: train D loss: 0.4689, train F loss: 0.000146, acc 0.984400\n",
      "5.31s, epoch [358\\1500]: train D loss: 0.4623, train F loss: -0.010556, acc 0.989600\n",
      "5.30s, epoch [359\\1500]: train D loss: 0.4646, train F loss: 0.003271, acc 0.983800\n",
      "5.29s, epoch [360\\1500]: train D loss: 0.4653, train F loss: -0.010488, acc 0.988600\n",
      "5.30s, epoch [361\\1500]: train D loss: 0.4625, train F loss: -0.011056, acc 0.989800\n",
      "5.28s, epoch [362\\1500]: train D loss: 0.4662, train F loss: 0.000841, acc 0.987200\n",
      "5.28s, epoch [363\\1500]: train D loss: 0.4776, train F loss: -0.014525, acc 0.989800\n",
      "5.28s, epoch [364\\1500]: train D loss: 0.4624, train F loss: -0.008943, acc 0.987800\n",
      "5.29s, epoch [365\\1500]: train D loss: 0.4622, train F loss: -0.006013, acc 0.988000\n",
      "5.28s, epoch [366\\1500]: train D loss: 0.4685, train F loss: -0.011536, acc 0.990000\n",
      "5.30s, epoch [367\\1500]: train D loss: 0.4688, train F loss: -0.014719, acc 0.989400\n",
      "5.28s, epoch [368\\1500]: train D loss: 0.4668, train F loss: -0.000945, acc 0.985400\n",
      "5.29s, epoch [369\\1500]: train D loss: 0.4619, train F loss: -0.015011, acc 0.989000\n",
      "5.29s, epoch [370\\1500]: train D loss: 0.4582, train F loss: -0.011908, acc 0.987800\n",
      "5.29s, epoch [371\\1500]: train D loss: 0.4712, train F loss: -0.011235, acc 0.989200\n",
      "5.28s, epoch [372\\1500]: train D loss: 0.4694, train F loss: -0.003782, acc 0.988200\n",
      "5.28s, epoch [373\\1500]: train D loss: 0.4771, train F loss: -0.010717, acc 0.988400\n",
      "5.28s, epoch [374\\1500]: train D loss: 0.4744, train F loss: -0.010145, acc 0.987800\n",
      "5.29s, epoch [375\\1500]: train D loss: 0.4781, train F loss: -0.008913, acc 0.987000\n",
      "5.39s, epoch [376\\1500]: train D loss: 0.4746, train F loss: -0.011604, acc 0.987800\n",
      "5.28s, epoch [377\\1500]: train D loss: 0.4777, train F loss: -0.004906, acc 0.987000\n",
      "5.29s, epoch [378\\1500]: train D loss: 0.4753, train F loss: -0.008806, acc 0.988600\n",
      "5.28s, epoch [379\\1500]: train D loss: 0.4857, train F loss: -0.007848, acc 0.986600\n",
      "5.29s, epoch [380\\1500]: train D loss: 0.4863, train F loss: -0.016435, acc 0.990400\n",
      "5.29s, epoch [381\\1500]: train D loss: 0.4689, train F loss: -0.004069, acc 0.986400\n",
      "5.29s, epoch [382\\1500]: train D loss: 0.4900, train F loss: -0.002401, acc 0.984400\n",
      "5.28s, epoch [383\\1500]: train D loss: 0.4673, train F loss: -0.000894, acc 0.985600\n",
      "5.28s, epoch [384\\1500]: train D loss: 0.4942, train F loss: -0.010907, acc 0.988000\n",
      "5.29s, epoch [385\\1500]: train D loss: 0.4748, train F loss: -0.012394, acc 0.987200\n",
      "5.27s, epoch [386\\1500]: train D loss: 0.4755, train F loss: -0.001870, acc 0.987400\n",
      "5.28s, epoch [387\\1500]: train D loss: 0.4848, train F loss: -0.013869, acc 0.988400\n",
      "5.30s, epoch [388\\1500]: train D loss: 0.4675, train F loss: 0.001931, acc 0.983600\n",
      "5.29s, epoch [389\\1500]: train D loss: 0.4827, train F loss: -0.015475, acc 0.988200\n",
      "5.29s, epoch [390\\1500]: train D loss: 0.4830, train F loss: -0.004001, acc 0.987000\n",
      "5.29s, epoch [391\\1500]: train D loss: 0.4674, train F loss: -0.008559, acc 0.988400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.29s, epoch [392\\1500]: train D loss: 0.4588, train F loss: -0.016961, acc 0.992000\n",
      "5.28s, epoch [393\\1500]: train D loss: 0.4616, train F loss: -0.017881, acc 0.990000\n",
      "5.29s, epoch [394\\1500]: train D loss: 0.4745, train F loss: -0.008878, acc 0.988600\n",
      "5.28s, epoch [395\\1500]: train D loss: 0.4822, train F loss: -0.006250, acc 0.986200\n",
      "5.28s, epoch [396\\1500]: train D loss: 0.4571, train F loss: -0.010853, acc 0.988200\n",
      "5.28s, epoch [397\\1500]: train D loss: 0.4563, train F loss: -0.018130, acc 0.990200\n",
      "5.29s, epoch [398\\1500]: train D loss: 0.4768, train F loss: -0.015385, acc 0.991000\n",
      "5.30s, epoch [399\\1500]: train D loss: 0.4918, train F loss: 0.021676, acc 0.978200\n",
      "5.28s, epoch [400\\1500]: train D loss: 0.4830, train F loss: -0.012567, acc 0.990400\n",
      "5.28s, epoch [401\\1500]: train D loss: 0.4693, train F loss: -0.009581, acc 0.990000\n",
      "5.27s, epoch [402\\1500]: train D loss: 0.4846, train F loss: -0.008088, acc 0.986200\n",
      "5.27s, epoch [403\\1500]: train D loss: 0.4768, train F loss: -0.012917, acc 0.989600\n",
      "5.28s, epoch [404\\1500]: train D loss: 0.4687, train F loss: -0.003013, acc 0.987800\n",
      "5.48s, epoch [405\\1500]: train D loss: 0.4613, train F loss: -0.013011, acc 0.991200\n",
      "5.28s, epoch [406\\1500]: train D loss: 0.4565, train F loss: -0.011859, acc 0.989200\n",
      "5.27s, epoch [407\\1500]: train D loss: 0.4694, train F loss: -0.016197, acc 0.991400\n",
      "5.27s, epoch [408\\1500]: train D loss: 0.4683, train F loss: -0.010494, acc 0.988600\n",
      "5.27s, epoch [409\\1500]: train D loss: 0.4832, train F loss: -0.014046, acc 0.987600\n",
      "5.27s, epoch [410\\1500]: train D loss: 0.4861, train F loss: -0.012750, acc 0.988600\n",
      "5.27s, epoch [411\\1500]: train D loss: 0.4726, train F loss: -0.014218, acc 0.989400\n",
      "5.27s, epoch [412\\1500]: train D loss: 0.4760, train F loss: -0.001282, acc 0.987000\n",
      "5.27s, epoch [413\\1500]: train D loss: 0.4829, train F loss: 0.006958, acc 0.984200\n",
      "5.26s, epoch [414\\1500]: train D loss: 0.4690, train F loss: -0.016683, acc 0.989000\n",
      "5.26s, epoch [415\\1500]: train D loss: 0.4653, train F loss: -0.008070, acc 0.988000\n",
      "5.27s, epoch [416\\1500]: train D loss: 0.4612, train F loss: -0.017286, acc 0.990200\n",
      "5.27s, epoch [417\\1500]: train D loss: 0.4829, train F loss: -0.021010, acc 0.991000\n",
      "5.27s, epoch [418\\1500]: train D loss: 0.4707, train F loss: -0.020708, acc 0.992400\n",
      "5.26s, epoch [419\\1500]: train D loss: 0.4907, train F loss: -0.014869, acc 0.988600\n",
      "5.28s, epoch [420\\1500]: train D loss: 0.4898, train F loss: -0.001701, acc 0.987000\n",
      "5.29s, epoch [421\\1500]: train D loss: 0.4730, train F loss: -0.016148, acc 0.988800\n",
      "5.27s, epoch [422\\1500]: train D loss: 0.4613, train F loss: -0.010307, acc 0.989800\n",
      "5.27s, epoch [423\\1500]: train D loss: 0.4889, train F loss: 0.002447, acc 0.985000\n",
      "5.27s, epoch [424\\1500]: train D loss: 0.4753, train F loss: -0.010187, acc 0.987800\n",
      "5.27s, epoch [425\\1500]: train D loss: 0.4698, train F loss: -0.014317, acc 0.990200\n",
      "5.26s, epoch [426\\1500]: train D loss: 0.4828, train F loss: -0.021370, acc 0.991200\n",
      "5.26s, epoch [427\\1500]: train D loss: 0.5076, train F loss: -0.012911, acc 0.987600\n",
      "5.27s, epoch [428\\1500]: train D loss: 0.4639, train F loss: -0.009528, acc 0.988000\n",
      "5.26s, epoch [429\\1500]: train D loss: 0.4992, train F loss: -0.008043, acc 0.988200\n",
      "5.24s, epoch [430\\1500]: train D loss: 0.4927, train F loss: -0.019087, acc 0.990200\n",
      "5.24s, epoch [431\\1500]: train D loss: 0.4820, train F loss: -0.010095, acc 0.989200\n",
      "5.23s, epoch [432\\1500]: train D loss: 0.4925, train F loss: 0.013115, acc 0.980000\n",
      "5.25s, epoch [433\\1500]: train D loss: 0.4870, train F loss: -0.000302, acc 0.984000\n",
      "5.25s, epoch [434\\1500]: train D loss: 0.4691, train F loss: -0.015730, acc 0.991400\n",
      "5.23s, epoch [435\\1500]: train D loss: 0.4687, train F loss: -0.014791, acc 0.990800\n",
      "5.24s, epoch [436\\1500]: train D loss: 0.4746, train F loss: -0.017037, acc 0.990600\n",
      "5.25s, epoch [437\\1500]: train D loss: 0.4740, train F loss: -0.018036, acc 0.991600\n",
      "5.24s, epoch [438\\1500]: train D loss: 0.4911, train F loss: -0.013331, acc 0.988600\n",
      "5.24s, epoch [439\\1500]: train D loss: 0.4874, train F loss: -0.015696, acc 0.990200\n",
      "5.23s, epoch [440\\1500]: train D loss: 0.4773, train F loss: -0.003969, acc 0.988800\n",
      "5.28s, epoch [441\\1500]: train D loss: 0.4859, train F loss: -0.007155, acc 0.987200\n",
      "5.38s, epoch [442\\1500]: train D loss: 0.4842, train F loss: -0.010637, acc 0.988000\n",
      "5.33s, epoch [443\\1500]: train D loss: 0.4719, train F loss: -0.012436, acc 0.991000\n",
      "5.29s, epoch [444\\1500]: train D loss: 0.4931, train F loss: -0.004734, acc 0.985000\n",
      "5.31s, epoch [445\\1500]: train D loss: 0.4994, train F loss: -0.016680, acc 0.988600\n",
      "5.32s, epoch [446\\1500]: train D loss: 0.4742, train F loss: -0.015618, acc 0.990000\n",
      "5.46s, epoch [447\\1500]: train D loss: 0.4828, train F loss: -0.018714, acc 0.990400\n",
      "5.37s, epoch [448\\1500]: train D loss: 0.4739, train F loss: -0.011650, acc 0.990800\n",
      "5.37s, epoch [449\\1500]: train D loss: 0.4736, train F loss: -0.016446, acc 0.989400\n",
      "5.36s, epoch [450\\1500]: train D loss: 0.4964, train F loss: -0.011162, acc 0.989400\n",
      "5.36s, epoch [451\\1500]: train D loss: 0.4814, train F loss: -0.009110, acc 0.988600\n",
      "5.36s, epoch [452\\1500]: train D loss: 0.4906, train F loss: -0.018440, acc 0.990400\n",
      "5.36s, epoch [453\\1500]: train D loss: 0.4840, train F loss: -0.014003, acc 0.988800\n",
      "5.36s, epoch [454\\1500]: train D loss: 0.4882, train F loss: -0.018511, acc 0.990400\n",
      "5.37s, epoch [455\\1500]: train D loss: 0.4859, train F loss: -0.005955, acc 0.987600\n",
      "5.37s, epoch [456\\1500]: train D loss: 0.4670, train F loss: -0.021806, acc 0.992200\n",
      "5.36s, epoch [457\\1500]: train D loss: 0.4676, train F loss: -0.006953, acc 0.987800\n",
      "5.36s, epoch [458\\1500]: train D loss: 0.4870, train F loss: -0.021610, acc 0.991800\n",
      "5.36s, epoch [459\\1500]: train D loss: 0.4944, train F loss: -0.017506, acc 0.990000\n",
      "5.36s, epoch [460\\1500]: train D loss: 0.4761, train F loss: -0.005970, acc 0.987400\n",
      "5.37s, epoch [461\\1500]: train D loss: 0.4961, train F loss: 0.000747, acc 0.983800\n",
      "5.36s, epoch [462\\1500]: train D loss: 0.4904, train F loss: -0.011613, acc 0.989000\n",
      "5.36s, epoch [463\\1500]: train D loss: 0.4779, train F loss: -0.007124, acc 0.987800\n",
      "5.36s, epoch [464\\1500]: train D loss: 0.4916, train F loss: -0.017149, acc 0.990000\n",
      "5.36s, epoch [465\\1500]: train D loss: 0.4903, train F loss: -0.022154, acc 0.992000\n",
      "5.37s, epoch [466\\1500]: train D loss: 0.4851, train F loss: -0.009811, acc 0.987600\n",
      "5.36s, epoch [467\\1500]: train D loss: 0.4804, train F loss: -0.014784, acc 0.990400\n",
      "5.37s, epoch [468\\1500]: train D loss: 0.4892, train F loss: -0.013962, acc 0.989400\n",
      "5.36s, epoch [469\\1500]: train D loss: 0.4800, train F loss: -0.008732, acc 0.987200\n",
      "5.35s, epoch [470\\1500]: train D loss: 0.4833, train F loss: -0.021266, acc 0.990800\n",
      "5.36s, epoch [471\\1500]: train D loss: 0.4980, train F loss: -0.008325, acc 0.987000\n",
      "5.37s, epoch [472\\1500]: train D loss: 0.4841, train F loss: -0.011986, acc 0.988000\n",
      "5.36s, epoch [473\\1500]: train D loss: 0.4863, train F loss: -0.010959, acc 0.988400\n",
      "5.36s, epoch [474\\1500]: train D loss: 0.4872, train F loss: -0.019641, acc 0.990200\n",
      "5.37s, epoch [475\\1500]: train D loss: 0.4924, train F loss: -0.010833, acc 0.987000\n",
      "5.36s, epoch [476\\1500]: train D loss: 0.4642, train F loss: -0.012585, acc 0.990400\n",
      "5.36s, epoch [477\\1500]: train D loss: 0.4688, train F loss: -0.015186, acc 0.989800\n",
      "5.37s, epoch [478\\1500]: train D loss: 0.4852, train F loss: -0.023164, acc 0.992400\n",
      "5.36s, epoch [479\\1500]: train D loss: 0.4754, train F loss: -0.024069, acc 0.991600\n",
      "5.35s, epoch [480\\1500]: train D loss: 0.4828, train F loss: -0.009404, acc 0.989200\n",
      "5.36s, epoch [481\\1500]: train D loss: 0.4709, train F loss: -0.018461, acc 0.991200\n",
      "5.36s, epoch [482\\1500]: train D loss: 0.4706, train F loss: -0.019555, acc 0.990400\n",
      "5.36s, epoch [483\\1500]: train D loss: 0.4929, train F loss: -0.022578, acc 0.992200\n",
      "5.35s, epoch [484\\1500]: train D loss: 0.4897, train F loss: -0.011878, acc 0.990200\n",
      "5.36s, epoch [485\\1500]: train D loss: 0.4949, train F loss: -0.016925, acc 0.988200\n",
      "5.36s, epoch [486\\1500]: train D loss: 0.5182, train F loss: -0.006813, acc 0.986000\n",
      "5.36s, epoch [487\\1500]: train D loss: 0.4850, train F loss: -0.007206, acc 0.988200\n",
      "5.38s, epoch [488\\1500]: train D loss: 0.5068, train F loss: 0.021487, acc 0.979200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.36s, epoch [489\\1500]: train D loss: 0.4731, train F loss: -0.012729, acc 0.988600\n",
      "5.36s, epoch [490\\1500]: train D loss: 0.4673, train F loss: -0.015470, acc 0.991400\n",
      "5.37s, epoch [491\\1500]: train D loss: 0.4931, train F loss: -0.021865, acc 0.988800\n",
      "5.32s, epoch [492\\1500]: train D loss: 0.4693, train F loss: -0.017299, acc 0.990800\n",
      "5.27s, epoch [493\\1500]: train D loss: 0.5067, train F loss: -0.002545, acc 0.984200\n",
      "5.27s, epoch [494\\1500]: train D loss: 0.4891, train F loss: -0.011560, acc 0.986600\n",
      "5.27s, epoch [495\\1500]: train D loss: 0.4666, train F loss: -0.022928, acc 0.991600\n",
      "5.26s, epoch [496\\1500]: train D loss: 0.4629, train F loss: -0.022368, acc 0.992600\n",
      "5.27s, epoch [497\\1500]: train D loss: 0.4672, train F loss: -0.027265, acc 0.994000\n",
      "5.59s, epoch [498\\1500]: train D loss: 0.4710, train F loss: -0.000575, acc 0.986400\n",
      "5.33s, epoch [499\\1500]: train D loss: 0.4731, train F loss: -0.007037, acc 0.988800\n",
      "5.28s, epoch [500\\1500]: train D loss: 0.4787, train F loss: -0.016745, acc 0.990400\n",
      "5.28s, epoch [501\\1500]: train D loss: 0.4754, train F loss: -0.020965, acc 0.992000\n",
      "5.27s, epoch [502\\1500]: train D loss: 0.4827, train F loss: -0.012944, acc 0.990800\n",
      "5.28s, epoch [503\\1500]: train D loss: 0.4670, train F loss: -0.025228, acc 0.993600\n",
      "5.27s, epoch [504\\1500]: train D loss: 0.4863, train F loss: -0.005383, acc 0.987800\n",
      "5.27s, epoch [505\\1500]: train D loss: 0.4894, train F loss: -0.023780, acc 0.991800\n",
      "5.29s, epoch [506\\1500]: train D loss: 0.4918, train F loss: -0.007437, acc 0.987800\n",
      "5.27s, epoch [507\\1500]: train D loss: 0.4758, train F loss: -0.024543, acc 0.992800\n",
      "5.27s, epoch [508\\1500]: train D loss: 0.4912, train F loss: -0.020924, acc 0.992000\n",
      "5.28s, epoch [509\\1500]: train D loss: 0.4826, train F loss: -0.009319, acc 0.987600\n",
      "5.27s, epoch [510\\1500]: train D loss: 0.4965, train F loss: -0.016108, acc 0.990400\n",
      "5.27s, epoch [511\\1500]: train D loss: 0.4858, train F loss: -0.021637, acc 0.991600\n",
      "5.27s, epoch [512\\1500]: train D loss: 0.4902, train F loss: -0.019145, acc 0.990800\n",
      "5.27s, epoch [513\\1500]: train D loss: 0.4833, train F loss: -0.013471, acc 0.988400\n",
      "5.28s, epoch [514\\1500]: train D loss: 0.4991, train F loss: -0.019939, acc 0.989600\n",
      "5.28s, epoch [515\\1500]: train D loss: 0.4991, train F loss: -0.004939, acc 0.988000\n",
      "5.27s, epoch [516\\1500]: train D loss: 0.4818, train F loss: -0.023768, acc 0.991200\n",
      "5.26s, epoch [517\\1500]: train D loss: 0.4983, train F loss: -0.004956, acc 0.988200\n",
      "5.26s, epoch [518\\1500]: train D loss: 0.4988, train F loss: -0.021620, acc 0.991000\n",
      "5.27s, epoch [519\\1500]: train D loss: 0.4955, train F loss: -0.000670, acc 0.987000\n",
      "5.27s, epoch [520\\1500]: train D loss: 0.4802, train F loss: -0.007229, acc 0.988800\n",
      "5.28s, epoch [521\\1500]: train D loss: 0.5047, train F loss: -0.000518, acc 0.986200\n",
      "5.28s, epoch [522\\1500]: train D loss: 0.4792, train F loss: -0.021603, acc 0.991600\n",
      "5.27s, epoch [523\\1500]: train D loss: 0.4760, train F loss: -0.023135, acc 0.992600\n",
      "5.27s, epoch [524\\1500]: train D loss: 0.4846, train F loss: -0.015818, acc 0.989000\n",
      "5.27s, epoch [525\\1500]: train D loss: 0.4934, train F loss: -0.008352, acc 0.987400\n",
      "5.28s, epoch [526\\1500]: train D loss: 0.4650, train F loss: -0.022077, acc 0.991400\n",
      "5.26s, epoch [527\\1500]: train D loss: 0.4845, train F loss: -0.022318, acc 0.992000\n",
      "5.29s, epoch [528\\1500]: train D loss: 0.4666, train F loss: -0.024822, acc 0.993200\n",
      "5.24s, epoch [529\\1500]: train D loss: 0.4736, train F loss: -0.022689, acc 0.992200\n",
      "5.24s, epoch [530\\1500]: train D loss: 0.4801, train F loss: -0.019602, acc 0.992800\n",
      "5.24s, epoch [531\\1500]: train D loss: 0.4797, train F loss: -0.018427, acc 0.991600\n",
      "5.25s, epoch [532\\1500]: train D loss: 0.4852, train F loss: -0.024184, acc 0.992800\n",
      "5.23s, epoch [533\\1500]: train D loss: 0.4972, train F loss: -0.013986, acc 0.988800\n",
      "5.24s, epoch [534\\1500]: train D loss: 0.4985, train F loss: -0.019284, acc 0.990800\n",
      "5.23s, epoch [535\\1500]: train D loss: 0.4891, train F loss: -0.014381, acc 0.989200\n",
      "5.31s, epoch [536\\1500]: train D loss: 0.4793, train F loss: -0.020775, acc 0.991400\n",
      "5.22s, epoch [537\\1500]: train D loss: 0.4814, train F loss: -0.021501, acc 0.991400\n",
      "5.24s, epoch [538\\1500]: train D loss: 0.4832, train F loss: 0.000049, acc 0.989000\n",
      "5.23s, epoch [539\\1500]: train D loss: 0.4917, train F loss: -0.021383, acc 0.991600\n",
      "5.24s, epoch [540\\1500]: train D loss: 0.4700, train F loss: -0.017534, acc 0.989800\n",
      "5.24s, epoch [541\\1500]: train D loss: 0.4840, train F loss: -0.006527, acc 0.990800\n",
      "5.24s, epoch [542\\1500]: train D loss: 0.5014, train F loss: 0.012558, acc 0.981600\n",
      "5.24s, epoch [543\\1500]: train D loss: 0.4958, train F loss: -0.011834, acc 0.987800\n",
      "5.24s, epoch [544\\1500]: train D loss: 0.4770, train F loss: -0.017526, acc 0.990400\n",
      "5.24s, epoch [545\\1500]: train D loss: 0.4876, train F loss: -0.022313, acc 0.992200\n",
      "5.24s, epoch [546\\1500]: train D loss: 0.5002, train F loss: -0.020020, acc 0.990000\n",
      "5.23s, epoch [547\\1500]: train D loss: 0.4877, train F loss: -0.021665, acc 0.991600\n",
      "5.23s, epoch [548\\1500]: train D loss: 0.4846, train F loss: -0.020953, acc 0.991200\n",
      "5.24s, epoch [549\\1500]: train D loss: 0.4965, train F loss: -0.017831, acc 0.990200\n",
      "5.23s, epoch [550\\1500]: train D loss: 0.4872, train F loss: -0.025924, acc 0.992000\n",
      "5.26s, epoch [551\\1500]: train D loss: 0.4999, train F loss: -0.016155, acc 0.991200\n",
      "5.24s, epoch [552\\1500]: train D loss: 0.4820, train F loss: -0.022814, acc 0.990800\n",
      "5.23s, epoch [553\\1500]: train D loss: 0.4892, train F loss: -0.016233, acc 0.990200\n",
      "5.23s, epoch [554\\1500]: train D loss: 0.4838, train F loss: -0.012981, acc 0.990600\n",
      "5.24s, epoch [555\\1500]: train D loss: 0.4951, train F loss: -0.015537, acc 0.990800\n",
      "5.25s, epoch [556\\1500]: train D loss: 0.5044, train F loss: -0.006665, acc 0.983000\n",
      "5.24s, epoch [557\\1500]: train D loss: 0.4911, train F loss: -0.019496, acc 0.992000\n",
      "5.23s, epoch [558\\1500]: train D loss: 0.4834, train F loss: -0.013310, acc 0.991800\n",
      "5.24s, epoch [559\\1500]: train D loss: 0.4957, train F loss: -0.017480, acc 0.990200\n",
      "5.24s, epoch [560\\1500]: train D loss: 0.4993, train F loss: -0.027965, acc 0.992400\n",
      "5.24s, epoch [561\\1500]: train D loss: 0.4947, train F loss: -0.015781, acc 0.989000\n",
      "5.23s, epoch [562\\1500]: train D loss: 0.4899, train F loss: -0.022069, acc 0.991400\n",
      "5.25s, epoch [563\\1500]: train D loss: 0.5028, train F loss: -0.007922, acc 0.985800\n",
      "5.23s, epoch [564\\1500]: train D loss: 0.4895, train F loss: -0.027307, acc 0.993000\n",
      "5.25s, epoch [565\\1500]: train D loss: 0.4861, train F loss: -0.021300, acc 0.991600\n",
      "5.24s, epoch [566\\1500]: train D loss: 0.4902, train F loss: -0.017039, acc 0.989800\n",
      "5.24s, epoch [567\\1500]: train D loss: 0.4951, train F loss: -0.024200, acc 0.992000\n",
      "5.24s, epoch [568\\1500]: train D loss: 0.4981, train F loss: -0.013806, acc 0.989600\n",
      "5.25s, epoch [569\\1500]: train D loss: 0.4873, train F loss: -0.020304, acc 0.992200\n",
      "5.23s, epoch [570\\1500]: train D loss: 0.4841, train F loss: -0.011243, acc 0.988200\n",
      "5.24s, epoch [571\\1500]: train D loss: 0.5038, train F loss: -0.018546, acc 0.991200\n",
      "5.23s, epoch [572\\1500]: train D loss: 0.4897, train F loss: -0.018280, acc 0.989800\n",
      "5.24s, epoch [573\\1500]: train D loss: 0.4893, train F loss: -0.025697, acc 0.992200\n",
      "5.25s, epoch [574\\1500]: train D loss: 0.4873, train F loss: -0.021472, acc 0.990800\n",
      "5.24s, epoch [575\\1500]: train D loss: 0.5107, train F loss: -0.011964, acc 0.986600\n",
      "5.24s, epoch [576\\1500]: train D loss: 0.5053, train F loss: -0.016687, acc 0.988200\n",
      "5.24s, epoch [577\\1500]: train D loss: 0.4987, train F loss: -0.020726, acc 0.990200\n",
      "5.24s, epoch [578\\1500]: train D loss: 0.4870, train F loss: -0.027098, acc 0.993000\n",
      "5.25s, epoch [579\\1500]: train D loss: 0.4901, train F loss: -0.023368, acc 0.992200\n",
      "5.24s, epoch [580\\1500]: train D loss: 0.4916, train F loss: -0.027222, acc 0.992600\n",
      "5.24s, epoch [581\\1500]: train D loss: 0.4798, train F loss: -0.017720, acc 0.989600\n",
      "5.23s, epoch [582\\1500]: train D loss: 0.4850, train F loss: -0.023170, acc 0.992400\n",
      "5.23s, epoch [583\\1500]: train D loss: 0.5063, train F loss: -0.019297, acc 0.991400\n",
      "5.23s, epoch [584\\1500]: train D loss: 0.4775, train F loss: -0.025550, acc 0.991400\n",
      "5.24s, epoch [585\\1500]: train D loss: 0.4805, train F loss: -0.011251, acc 0.988800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.24s, epoch [586\\1500]: train D loss: 0.5108, train F loss: -0.016745, acc 0.990800\n",
      "5.24s, epoch [587\\1500]: train D loss: 0.4939, train F loss: -0.028695, acc 0.992600\n",
      "5.24s, epoch [588\\1500]: train D loss: 0.4973, train F loss: -0.021261, acc 0.991200\n",
      "5.23s, epoch [589\\1500]: train D loss: 0.4940, train F loss: -0.020806, acc 0.990400\n",
      "5.25s, epoch [590\\1500]: train D loss: 0.4897, train F loss: -0.025263, acc 0.993000\n",
      "5.23s, epoch [591\\1500]: train D loss: 0.5136, train F loss: 0.035266, acc 0.976800\n",
      "5.23s, epoch [592\\1500]: train D loss: 0.4814, train F loss: -0.025632, acc 0.993200\n",
      "5.24s, epoch [593\\1500]: train D loss: 0.4911, train F loss: -0.021348, acc 0.991800\n",
      "5.25s, epoch [594\\1500]: train D loss: 0.4949, train F loss: -0.015876, acc 0.990000\n",
      "5.23s, epoch [595\\1500]: train D loss: 0.4823, train F loss: -0.019856, acc 0.990400\n",
      "5.24s, epoch [596\\1500]: train D loss: 0.4948, train F loss: -0.021294, acc 0.991800\n",
      "5.25s, epoch [597\\1500]: train D loss: 0.4993, train F loss: -0.016626, acc 0.990000\n",
      "5.23s, epoch [598\\1500]: train D loss: 0.4772, train F loss: -0.017949, acc 0.993800\n",
      "5.23s, epoch [599\\1500]: train D loss: 0.5168, train F loss: 0.013196, acc 0.980800\n",
      "5.24s, epoch [600\\1500]: train D loss: 0.5036, train F loss: -0.022291, acc 0.991000\n",
      "5.24s, epoch [601\\1500]: train D loss: 0.4771, train F loss: -0.017300, acc 0.992200\n",
      "5.25s, epoch [602\\1500]: train D loss: 0.4924, train F loss: -0.026827, acc 0.993000\n",
      "5.25s, epoch [603\\1500]: train D loss: 0.5074, train F loss: 0.006771, acc 0.981800\n",
      "5.24s, epoch [604\\1500]: train D loss: 0.4887, train F loss: -0.019791, acc 0.990400\n",
      "5.28s, epoch [605\\1500]: train D loss: 0.4874, train F loss: -0.028810, acc 0.993400\n",
      "5.25s, epoch [606\\1500]: train D loss: 0.4896, train F loss: -0.021260, acc 0.990400\n",
      "5.25s, epoch [607\\1500]: train D loss: 0.4925, train F loss: -0.021858, acc 0.991400\n",
      "5.25s, epoch [608\\1500]: train D loss: 0.5026, train F loss: -0.019740, acc 0.989000\n",
      "5.25s, epoch [609\\1500]: train D loss: 0.4918, train F loss: -0.026554, acc 0.992400\n",
      "5.25s, epoch [610\\1500]: train D loss: 0.4797, train F loss: -0.026452, acc 0.993600\n",
      "5.25s, epoch [611\\1500]: train D loss: 0.4941, train F loss: -0.026409, acc 0.991600\n",
      "5.25s, epoch [612\\1500]: train D loss: 0.4861, train F loss: -0.034493, acc 0.995000\n",
      "5.25s, epoch [613\\1500]: train D loss: 0.4959, train F loss: -0.021553, acc 0.991800\n",
      "5.24s, epoch [614\\1500]: train D loss: 0.5012, train F loss: -0.023653, acc 0.992600\n",
      "5.26s, epoch [615\\1500]: train D loss: 0.5022, train F loss: -0.028443, acc 0.994400\n",
      "5.24s, epoch [616\\1500]: train D loss: 0.5120, train F loss: -0.011082, acc 0.989800\n",
      "5.25s, epoch [617\\1500]: train D loss: 0.4944, train F loss: -0.012542, acc 0.990200\n",
      "5.25s, epoch [618\\1500]: train D loss: 0.4848, train F loss: -0.025922, acc 0.992600\n",
      "5.25s, epoch [619\\1500]: train D loss: 0.4853, train F loss: -0.025790, acc 0.992400\n",
      "5.25s, epoch [620\\1500]: train D loss: 0.4956, train F loss: -0.026445, acc 0.992000\n",
      "5.24s, epoch [621\\1500]: train D loss: 0.4900, train F loss: -0.014867, acc 0.991800\n",
      "5.25s, epoch [622\\1500]: train D loss: 0.4850, train F loss: -0.014960, acc 0.989400\n",
      "5.24s, epoch [623\\1500]: train D loss: 0.5035, train F loss: -0.026704, acc 0.992800\n",
      "5.25s, epoch [624\\1500]: train D loss: 0.5026, train F loss: -0.018834, acc 0.989000\n",
      "5.24s, epoch [625\\1500]: train D loss: 0.5045, train F loss: -0.019272, acc 0.991000\n",
      "5.25s, epoch [626\\1500]: train D loss: 0.4951, train F loss: -0.025725, acc 0.993600\n",
      "5.25s, epoch [627\\1500]: train D loss: 0.4970, train F loss: -0.018804, acc 0.990400\n",
      "5.25s, epoch [628\\1500]: train D loss: 0.5045, train F loss: -0.017825, acc 0.988200\n",
      "5.26s, epoch [629\\1500]: train D loss: 0.5063, train F loss: -0.019297, acc 0.990800\n",
      "5.25s, epoch [630\\1500]: train D loss: 0.5069, train F loss: -0.021535, acc 0.991400\n",
      "5.25s, epoch [631\\1500]: train D loss: 0.5091, train F loss: -0.026420, acc 0.991000\n",
      "5.26s, epoch [632\\1500]: train D loss: 0.5003, train F loss: -0.022540, acc 0.992200\n",
      "5.25s, epoch [633\\1500]: train D loss: 0.4917, train F loss: -0.033143, acc 0.995200\n",
      "5.24s, epoch [634\\1500]: train D loss: 0.5271, train F loss: -0.019212, acc 0.989800\n",
      "5.24s, epoch [635\\1500]: train D loss: 0.5184, train F loss: -0.020692, acc 0.991400\n",
      "5.25s, epoch [636\\1500]: train D loss: 0.5081, train F loss: -0.018931, acc 0.990400\n",
      "5.24s, epoch [637\\1500]: train D loss: 0.5093, train F loss: -0.019891, acc 0.990800\n",
      "5.26s, epoch [638\\1500]: train D loss: 0.5119, train F loss: -0.031630, acc 0.994000\n",
      "5.25s, epoch [639\\1500]: train D loss: 0.5269, train F loss: -0.012526, acc 0.988000\n",
      "5.27s, epoch [640\\1500]: train D loss: 0.5143, train F loss: -0.014674, acc 0.989000\n",
      "5.24s, epoch [641\\1500]: train D loss: 0.5110, train F loss: -0.022915, acc 0.990600\n",
      "5.25s, epoch [642\\1500]: train D loss: 0.5107, train F loss: -0.014063, acc 0.989000\n",
      "5.25s, epoch [643\\1500]: train D loss: 0.5084, train F loss: -0.018862, acc 0.989400\n",
      "5.25s, epoch [644\\1500]: train D loss: 0.4918, train F loss: -0.025987, acc 0.992200\n",
      "5.25s, epoch [645\\1500]: train D loss: 0.5014, train F loss: -0.020346, acc 0.990600\n",
      "5.25s, epoch [646\\1500]: train D loss: 0.5043, train F loss: -0.021568, acc 0.990800\n",
      "5.25s, epoch [647\\1500]: train D loss: 0.5028, train F loss: -0.014287, acc 0.989400\n",
      "5.26s, epoch [648\\1500]: train D loss: 0.5056, train F loss: -0.005450, acc 0.988200\n",
      "5.25s, epoch [649\\1500]: train D loss: 0.5036, train F loss: -0.024721, acc 0.991400\n",
      "5.25s, epoch [650\\1500]: train D loss: 0.5039, train F loss: -0.025925, acc 0.992400\n",
      "5.23s, epoch [651\\1500]: train D loss: 0.4935, train F loss: -0.025727, acc 0.992600\n",
      "5.23s, epoch [652\\1500]: train D loss: 0.5041, train F loss: -0.024527, acc 0.992600\n",
      "5.24s, epoch [653\\1500]: train D loss: 0.5015, train F loss: -0.023411, acc 0.992000\n",
      "5.23s, epoch [654\\1500]: train D loss: 0.4886, train F loss: -0.027562, acc 0.991800\n",
      "5.25s, epoch [655\\1500]: train D loss: 0.4983, train F loss: -0.019618, acc 0.990600\n",
      "5.24s, epoch [656\\1500]: train D loss: 0.5240, train F loss: -0.021277, acc 0.990800\n",
      "5.22s, epoch [657\\1500]: train D loss: 0.4999, train F loss: -0.022768, acc 0.991400\n",
      "5.22s, epoch [658\\1500]: train D loss: 0.5047, train F loss: -0.018704, acc 0.990600\n",
      "5.22s, epoch [659\\1500]: train D loss: 0.5039, train F loss: -0.017238, acc 0.989800\n",
      "5.22s, epoch [660\\1500]: train D loss: 0.5187, train F loss: 0.014961, acc 0.985200\n",
      "5.24s, epoch [661\\1500]: train D loss: 0.5074, train F loss: -0.021089, acc 0.991000\n",
      "5.23s, epoch [662\\1500]: train D loss: 0.5160, train F loss: -0.020025, acc 0.991000\n",
      "5.23s, epoch [663\\1500]: train D loss: 0.5238, train F loss: 0.007327, acc 0.985000\n",
      "5.23s, epoch [664\\1500]: train D loss: 0.4970, train F loss: -0.017644, acc 0.989600\n",
      "5.22s, epoch [665\\1500]: train D loss: 0.5096, train F loss: -0.011593, acc 0.987400\n",
      "5.22s, epoch [666\\1500]: train D loss: 0.5019, train F loss: -0.021485, acc 0.991800\n",
      "5.23s, epoch [667\\1500]: train D loss: 0.4991, train F loss: -0.030101, acc 0.994200\n",
      "5.23s, epoch [668\\1500]: train D loss: 0.5224, train F loss: -0.008280, acc 0.989600\n",
      "5.22s, epoch [669\\1500]: train D loss: 0.4975, train F loss: -0.028372, acc 0.993400\n",
      "5.22s, epoch [670\\1500]: train D loss: 0.4866, train F loss: -0.024638, acc 0.993800\n",
      "5.22s, epoch [671\\1500]: train D loss: 0.4998, train F loss: -0.022832, acc 0.991000\n",
      "5.23s, epoch [672\\1500]: train D loss: 0.5059, train F loss: -0.027896, acc 0.991200\n",
      "5.23s, epoch [673\\1500]: train D loss: 0.5051, train F loss: -0.020686, acc 0.991600\n",
      "5.22s, epoch [674\\1500]: train D loss: 0.5109, train F loss: -0.023755, acc 0.991800\n",
      "5.23s, epoch [675\\1500]: train D loss: 0.4961, train F loss: -0.028821, acc 0.994200\n",
      "5.22s, epoch [676\\1500]: train D loss: 0.4938, train F loss: -0.024394, acc 0.991800\n",
      "5.23s, epoch [677\\1500]: train D loss: 0.4935, train F loss: -0.030142, acc 0.994200\n",
      "5.24s, epoch [678\\1500]: train D loss: 0.4963, train F loss: -0.026442, acc 0.992800\n",
      "5.23s, epoch [679\\1500]: train D loss: 0.4998, train F loss: -0.022343, acc 0.992600\n",
      "5.22s, epoch [680\\1500]: train D loss: 0.4960, train F loss: -0.023503, acc 0.992800\n",
      "5.22s, epoch [681\\1500]: train D loss: 0.4902, train F loss: -0.021576, acc 0.993000\n",
      "5.23s, epoch [682\\1500]: train D loss: 0.5117, train F loss: -0.010975, acc 0.987400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.23s, epoch [683\\1500]: train D loss: 0.5089, train F loss: -0.022102, acc 0.990600\n",
      "5.25s, epoch [684\\1500]: train D loss: 0.4940, train F loss: -0.019578, acc 0.989600\n",
      "5.23s, epoch [685\\1500]: train D loss: 0.4899, train F loss: -0.021297, acc 0.990600\n",
      "5.24s, epoch [686\\1500]: train D loss: 0.5012, train F loss: -0.022954, acc 0.991400\n",
      "5.24s, epoch [687\\1500]: train D loss: 0.4997, train F loss: -0.021038, acc 0.991400\n",
      "5.23s, epoch [688\\1500]: train D loss: 0.5025, train F loss: -0.026829, acc 0.991000\n",
      "5.23s, epoch [689\\1500]: train D loss: 0.5008, train F loss: -0.028648, acc 0.994800\n",
      "5.22s, epoch [690\\1500]: train D loss: 0.5039, train F loss: -0.026495, acc 0.992400\n",
      "5.22s, epoch [691\\1500]: train D loss: 0.5011, train F loss: -0.020413, acc 0.991600\n",
      "5.23s, epoch [692\\1500]: train D loss: 0.5232, train F loss: 0.002130, acc 0.985800\n",
      "5.24s, epoch [693\\1500]: train D loss: 0.5039, train F loss: -0.028878, acc 0.992800\n",
      "5.24s, epoch [694\\1500]: train D loss: 0.5176, train F loss: -0.027450, acc 0.993200\n",
      "5.24s, epoch [695\\1500]: train D loss: 0.4945, train F loss: -0.024640, acc 0.991800\n",
      "5.24s, epoch [696\\1500]: train D loss: 0.5004, train F loss: -0.022213, acc 0.992000\n",
      "5.25s, epoch [697\\1500]: train D loss: 0.4982, train F loss: -0.019784, acc 0.990200\n",
      "5.25s, epoch [698\\1500]: train D loss: 0.4956, train F loss: -0.025536, acc 0.991400\n",
      "5.25s, epoch [699\\1500]: train D loss: 0.4949, train F loss: -0.031926, acc 0.994600\n",
      "5.25s, epoch [700\\1500]: train D loss: 0.4970, train F loss: -0.016441, acc 0.991800\n",
      "5.25s, epoch [701\\1500]: train D loss: 0.5059, train F loss: -0.028794, acc 0.992400\n",
      "5.27s, epoch [702\\1500]: train D loss: 0.5043, train F loss: -0.029489, acc 0.993600\n",
      "5.25s, epoch [703\\1500]: train D loss: 0.5208, train F loss: -0.030173, acc 0.993000\n",
      "5.26s, epoch [704\\1500]: train D loss: 0.5154, train F loss: -0.023551, acc 0.991000\n",
      "5.24s, epoch [705\\1500]: train D loss: 0.5150, train F loss: -0.026099, acc 0.992600\n",
      "5.25s, epoch [706\\1500]: train D loss: 0.5145, train F loss: -0.028089, acc 0.993000\n",
      "5.25s, epoch [707\\1500]: train D loss: 0.5004, train F loss: -0.024411, acc 0.992800\n",
      "5.25s, epoch [708\\1500]: train D loss: 0.5125, train F loss: -0.026060, acc 0.991000\n",
      "5.24s, epoch [709\\1500]: train D loss: 0.5230, train F loss: -0.031057, acc 0.994200\n",
      "5.25s, epoch [710\\1500]: train D loss: 0.5030, train F loss: -0.024709, acc 0.992400\n",
      "5.25s, epoch [711\\1500]: train D loss: 0.5295, train F loss: -0.029298, acc 0.992800\n",
      "5.25s, epoch [712\\1500]: train D loss: 0.5156, train F loss: -0.028299, acc 0.992400\n",
      "5.25s, epoch [713\\1500]: train D loss: 0.5113, train F loss: -0.024745, acc 0.991800\n",
      "5.25s, epoch [714\\1500]: train D loss: 0.5094, train F loss: -0.020324, acc 0.990200\n",
      "5.25s, epoch [715\\1500]: train D loss: 0.5245, train F loss: -0.024118, acc 0.991000\n",
      "5.25s, epoch [716\\1500]: train D loss: 0.5069, train F loss: -0.019718, acc 0.990400\n",
      "5.26s, epoch [717\\1500]: train D loss: 0.5155, train F loss: -0.019305, acc 0.989800\n",
      "5.25s, epoch [718\\1500]: train D loss: 0.5125, train F loss: -0.031999, acc 0.993400\n",
      "5.25s, epoch [719\\1500]: train D loss: 0.5128, train F loss: -0.025675, acc 0.992400\n",
      "5.27s, epoch [720\\1500]: train D loss: 0.5150, train F loss: -0.033528, acc 0.994800\n",
      "5.25s, epoch [721\\1500]: train D loss: 0.4963, train F loss: -0.028779, acc 0.993400\n",
      "5.25s, epoch [722\\1500]: train D loss: 0.5146, train F loss: -0.025843, acc 0.991400\n",
      "5.25s, epoch [723\\1500]: train D loss: 0.5048, train F loss: -0.024518, acc 0.992800\n",
      "5.26s, epoch [724\\1500]: train D loss: 0.4820, train F loss: -0.029187, acc 0.992800\n",
      "5.26s, epoch [725\\1500]: train D loss: 0.5109, train F loss: -0.024895, acc 0.992000\n",
      "5.24s, epoch [726\\1500]: train D loss: 0.5167, train F loss: -0.028899, acc 0.994200\n",
      "5.26s, epoch [727\\1500]: train D loss: 0.5219, train F loss: -0.022894, acc 0.990000\n",
      "5.25s, epoch [728\\1500]: train D loss: 0.5263, train F loss: -0.024052, acc 0.991000\n",
      "5.25s, epoch [729\\1500]: train D loss: 0.5150, train F loss: -0.025718, acc 0.993400\n",
      "5.26s, epoch [730\\1500]: train D loss: 0.5020, train F loss: -0.021780, acc 0.991200\n",
      "5.25s, epoch [731\\1500]: train D loss: 0.5084, train F loss: -0.026485, acc 0.991800\n",
      "5.25s, epoch [732\\1500]: train D loss: 0.5111, train F loss: -0.022791, acc 0.990200\n",
      "5.25s, epoch [733\\1500]: train D loss: 0.5238, train F loss: -0.015307, acc 0.989200\n",
      "5.25s, epoch [734\\1500]: train D loss: 0.5028, train F loss: -0.027861, acc 0.992600\n",
      "5.25s, epoch [735\\1500]: train D loss: 0.5162, train F loss: -0.026504, acc 0.991800\n",
      "5.25s, epoch [736\\1500]: train D loss: 0.5355, train F loss: -0.026827, acc 0.992400\n",
      "5.25s, epoch [737\\1500]: train D loss: 0.5214, train F loss: -0.028664, acc 0.992200\n",
      "5.25s, epoch [738\\1500]: train D loss: 0.5281, train F loss: -0.027392, acc 0.991600\n",
      "5.25s, epoch [739\\1500]: train D loss: 0.5106, train F loss: -0.027467, acc 0.992400\n",
      "5.26s, epoch [740\\1500]: train D loss: 0.4967, train F loss: -0.024883, acc 0.991400\n",
      "5.25s, epoch [741\\1500]: train D loss: 0.5047, train F loss: -0.022674, acc 0.991800\n",
      "5.25s, epoch [742\\1500]: train D loss: 0.5085, train F loss: -0.029412, acc 0.992800\n",
      "5.25s, epoch [743\\1500]: train D loss: 0.5156, train F loss: -0.033924, acc 0.994600\n",
      "5.25s, epoch [744\\1500]: train D loss: 0.5141, train F loss: -0.010699, acc 0.988400\n",
      "5.25s, epoch [745\\1500]: train D loss: 0.5057, train F loss: -0.025839, acc 0.992600\n",
      "5.24s, epoch [746\\1500]: train D loss: 0.4895, train F loss: -0.027823, acc 0.991800\n",
      "5.27s, epoch [747\\1500]: train D loss: 0.5088, train F loss: -0.024575, acc 0.991400\n",
      "5.26s, epoch [748\\1500]: train D loss: 0.5078, train F loss: -0.024023, acc 0.990600\n",
      "5.25s, epoch [749\\1500]: train D loss: 0.5055, train F loss: -0.025486, acc 0.992200\n",
      "5.26s, epoch [750\\1500]: train D loss: 0.5121, train F loss: -0.024412, acc 0.992200\n",
      "5.26s, epoch [751\\1500]: train D loss: 0.5111, train F loss: -0.005738, acc 0.987400\n",
      "5.25s, epoch [752\\1500]: train D loss: 0.5135, train F loss: -0.031574, acc 0.994000\n",
      "5.26s, epoch [753\\1500]: train D loss: 0.4985, train F loss: -0.027234, acc 0.993400\n",
      "5.25s, epoch [754\\1500]: train D loss: 0.5025, train F loss: -0.019126, acc 0.991000\n",
      "5.26s, epoch [755\\1500]: train D loss: 0.5052, train F loss: -0.024711, acc 0.991600\n",
      "5.24s, epoch [756\\1500]: train D loss: 0.5189, train F loss: -0.029816, acc 0.991800\n",
      "5.25s, epoch [757\\1500]: train D loss: 0.5107, train F loss: -0.021797, acc 0.991000\n",
      "5.24s, epoch [758\\1500]: train D loss: 0.5092, train F loss: -0.010189, acc 0.987600\n",
      "5.25s, epoch [759\\1500]: train D loss: 0.5046, train F loss: -0.022169, acc 0.991400\n",
      "5.25s, epoch [760\\1500]: train D loss: 0.4950, train F loss: -0.025923, acc 0.992600\n",
      "5.25s, epoch [761\\1500]: train D loss: 0.4877, train F loss: -0.031514, acc 0.994200\n",
      "5.25s, epoch [762\\1500]: train D loss: 0.5002, train F loss: -0.027114, acc 0.993200\n",
      "5.24s, epoch [763\\1500]: train D loss: 0.5050, train F loss: -0.023071, acc 0.992000\n",
      "5.25s, epoch [764\\1500]: train D loss: 0.5200, train F loss: -0.027933, acc 0.992000\n",
      "5.26s, epoch [765\\1500]: train D loss: 0.5125, train F loss: -0.033877, acc 0.994400\n",
      "5.25s, epoch [766\\1500]: train D loss: 0.4876, train F loss: -0.030621, acc 0.994000\n",
      "5.26s, epoch [767\\1500]: train D loss: 0.5168, train F loss: -0.024187, acc 0.992600\n",
      "5.25s, epoch [768\\1500]: train D loss: 0.5161, train F loss: -0.033121, acc 0.994000\n",
      "5.25s, epoch [769\\1500]: train D loss: 0.5039, train F loss: -0.024637, acc 0.991400\n",
      "5.26s, epoch [770\\1500]: train D loss: 0.5177, train F loss: -0.020725, acc 0.990200\n",
      "5.30s, epoch [771\\1500]: train D loss: 0.5227, train F loss: -0.032110, acc 0.992000\n",
      "5.28s, epoch [772\\1500]: train D loss: 0.5280, train F loss: -0.022172, acc 0.988600\n",
      "5.26s, epoch [773\\1500]: train D loss: 0.4998, train F loss: -0.019484, acc 0.989600\n",
      "5.24s, epoch [774\\1500]: train D loss: 0.4999, train F loss: -0.029967, acc 0.992400\n",
      "5.24s, epoch [775\\1500]: train D loss: 0.5061, train F loss: -0.025893, acc 0.993600\n",
      "5.24s, epoch [776\\1500]: train D loss: 0.5264, train F loss: -0.006584, acc 0.985600\n",
      "5.25s, epoch [777\\1500]: train D loss: 0.5135, train F loss: -0.019634, acc 0.991600\n",
      "5.24s, epoch [778\\1500]: train D loss: 0.5127, train F loss: -0.029509, acc 0.993000\n",
      "5.24s, epoch [779\\1500]: train D loss: 0.5195, train F loss: -0.024252, acc 0.991600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.24s, epoch [780\\1500]: train D loss: 0.5132, train F loss: -0.025073, acc 0.991400\n",
      "5.24s, epoch [781\\1500]: train D loss: 0.5180, train F loss: -0.023732, acc 0.990800\n",
      "5.24s, epoch [782\\1500]: train D loss: 0.5109, train F loss: -0.032665, acc 0.993000\n",
      "5.25s, epoch [783\\1500]: train D loss: 0.5107, train F loss: -0.027011, acc 0.992200\n",
      "5.25s, epoch [784\\1500]: train D loss: 0.4971, train F loss: -0.038187, acc 0.996600\n",
      "5.25s, epoch [785\\1500]: train D loss: 0.5039, train F loss: -0.029494, acc 0.993000\n",
      "5.26s, epoch [786\\1500]: train D loss: 0.5185, train F loss: -0.032532, acc 0.994000\n",
      "5.26s, epoch [787\\1500]: train D loss: 0.5140, train F loss: -0.030116, acc 0.993600\n",
      "5.25s, epoch [788\\1500]: train D loss: 0.5219, train F loss: -0.029123, acc 0.993200\n",
      "5.27s, epoch [789\\1500]: train D loss: 0.5281, train F loss: -0.022815, acc 0.988800\n",
      "5.27s, epoch [790\\1500]: train D loss: 0.5246, train F loss: -0.025436, acc 0.991600\n",
      "5.27s, epoch [791\\1500]: train D loss: 0.5060, train F loss: -0.029498, acc 0.992800\n",
      "5.27s, epoch [792\\1500]: train D loss: 0.5153, train F loss: -0.015491, acc 0.990800\n",
      "5.28s, epoch [793\\1500]: train D loss: 0.5061, train F loss: -0.033247, acc 0.994000\n",
      "5.26s, epoch [794\\1500]: train D loss: 0.5202, train F loss: -0.029195, acc 0.992600\n",
      "5.26s, epoch [795\\1500]: train D loss: 0.5147, train F loss: -0.034854, acc 0.994200\n",
      "5.27s, epoch [796\\1500]: train D loss: 0.5057, train F loss: -0.033447, acc 0.996000\n",
      "5.26s, epoch [797\\1500]: train D loss: 0.5332, train F loss: -0.010603, acc 0.986200\n",
      "5.26s, epoch [798\\1500]: train D loss: 0.5141, train F loss: -0.030003, acc 0.993400\n",
      "5.26s, epoch [799\\1500]: train D loss: 0.5290, train F loss: -0.027687, acc 0.990600\n",
      "5.26s, epoch [800\\1500]: train D loss: 0.5249, train F loss: -0.025667, acc 0.990200\n",
      "5.27s, epoch [801\\1500]: train D loss: 0.5302, train F loss: -0.009658, acc 0.989000\n",
      "5.27s, epoch [802\\1500]: train D loss: 0.5239, train F loss: -0.027280, acc 0.991200\n",
      "5.25s, epoch [803\\1500]: train D loss: 0.5138, train F loss: -0.026189, acc 0.993600\n",
      "5.26s, epoch [804\\1500]: train D loss: 0.5170, train F loss: -0.027714, acc 0.993600\n",
      "5.27s, epoch [805\\1500]: train D loss: 0.5221, train F loss: -0.027241, acc 0.992600\n",
      "5.27s, epoch [806\\1500]: train D loss: 0.5099, train F loss: -0.025420, acc 0.992600\n",
      "5.27s, epoch [807\\1500]: train D loss: 0.5142, train F loss: -0.024527, acc 0.991200\n",
      "5.26s, epoch [808\\1500]: train D loss: 0.5066, train F loss: -0.025535, acc 0.990400\n",
      "5.26s, epoch [809\\1500]: train D loss: 0.5138, train F loss: -0.029195, acc 0.992000\n",
      "5.26s, epoch [810\\1500]: train D loss: 0.5099, train F loss: -0.019255, acc 0.991400\n",
      "5.28s, epoch [811\\1500]: train D loss: 0.5070, train F loss: -0.034472, acc 0.994800\n",
      "5.27s, epoch [812\\1500]: train D loss: 0.5080, train F loss: -0.028056, acc 0.993000\n",
      "5.25s, epoch [813\\1500]: train D loss: 0.5092, train F loss: -0.034727, acc 0.995000\n",
      "5.27s, epoch [814\\1500]: train D loss: 0.5237, train F loss: -0.011851, acc 0.989000\n",
      "5.27s, epoch [815\\1500]: train D loss: 0.5233, train F loss: -0.026451, acc 0.992000\n",
      "5.26s, epoch [816\\1500]: train D loss: 0.5191, train F loss: -0.028174, acc 0.992400\n",
      "5.27s, epoch [817\\1500]: train D loss: 0.5306, train F loss: -0.016745, acc 0.989600\n",
      "5.26s, epoch [818\\1500]: train D loss: 0.5180, train F loss: -0.030146, acc 0.992400\n",
      "5.25s, epoch [819\\1500]: train D loss: 0.5138, train F loss: -0.022360, acc 0.991200\n",
      "5.26s, epoch [820\\1500]: train D loss: 0.5100, train F loss: -0.028725, acc 0.992800\n",
      "5.27s, epoch [821\\1500]: train D loss: 0.5232, train F loss: -0.020713, acc 0.990000\n",
      "5.26s, epoch [822\\1500]: train D loss: 0.5261, train F loss: -0.020439, acc 0.989400\n",
      "5.26s, epoch [823\\1500]: train D loss: 0.5069, train F loss: -0.031652, acc 0.994200\n",
      "5.26s, epoch [824\\1500]: train D loss: 0.5141, train F loss: -0.035468, acc 0.995400\n",
      "5.27s, epoch [825\\1500]: train D loss: 0.5043, train F loss: -0.034048, acc 0.996000\n",
      "5.26s, epoch [826\\1500]: train D loss: 0.4994, train F loss: -0.014901, acc 0.992800\n",
      "5.27s, epoch [827\\1500]: train D loss: 0.5295, train F loss: -0.002901, acc 0.987400\n",
      "5.27s, epoch [828\\1500]: train D loss: 0.5097, train F loss: -0.033573, acc 0.995200\n",
      "5.26s, epoch [829\\1500]: train D loss: 0.5134, train F loss: -0.022225, acc 0.990200\n",
      "5.27s, epoch [830\\1500]: train D loss: 0.5226, train F loss: -0.021409, acc 0.991200\n",
      "5.27s, epoch [831\\1500]: train D loss: 0.5110, train F loss: -0.031342, acc 0.993800\n",
      "5.27s, epoch [832\\1500]: train D loss: 0.5119, train F loss: -0.018776, acc 0.990600\n",
      "5.27s, epoch [833\\1500]: train D loss: 0.5120, train F loss: -0.029571, acc 0.993000\n",
      "5.28s, epoch [834\\1500]: train D loss: 0.5149, train F loss: -0.026777, acc 0.991600\n",
      "5.26s, epoch [835\\1500]: train D loss: 0.5101, train F loss: -0.029957, acc 0.993800\n",
      "5.26s, epoch [836\\1500]: train D loss: 0.4974, train F loss: -0.030647, acc 0.994200\n",
      "5.26s, epoch [837\\1500]: train D loss: 0.5159, train F loss: -0.030071, acc 0.993200\n",
      "5.26s, epoch [838\\1500]: train D loss: 0.5163, train F loss: -0.031021, acc 0.993400\n",
      "5.26s, epoch [839\\1500]: train D loss: 0.5229, train F loss: -0.005406, acc 0.987600\n",
      "5.27s, epoch [840\\1500]: train D loss: 0.5153, train F loss: -0.033196, acc 0.994400\n",
      "5.26s, epoch [841\\1500]: train D loss: 0.5267, train F loss: -0.033246, acc 0.993000\n",
      "5.26s, epoch [842\\1500]: train D loss: 0.5114, train F loss: -0.024574, acc 0.992000\n",
      "5.27s, epoch [843\\1500]: train D loss: 0.5129, train F loss: -0.029286, acc 0.992800\n",
      "5.26s, epoch [844\\1500]: train D loss: 0.5172, train F loss: -0.028811, acc 0.992200\n",
      "5.27s, epoch [845\\1500]: train D loss: 0.5110, train F loss: -0.032070, acc 0.994000\n",
      "5.26s, epoch [846\\1500]: train D loss: 0.5267, train F loss: -0.024035, acc 0.991800\n",
      "5.26s, epoch [847\\1500]: train D loss: 0.5218, train F loss: -0.028360, acc 0.992200\n",
      "5.27s, epoch [848\\1500]: train D loss: 0.5218, train F loss: -0.016829, acc 0.991600\n",
      "5.26s, epoch [849\\1500]: train D loss: 0.5224, train F loss: -0.024914, acc 0.990000\n",
      "5.26s, epoch [850\\1500]: train D loss: 0.5102, train F loss: -0.029295, acc 0.991800\n",
      "5.26s, epoch [851\\1500]: train D loss: 0.5248, train F loss: -0.006700, acc 0.990400\n",
      "5.27s, epoch [852\\1500]: train D loss: 0.5129, train F loss: -0.028115, acc 0.992800\n",
      "5.27s, epoch [853\\1500]: train D loss: 0.5051, train F loss: -0.030943, acc 0.994400\n",
      "5.26s, epoch [854\\1500]: train D loss: 0.5102, train F loss: -0.028301, acc 0.994200\n",
      "5.26s, epoch [855\\1500]: train D loss: 0.5114, train F loss: -0.029810, acc 0.995000\n",
      "5.27s, epoch [856\\1500]: train D loss: 0.5190, train F loss: -0.025615, acc 0.992400\n",
      "5.28s, epoch [857\\1500]: train D loss: 0.5191, train F loss: -0.018771, acc 0.989000\n",
      "5.27s, epoch [858\\1500]: train D loss: 0.5248, train F loss: -0.029488, acc 0.992000\n",
      "5.28s, epoch [859\\1500]: train D loss: 0.5181, train F loss: -0.023180, acc 0.989000\n",
      "5.26s, epoch [860\\1500]: train D loss: 0.5158, train F loss: -0.024869, acc 0.992600\n",
      "5.27s, epoch [861\\1500]: train D loss: 0.5345, train F loss: -0.029548, acc 0.992000\n",
      "5.27s, epoch [862\\1500]: train D loss: 0.5070, train F loss: -0.022888, acc 0.992400\n",
      "5.26s, epoch [863\\1500]: train D loss: 0.5195, train F loss: -0.032746, acc 0.994000\n",
      "5.26s, epoch [864\\1500]: train D loss: 0.5039, train F loss: -0.036181, acc 0.995400\n",
      "5.33s, epoch [865\\1500]: train D loss: 0.5192, train F loss: -0.030034, acc 0.993600\n",
      "5.26s, epoch [866\\1500]: train D loss: 0.5110, train F loss: -0.028034, acc 0.991600\n",
      "5.26s, epoch [867\\1500]: train D loss: 0.5157, train F loss: -0.031033, acc 0.995000\n",
      "5.27s, epoch [868\\1500]: train D loss: 0.5146, train F loss: -0.034016, acc 0.992800\n",
      "5.26s, epoch [869\\1500]: train D loss: 0.5284, train F loss: -0.014833, acc 0.991600\n",
      "5.27s, epoch [870\\1500]: train D loss: 0.5216, train F loss: -0.032017, acc 0.994000\n",
      "5.27s, epoch [871\\1500]: train D loss: 0.5371, train F loss: -0.028397, acc 0.991200\n",
      "5.26s, epoch [872\\1500]: train D loss: 0.5158, train F loss: -0.032919, acc 0.993800\n",
      "5.26s, epoch [873\\1500]: train D loss: 0.5197, train F loss: -0.027369, acc 0.991800\n",
      "5.26s, epoch [874\\1500]: train D loss: 0.5323, train F loss: -0.025896, acc 0.992400\n",
      "5.27s, epoch [875\\1500]: train D loss: 0.5260, train F loss: -0.024252, acc 0.990800\n",
      "5.26s, epoch [876\\1500]: train D loss: 0.5166, train F loss: -0.028932, acc 0.992800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.27s, epoch [877\\1500]: train D loss: 0.5182, train F loss: -0.027385, acc 0.991600\n",
      "5.26s, epoch [878\\1500]: train D loss: 0.5209, train F loss: -0.028811, acc 0.993800\n",
      "5.29s, epoch [879\\1500]: train D loss: 0.5136, train F loss: -0.029681, acc 0.992600\n",
      "5.29s, epoch [880\\1500]: train D loss: 0.5214, train F loss: -0.026597, acc 0.992800\n",
      "5.30s, epoch [881\\1500]: train D loss: 0.5331, train F loss: -0.017772, acc 0.990000\n",
      "5.28s, epoch [882\\1500]: train D loss: 0.5219, train F loss: -0.027684, acc 0.993400\n",
      "5.26s, epoch [883\\1500]: train D loss: 0.5142, train F loss: -0.032120, acc 0.994200\n",
      "5.26s, epoch [884\\1500]: train D loss: 0.5104, train F loss: -0.031014, acc 0.994600\n",
      "5.27s, epoch [885\\1500]: train D loss: 0.5147, train F loss: -0.029444, acc 0.993200\n",
      "5.26s, epoch [886\\1500]: train D loss: 0.5393, train F loss: -0.017134, acc 0.989400\n",
      "5.27s, epoch [887\\1500]: train D loss: 0.5233, train F loss: -0.029957, acc 0.992400\n",
      "5.27s, epoch [888\\1500]: train D loss: 0.5110, train F loss: -0.021594, acc 0.991200\n",
      "5.27s, epoch [889\\1500]: train D loss: 0.5194, train F loss: -0.033483, acc 0.994000\n",
      "5.27s, epoch [890\\1500]: train D loss: 0.5215, train F loss: -0.027189, acc 0.991400\n",
      "5.27s, epoch [891\\1500]: train D loss: 0.5175, train F loss: -0.035085, acc 0.994000\n",
      "5.27s, epoch [892\\1500]: train D loss: 0.5122, train F loss: -0.037197, acc 0.995200\n",
      "5.27s, epoch [893\\1500]: train D loss: 0.5205, train F loss: -0.019823, acc 0.990400\n",
      "5.27s, epoch [894\\1500]: train D loss: 0.5311, train F loss: -0.022198, acc 0.989600\n",
      "5.27s, epoch [895\\1500]: train D loss: 0.5139, train F loss: -0.032706, acc 0.994200\n",
      "5.27s, epoch [896\\1500]: train D loss: 0.5254, train F loss: -0.021832, acc 0.991600\n",
      "5.27s, epoch [897\\1500]: train D loss: 0.5264, train F loss: -0.026303, acc 0.992800\n",
      "5.26s, epoch [898\\1500]: train D loss: 0.5193, train F loss: -0.024499, acc 0.992600\n",
      "5.27s, epoch [899\\1500]: train D loss: 0.5385, train F loss: -0.017530, acc 0.989200\n",
      "5.27s, epoch [900\\1500]: train D loss: 0.5061, train F loss: -0.031226, acc 0.992800\n",
      "5.25s, epoch [901\\1500]: train D loss: 0.5119, train F loss: -0.034452, acc 0.995200\n",
      "5.26s, epoch [902\\1500]: train D loss: 0.5172, train F loss: -0.032096, acc 0.994200\n",
      "5.25s, epoch [903\\1500]: train D loss: 0.5277, train F loss: -0.026438, acc 0.991800\n",
      "5.26s, epoch [904\\1500]: train D loss: 0.5189, train F loss: -0.017402, acc 0.989600\n",
      "5.25s, epoch [905\\1500]: train D loss: 0.5171, train F loss: -0.027823, acc 0.992200\n",
      "5.25s, epoch [906\\1500]: train D loss: 0.5138, train F loss: -0.032747, acc 0.994600\n",
      "5.25s, epoch [907\\1500]: train D loss: 0.5345, train F loss: -0.012344, acc 0.989400\n",
      "5.25s, epoch [908\\1500]: train D loss: 0.5150, train F loss: -0.031272, acc 0.993200\n",
      "5.25s, epoch [909\\1500]: train D loss: 0.5180, train F loss: -0.029583, acc 0.993600\n",
      "5.25s, epoch [910\\1500]: train D loss: 0.5076, train F loss: -0.025978, acc 0.994800\n",
      "5.25s, epoch [911\\1500]: train D loss: 0.5361, train F loss: 0.005005, acc 0.984600\n",
      "5.25s, epoch [912\\1500]: train D loss: 0.5107, train F loss: -0.029453, acc 0.992400\n",
      "5.26s, epoch [913\\1500]: train D loss: 0.5101, train F loss: -0.030194, acc 0.993800\n",
      "5.26s, epoch [914\\1500]: train D loss: 0.5234, train F loss: -0.031704, acc 0.992200\n",
      "5.25s, epoch [915\\1500]: train D loss: 0.5199, train F loss: -0.030860, acc 0.993400\n",
      "5.25s, epoch [916\\1500]: train D loss: 0.5111, train F loss: -0.032997, acc 0.994000\n",
      "5.25s, epoch [917\\1500]: train D loss: 0.5201, train F loss: -0.028406, acc 0.993600\n",
      "5.25s, epoch [918\\1500]: train D loss: 0.5174, train F loss: -0.011325, acc 0.991400\n",
      "5.25s, epoch [919\\1500]: train D loss: 0.5511, train F loss: 0.062294, acc 0.975400\n",
      "5.26s, epoch [920\\1500]: train D loss: 0.5203, train F loss: -0.025561, acc 0.991800\n",
      "5.25s, epoch [921\\1500]: train D loss: 0.5090, train F loss: -0.034888, acc 0.994400\n",
      "5.26s, epoch [922\\1500]: train D loss: 0.4972, train F loss: -0.019630, acc 0.991400\n",
      "5.26s, epoch [923\\1500]: train D loss: 0.5084, train F loss: -0.024922, acc 0.992000\n",
      "5.25s, epoch [924\\1500]: train D loss: 0.5171, train F loss: -0.028897, acc 0.993800\n",
      "5.25s, epoch [925\\1500]: train D loss: 0.5219, train F loss: -0.031160, acc 0.993400\n",
      "5.27s, epoch [926\\1500]: train D loss: 0.5146, train F loss: -0.037056, acc 0.995400\n",
      "5.25s, epoch [927\\1500]: train D loss: 0.5133, train F loss: -0.033861, acc 0.994400\n",
      "5.25s, epoch [928\\1500]: train D loss: 0.5130, train F loss: -0.031417, acc 0.993000\n",
      "5.25s, epoch [929\\1500]: train D loss: 0.5160, train F loss: -0.033063, acc 0.993800\n",
      "5.25s, epoch [930\\1500]: train D loss: 0.5149, train F loss: -0.028584, acc 0.993000\n",
      "5.25s, epoch [931\\1500]: train D loss: 0.5199, train F loss: -0.025802, acc 0.993000\n",
      "5.26s, epoch [932\\1500]: train D loss: 0.5210, train F loss: -0.034120, acc 0.994400\n",
      "5.26s, epoch [933\\1500]: train D loss: 0.5102, train F loss: -0.036829, acc 0.995600\n",
      "5.25s, epoch [934\\1500]: train D loss: 0.5149, train F loss: -0.036994, acc 0.996000\n",
      "5.26s, epoch [935\\1500]: train D loss: 0.5072, train F loss: -0.038283, acc 0.996000\n",
      "5.25s, epoch [936\\1500]: train D loss: 0.5309, train F loss: -0.019099, acc 0.992200\n",
      "5.25s, epoch [937\\1500]: train D loss: 0.5398, train F loss: -0.010677, acc 0.986400\n",
      "5.25s, epoch [938\\1500]: train D loss: 0.5210, train F loss: -0.028603, acc 0.993400\n",
      "5.25s, epoch [939\\1500]: train D loss: 0.5224, train F loss: -0.020607, acc 0.991600\n",
      "5.26s, epoch [940\\1500]: train D loss: 0.5227, train F loss: -0.024013, acc 0.992600\n",
      "5.25s, epoch [941\\1500]: train D loss: 0.5330, train F loss: -0.036591, acc 0.995600\n",
      "5.26s, epoch [942\\1500]: train D loss: 0.5179, train F loss: -0.030447, acc 0.993800\n",
      "5.27s, epoch [943\\1500]: train D loss: 0.5206, train F loss: -0.032033, acc 0.994400\n",
      "5.24s, epoch [944\\1500]: train D loss: 0.5106, train F loss: -0.035368, acc 0.995400\n",
      "5.25s, epoch [945\\1500]: train D loss: 0.5059, train F loss: -0.031069, acc 0.993600\n",
      "5.25s, epoch [946\\1500]: train D loss: 0.5235, train F loss: -0.034840, acc 0.993600\n",
      "5.26s, epoch [947\\1500]: train D loss: 0.5151, train F loss: -0.024586, acc 0.992800\n",
      "5.25s, epoch [948\\1500]: train D loss: 0.5346, train F loss: -0.026968, acc 0.991600\n",
      "5.25s, epoch [949\\1500]: train D loss: 0.5181, train F loss: -0.029269, acc 0.993400\n",
      "5.25s, epoch [950\\1500]: train D loss: 0.5100, train F loss: -0.034133, acc 0.994400\n",
      "5.24s, epoch [951\\1500]: train D loss: 0.5078, train F loss: -0.036944, acc 0.995400\n",
      "5.25s, epoch [952\\1500]: train D loss: 0.5218, train F loss: -0.033651, acc 0.994400\n",
      "5.27s, epoch [953\\1500]: train D loss: 0.5086, train F loss: -0.024668, acc 0.991800\n",
      "5.24s, epoch [954\\1500]: train D loss: 0.5337, train F loss: -0.023414, acc 0.989400\n",
      "5.25s, epoch [955\\1500]: train D loss: 0.5306, train F loss: -0.028433, acc 0.992400\n",
      "5.25s, epoch [956\\1500]: train D loss: 0.5295, train F loss: -0.026908, acc 0.992400\n",
      "5.25s, epoch [957\\1500]: train D loss: 0.5397, train F loss: -0.022600, acc 0.991000\n",
      "5.25s, epoch [958\\1500]: train D loss: 0.5233, train F loss: -0.024432, acc 0.993400\n",
      "5.25s, epoch [959\\1500]: train D loss: 0.5162, train F loss: -0.032939, acc 0.993600\n",
      "5.25s, epoch [960\\1500]: train D loss: 0.5191, train F loss: -0.023407, acc 0.991400\n",
      "5.25s, epoch [961\\1500]: train D loss: 0.5320, train F loss: -0.026506, acc 0.991800\n",
      "5.25s, epoch [962\\1500]: train D loss: 0.5279, train F loss: -0.033736, acc 0.993000\n",
      "5.25s, epoch [963\\1500]: train D loss: 0.5183, train F loss: -0.028263, acc 0.993000\n",
      "5.25s, epoch [964\\1500]: train D loss: 0.5263, train F loss: -0.026135, acc 0.991800\n",
      "5.25s, epoch [965\\1500]: train D loss: 0.5156, train F loss: -0.031064, acc 0.993800\n",
      "5.26s, epoch [966\\1500]: train D loss: 0.5254, train F loss: -0.037952, acc 0.995600\n",
      "5.27s, epoch [967\\1500]: train D loss: 0.5121, train F loss: -0.033422, acc 0.994800\n",
      "5.25s, epoch [968\\1500]: train D loss: 0.5145, train F loss: -0.027509, acc 0.993600\n",
      "5.24s, epoch [969\\1500]: train D loss: 0.5312, train F loss: -0.024290, acc 0.991800\n",
      "5.25s, epoch [970\\1500]: train D loss: 0.5396, train F loss: -0.036527, acc 0.994400\n",
      "5.26s, epoch [971\\1500]: train D loss: 0.5445, train F loss: -0.035733, acc 0.993000\n",
      "5.25s, epoch [972\\1500]: train D loss: 0.5157, train F loss: -0.024824, acc 0.991600\n",
      "5.26s, epoch [973\\1500]: train D loss: 0.5449, train F loss: -0.025874, acc 0.990000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.25s, epoch [974\\1500]: train D loss: 0.5285, train F loss: -0.029129, acc 0.992600\n",
      "5.25s, epoch [975\\1500]: train D loss: 0.5398, train F loss: -0.031335, acc 0.992000\n",
      "5.25s, epoch [976\\1500]: train D loss: 0.5230, train F loss: -0.026537, acc 0.991800\n",
      "5.25s, epoch [977\\1500]: train D loss: 0.5260, train F loss: -0.035304, acc 0.994200\n",
      "5.25s, epoch [978\\1500]: train D loss: 0.5219, train F loss: -0.033460, acc 0.994000\n",
      "5.25s, epoch [979\\1500]: train D loss: 0.5369, train F loss: -0.032442, acc 0.994600\n",
      "5.25s, epoch [980\\1500]: train D loss: 0.5216, train F loss: -0.038471, acc 0.995600\n",
      "5.24s, epoch [981\\1500]: train D loss: 0.5160, train F loss: -0.034684, acc 0.994200\n",
      "5.25s, epoch [982\\1500]: train D loss: 0.5390, train F loss: -0.017325, acc 0.990200\n",
      "5.28s, epoch [983\\1500]: train D loss: 0.5374, train F loss: -0.019995, acc 0.990400\n",
      "5.25s, epoch [984\\1500]: train D loss: 0.5317, train F loss: -0.025430, acc 0.990200\n",
      "5.26s, epoch [985\\1500]: train D loss: 0.5308, train F loss: -0.036849, acc 0.994400\n",
      "5.25s, epoch [986\\1500]: train D loss: 0.5228, train F loss: -0.035568, acc 0.994600\n",
      "5.26s, epoch [987\\1500]: train D loss: 0.5302, train F loss: -0.035147, acc 0.994000\n",
      "5.30s, epoch [988\\1500]: train D loss: 0.5396, train F loss: -0.036535, acc 0.993800\n",
      "5.32s, epoch [989\\1500]: train D loss: 0.5249, train F loss: -0.033314, acc 0.993600\n",
      "5.33s, epoch [990\\1500]: train D loss: 0.5259, train F loss: -0.017233, acc 0.989600\n",
      "5.30s, epoch [991\\1500]: train D loss: 0.5260, train F loss: -0.029769, acc 0.992800\n",
      "5.28s, epoch [992\\1500]: train D loss: 0.5312, train F loss: -0.034827, acc 0.993200\n",
      "5.32s, epoch [993\\1500]: train D loss: 0.5252, train F loss: -0.032213, acc 0.992800\n",
      "5.36s, epoch [994\\1500]: train D loss: 0.5322, train F loss: -0.036130, acc 0.994200\n",
      "5.37s, epoch [995\\1500]: train D loss: 0.5263, train F loss: -0.031169, acc 0.993200\n",
      "5.36s, epoch [996\\1500]: train D loss: 0.5439, train F loss: -0.030358, acc 0.993000\n",
      "5.32s, epoch [997\\1500]: train D loss: 0.5401, train F loss: -0.031285, acc 0.993000\n",
      "5.32s, epoch [998\\1500]: train D loss: 0.5373, train F loss: -0.032948, acc 0.992800\n",
      "5.27s, epoch [999\\1500]: train D loss: 0.5231, train F loss: -0.035489, acc 0.995200\n",
      "5.35s, epoch [1000\\1500]: train D loss: 0.5286, train F loss: -0.038010, acc 0.995000\n",
      "5.34s, epoch [1001\\1500]: train D loss: 0.5305, train F loss: -0.037883, acc 0.996200\n",
      "5.29s, epoch [1002\\1500]: train D loss: 0.5395, train F loss: -0.032335, acc 0.993000\n",
      "5.38s, epoch [1003\\1500]: train D loss: 0.5360, train F loss: -0.026568, acc 0.992400\n",
      "5.45s, epoch [1004\\1500]: train D loss: 0.5209, train F loss: -0.029003, acc 0.993000\n",
      "5.47s, epoch [1005\\1500]: train D loss: 0.5512, train F loss: -0.034144, acc 0.992000\n",
      "24"
     ]
    }
   ],
   "source": [
    "source_dataset = ImageFolder('real_or_drawing/train_data', transform=source_transform)\n",
    "target_dataset = ImageFolder('real_or_drawing/test_data', transform=target_transform)\n",
    "test_dataset = ImageFolder('real_or_drawing/test_data', transform=test_transform)\n",
    "source_loader = get_data_loader(source_dataset, batch_size=256, mode = 'train')\n",
    "target_loader = get_data_loader(target_dataset, batch_size=256, mode = 'train')\n",
    "test_loader = get_data_loader(test_dataset,batch_size=512, mode = 'eval')\n",
    "\n",
    "# epoch test 100,200, 300, 400,500\n",
    "# 100, 0.51349\n",
    "# 200, 0.49530\n",
    "# 300, 0.53430\n",
    "# 400, 0.56480\n",
    "# 500, 0.59531\n",
    "# 500, test without aug, 0.61848\n",
    "\n",
    "set_seed(0)\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "label_predictor = LabelPredictor().to(device)\n",
    "domain_classifier = DomainClassifier().to(device)\n",
    "\n",
    "class_criterion = nn.CrossEntropyLoss()\n",
    "domain_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_F = optim.Adam(feature_extractor.parameters())\n",
    "optimizer_C = optim.Adam(label_predictor.parameters())\n",
    "optimizer_D = optim.Adam(domain_classifier.parameters())\n",
    "\n",
    "# epoch test 100,200, 300, 400,500\n",
    "source_loader = get_data_loader(source_dataset, mode = 'train')\n",
    "target_loader = get_data_loader(target_dataset, mode = 'train')\n",
    "test_loader = get_data_loader(target_dataset, mode = 'eval')\n",
    "epochs = 1500\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    st = time.time()\n",
    "    train_D_loss, train_F_loss, train_acc = train_epoch(source_loader, target_loader, lamb=0.1)\n",
    "    ed = time.time()\n",
    "    if epoch % 100 == 0:\n",
    "        save_model(feature_extractor,  f'extractor_model_{epoch}.bin')\n",
    "        save_model(label_predictor, f'predictor_model_{epoch}.bin')\n",
    "        save_model(domain_classifier, f'domain_classifier_{epoch}.bin')\n",
    "        pred_labels, pred_probs,_ = testing(feature_extractor, label_predictor, test_loader)\n",
    "        df = pd.DataFrame({'id': np.arange(0,len(pred_labels)), 'label': pred_labels})\n",
    "        df.to_csv(f'DaNN_submission_{epoch}.csv',index=False)\n",
    "\n",
    "    print('{:4.2f}s, epoch [{:0>3d}\\{}]: train D loss: {:6.4f}, train F loss: {:6.6f}, acc {:6.6f}'.format(ed-st, epoch, epochs,  train_D_loss, train_F_loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOYFwkLsp9Qa"
   },
   "outputs": [],
   "source": [
    "# train without adversial\n",
    "set_seed(0)\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "label_predictor = LabelPredictor().to(device)\n",
    "domain_classifier = DomainClassifier().to(device)\n",
    "\n",
    "class_criterion = nn.CrossEntropyLoss()\n",
    "domain_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_F = optim.Adam(feature_extractor.parameters())\n",
    "optimizer_C = optim.Adam(label_predictor.parameters())\n",
    "optimizer_D = optim.Adam(domain_classifier.parameters())\n",
    "\n",
    "# epoch test 100,200, 300, 400,500\n",
    "source_loader = get_data_loader(source_dataset, mode = 'train')\n",
    "target_loader = get_data_loader(target_dataset, mode = 'train')\n",
    "test_loader = get_data_loader(target_dataset, mode = 'eval')\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    st = time.time()\n",
    "    train_D_loss, train_F_loss, train_acc = train_epoch(source_loader, target_loader, lamb=0)\n",
    "    ed = time.time()\n",
    "    if epoch % 50 == 0:\n",
    "        save_model(feature_extractor,  f'extractor_model_noadv_{epoch}.bin')\n",
    "        save_model(label_predictor, f'predictor_model_noadv_{epoch}.bin')\n",
    "        save_model(domain_classifier, f'domain_classifier_nodav_{epoch}.bin')\n",
    "\n",
    "    print('{:4.2f}s, epoch [{:0>3d}\\{}]: train D loss: {:6.4f}, train F loss: {:6.6f}, acc {:6.6f}'.format(ed-st, epoch, epochs,  train_D_loss, train_F_loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j12dF9Kyfm8v"
   },
   "outputs": [],
   "source": [
    "# train \n",
    "# 增大 lamb的值效果不好\n",
    "set_seed(0)\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "label_predictor = LabelPredictor().to(device)\n",
    "domain_classifier = DomainClassifier().to(device)\n",
    "\n",
    "class_criterion = nn.CrossEntropyLoss()\n",
    "domain_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_F = optim.Adam(feature_extractor.parameters())\n",
    "optimizer_C = optim.Adam(label_predictor.parameters())\n",
    "optimizer_D = optim.Adam(domain_classifier.parameters())\n",
    "\n",
    "# testing\n",
    "set_seed(0)\n",
    "# model name\n",
    "epochs_num = 500\n",
    "ex_name = f'extractor_model_{epochs_num}.bin'\n",
    "pr_name = f'predictor_model_{epochs_num}.bin'\n",
    "dc_name = f'domain_classifier_{epochs_num}.bin'\n",
    "# data\n",
    "test_loader = get_data_loader(test_dataset, mode = 'eval')\n",
    "# load model\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "label_predictr = LabelPredictor().to(device)\n",
    "domain_classifier = DomainClassifier().to(device)\n",
    "\n",
    "feature_extractor = load_model(feature_extractor, ex_name)\n",
    "label_predictor = load_model(label_predictor, pr_name)\n",
    "domain_classifier = DomainClassifier().to(device)\n",
    "base = 500\n",
    "epochs = 300\n",
    "\n",
    "for epoch in range(base+1, base+epochs+1):\n",
    "    st = time.time()\n",
    "    train_D_loss, train_F_loss, train_acc = train_epoch(source_loader, target_loader, lamb=0.1)\n",
    "    ed = time.time()\n",
    "    if epoch % 50 == 0:\n",
    "        save_model(feature_extractor,  f'extractor_model_{epoch}.bin')\n",
    "        save_model(label_predictor, f'predictor_model_{epoch}.bin')\n",
    "        save_model(domain_classifier, f'domain_classifier_{epoch}.bin')\n",
    "\n",
    "    print('{:4.2f}s, epoch [{:0>3d}\\{}]: train D loss: {:6.4f}, train F loss: {:6.6f}, acc {:6.6f}'.format(ed-st, epoch, epochs,  train_D_loss, train_F_loss, train_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pXAGSdgvST3v"
   },
   "source": [
    "## Self-training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EUO54JfpUagS"
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dd0XJV6FSTdX"
   },
   "outputs": [],
   "source": [
    "class DrawDataset(data.Dataset):\n",
    "    def __init__(self, data, labels, transform):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = self.transform(Image.open(self.data[i]))\n",
    "        y = self.labels[i]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class AddDataset(data.Dataset):\n",
    "    def __init__(self, data, labels, s_tf, t_tf):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.s_tf = s_tf\n",
    "        self.t_tf = t_tf\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        isadd = True if i >= 5000 else False\n",
    "        tf = self.t_tf if isadd else self.s_tf\n",
    "        x = tf(Image.open(self.data[i]))\n",
    "        y = self.labels[i]\n",
    "        return x, (y, isadd)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IcIJmy4sWJT0"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_data(data_path, name):\n",
    "    data_path = os.path.join(data_path, name)\n",
    "    paths = []\n",
    "    labels = []\n",
    "    for dir_ in os.listdir(data_path):\n",
    "        dir_path = os.path.join(data_path, dir_)\n",
    "        label = int(dir_)\n",
    "        for file in  os.listdir(dir_path):\n",
    "            file_path = os.path.join(dir_path, file)\n",
    "            paths.append(file_path)\n",
    "            labels.append(label)\n",
    "    paths = np.array(paths)\n",
    "    labels = np.array(labels)\n",
    "    return paths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uzhC7-Zn5XuQ"
   },
   "outputs": [],
   "source": [
    "def train_epoch(source_dataloader, target_dataloader, lamb):\n",
    "    '''\n",
    "      Args:\n",
    "        source_dataloader: source data的dataloader\n",
    "        target_dataloader: target data的dataloader\n",
    "        lamb: 調控adversarial的loss係數。\n",
    "    '''\n",
    "\n",
    "    # D loss: Domain Classifier的loss\n",
    "    # F loss: Feature Extrator & Label Predictor的loss\n",
    "    # total_hit: 計算目前對了幾筆 total_num: 目前經過了幾筆\n",
    "    set_seed(0)\n",
    "    lamb = 0.1\n",
    "    running_D_loss, running_F_loss = 0.0, 0.0\n",
    "    total_hit, total_num = 0.0, 0.0\n",
    "    d_num = 0\n",
    "    mix_num = 0\n",
    "    # 验证混合标签是否正确区分\n",
    "    t_num = 0\n",
    "    # 实际上只用了部分 target在训练， 但是由于shuffle，每次用的target都不太一样\n",
    "    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_dataloader, target_dataloader)):\n",
    "        # 对于混合数据集，需要标注来源，否则D的训练会被影响\n",
    "        source_label, isadd = source_label\n",
    "\n",
    "        source_data = source_data.to(device)\n",
    "        source_label = source_label.to(device, dtype = torch.long)\n",
    "        target_data = target_data.to(device)\n",
    "\n",
    "        # 我們把source data和target data混在一起，否則batch_norm可能會算錯 (兩邊的data的mean/var不太一樣)\n",
    "        # batch normlization layer\n",
    "        mixed_data = torch.cat([source_data, target_data], dim=0)\n",
    "        domain_label = torch.zeros([source_data.shape[0] + target_data.shape[0], 1]).to(device, dtype = torch.long)\n",
    "        # 設定source data的label為1, 需要把其中target混合部分正确标注为0\n",
    "        domain_label[:source_data.shape[0]] = 1\n",
    "        domain_label[:source_data.shape[0]][isadd] = 0\n",
    "        t_num += np.sum((domain_label == 1).cpu().numpy())\n",
    "        # Step 1 : 訓練Domain Classifier\n",
    "        feature = feature_extractor(mixed_data)\n",
    "        # 因為我們在Step 1不需要訓練Feature Extractor，所以把feature detach避免loss backprop上去。\n",
    "        domain_logits = domain_classifier(feature.detach())\n",
    "        loss = domain_criterion(domain_logits, domain_label.float())\n",
    "        running_D_loss+= loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Step 2 : 訓練Feature Extractor和Predictor\n",
    "        class_logits = label_predictor(feature[:source_data.shape[0]])\n",
    "        domain_logits = domain_classifier(feature)\n",
    "        # 计算预测结果\n",
    "        # domain \n",
    "        d_preds = nn.functional.sigmoid(domain_logits)\n",
    "        d_preds[d_preds>0.5] = 1\n",
    "        d_preds[d_preds<=0.5] = 0\n",
    "        d_num += np.sum((d_preds ==domain_label).cpu().numpy())\n",
    "        mix_num += len(domain_label)\n",
    "\n",
    "        # loss為原本的class CE - lamb * domain BCE，相減的原因同GAN中的Discriminator中的G loss\n",
    "        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label.float())\n",
    "        running_F_loss+= loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_F.step()\n",
    "        optimizer_C.step()\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        optimizer_F.zero_grad()\n",
    "        optimizer_C.zero_grad()\n",
    "\n",
    "        total_hit += torch.sum(torch.argmax(class_logits, dim=1) == source_label).item()\n",
    "        total_num += source_data.shape[0]\n",
    "        print(i, end='\\r')\n",
    "    print('D_acc: {:4.2f}'.format(d_num /mix_num), end=' ')\n",
    "    return running_D_loss / (i+1), running_F_loss / (i+1), total_hit / total_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zz-rfaEXY2j7"
   },
   "outputs": [],
   "source": [
    "data_path = 'c:/p/real_or_drawing'\n",
    "source_x_paths, source_y = get_data(data_path, 'train_data')\n",
    "target_x_paths, target_y = get_data(data_path, 'test_data')\n",
    "source_dataset = DrawDataset(source_x_paths, source_y, source_transform)\n",
    "target_dataset = DrawDataset(target_x_paths, target_y, target_transform)\n",
    "test_dataset = DrawDataset(target_x_paths, target_y, test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SVYoPlQFZXpf"
   },
   "outputs": [],
   "source": [
    "source_loader = get_data_loader(source_dataset, mode = 'train')\n",
    "target_loader = get_data_loader(target_dataset, mode = 'train')\n",
    "test_loader = get_data_loader(test_dataset, mode = 'eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dnzjeUcK5Xua"
   },
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "epochs_num = 1000\n",
    "ex_name = f'extractor_model_{epochs_num}.bin'\n",
    "pr_name = f'predictor_model_{epochs_num}.bin'\n",
    "dc_name = f'domain_classifier_{epochs_num}.bin'\n",
    "# data\n",
    "test_loader = get_data_loader(test_dataset, batch_size=512, mode = 'eval')\n",
    "# load model\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "label_predictr = LabelPredictor().to(device)\n",
    "domain_classifier = DomainClassifier().to(device)\n",
    "\n",
    "feature_extractor = load_model(feature_extractor, ex_name)\n",
    "label_predictor = load_model(label_predictor, pr_name)\n",
    "domain_classifier = DomainClassifier().to(device)\n",
    "\n",
    "class_criterion = nn.CrossEntropyLoss()\n",
    "domain_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_F = optim.Adam(feature_extractor.parameters())\n",
    "optimizer_C = optim.Adam(label_predictor.parameters())\n",
    "optimizer_D = optim.Adam(domain_classifier.parameters())\n",
    "\n",
    "\n",
    "total_epochs = 1000\n",
    "threshold = 0.9\n",
    "mix_num = 5000 # pesudo labeled data 的数量\n",
    "sl_epochs = 10  # self  training time\n",
    "epochs = 200  # training  epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OjglEAp8lDi8",
    "outputId": "9b6ed19a-ab5b-48ea-82bc-8d4d86099b1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add : 5000 new source size: 10000\n",
      "D_acc: 0.99 8.83s, epoch [1001\\200]: train D loss: 0.0905, train F loss: 12.319286, acc 0.747700\n",
      "D_acc: 1.00 8.33s, epoch [1002\\200]: train D loss: 0.0233, train F loss: 1.046878, acc 0.825800\n",
      "D_acc: 1.00 8.22s, epoch [1003\\200]: train D loss: 0.0152, train F loss: 0.610289, acc 0.844000\n",
      "D_acc: 1.00 8.12s, epoch [1004\\200]: train D loss: 0.0081, train F loss: 0.476415, acc 0.863000\n",
      "D_acc: 1.00 8.21s, epoch [1005\\200]: train D loss: 0.0077, train F loss: 0.406337, acc 0.877200\n",
      "D_acc: 1.00 8.13s, epoch [1006\\200]: train D loss: 0.0084, train F loss: 0.361707, acc 0.887700\n",
      "D_acc: 1.00 8.24s, epoch [1007\\200]: train D loss: 0.0059, train F loss: 0.325115, acc 0.898900\n",
      "D_acc: 1.00 8.09s, epoch [1008\\200]: train D loss: 0.0047, train F loss: 0.292194, acc 0.906400\n",
      "D_acc: 1.00 8.24s, epoch [1009\\200]: train D loss: 0.0021, train F loss: 0.262676, acc 0.916700\n",
      "D_acc: 1.00 8.32s, epoch [1010\\200]: train D loss: 0.0008, train F loss: 0.238063, acc 0.923800\n",
      "D_acc: 1.00 8.18s, epoch [1011\\200]: train D loss: 0.0006, train F loss: 0.216107, acc 0.930300\n",
      "D_acc: 1.00 8.22s, epoch [1012\\200]: train D loss: 0.0003, train F loss: 0.196760, acc 0.936700\n",
      "D_acc: 1.00 8.34s, epoch [1013\\200]: train D loss: 0.0002, train F loss: 0.178939, acc 0.940800\n",
      "D_acc: 1.00 8.19s, epoch [1014\\200]: train D loss: 0.0002, train F loss: 0.161987, acc 0.945400\n",
      "D_acc: 1.00 8.42s, epoch [1015\\200]: train D loss: 0.0002, train F loss: 0.147035, acc 0.951600\n",
      "D_acc: 1.00 8.09s, epoch [1016\\200]: train D loss: 0.0001, train F loss: 0.134251, acc 0.956400\n",
      "D_acc: 1.00 8.10s, epoch [1017\\200]: train D loss: 0.0001, train F loss: 0.122449, acc 0.959500\n",
      "D_acc: 1.00 8.46s, epoch [1018\\200]: train D loss: 0.0001, train F loss: 0.111954, acc 0.963500\n",
      "D_acc: 1.00 8.29s, epoch [1019\\200]: train D loss: 0.0001, train F loss: 0.102436, acc 0.966500\n",
      "D_acc: 1.00 8.14s, epoch [1020\\200]: train D loss: 0.0001, train F loss: 0.093778, acc 0.968900\n",
      "D_acc: 1.00 8.30s, epoch [1021\\200]: train D loss: 0.0001, train F loss: 0.085852, acc 0.971600\n",
      "D_acc: 1.00 8.20s, epoch [1022\\200]: train D loss: 0.0001, train F loss: 0.078551, acc 0.972900\n",
      "D_acc: 1.00 8.29s, epoch [1023\\200]: train D loss: 0.0001, train F loss: 0.071627, acc 0.975000\n",
      "D_acc: 1.00 8.23s, epoch [1024\\200]: train D loss: 0.0001, train F loss: 0.064777, acc 0.979200\n",
      "D_acc: 1.00 8.27s, epoch [1025\\200]: train D loss: 0.0001, train F loss: 0.058721, acc 0.981300\n",
      "D_acc: 1.00 8.15s, epoch [1026\\200]: train D loss: 0.0001, train F loss: 0.053335, acc 0.984200\n",
      "D_acc: 1.00 8.22s, epoch [1027\\200]: train D loss: 0.0001, train F loss: 0.048840, acc 0.984000\n",
      "D_acc: 1.00 8.27s, epoch [1028\\200]: train D loss: 0.0001, train F loss: 0.044407, acc 0.985400\n",
      "D_acc: 1.00 8.29s, epoch [1029\\200]: train D loss: 0.0001, train F loss: 0.039269, acc 0.988000\n",
      "D_acc: 1.00 8.21s, epoch [1030\\200]: train D loss: 0.0000, train F loss: 0.034534, acc 0.990000\n",
      "D_acc: 1.00 8.40s, epoch [1031\\200]: train D loss: 0.0000, train F loss: 0.031021, acc 0.991700\n",
      "D_acc: 1.00 8.14s, epoch [1032\\200]: train D loss: 0.0000, train F loss: 0.026579, acc 0.992500\n",
      "D_acc: 1.00 8.34s, epoch [1033\\200]: train D loss: 0.0000, train F loss: 0.023230, acc 0.993200\n",
      "D_acc: 1.00 8.29s, epoch [1034\\200]: train D loss: 0.0000, train F loss: 0.020160, acc 0.994900\n",
      "D_acc: 1.00 8.46s, epoch [1035\\200]: train D loss: 0.0000, train F loss: 0.017996, acc 0.995600\n",
      "D_acc: 1.00 8.28s, epoch [1036\\200]: train D loss: 0.0000, train F loss: 0.015876, acc 0.995700\n",
      "D_acc: 1.00 8.27s, epoch [1037\\200]: train D loss: 0.0000, train F loss: 0.013580, acc 0.996600\n",
      "D_acc: 1.00 8.39s, epoch [1038\\200]: train D loss: 0.0000, train F loss: 0.012147, acc 0.997100\n",
      "D_acc: 1.00 8.27s, epoch [1039\\200]: train D loss: 0.0000, train F loss: 0.010928, acc 0.997300\n",
      "D_acc: 1.00 8.26s, epoch [1040\\200]: train D loss: 0.0000, train F loss: 0.009985, acc 0.997400\n",
      "D_acc: 1.00 8.11s, epoch [1041\\200]: train D loss: 0.0000, train F loss: 0.009317, acc 0.997300\n",
      "D_acc: 1.00 8.25s, epoch [1042\\200]: train D loss: 0.0000, train F loss: 0.008173, acc 0.997600\n",
      "D_acc: 1.00 8.11s, epoch [1043\\200]: train D loss: 0.0000, train F loss: 0.007353, acc 0.998000\n",
      "D_acc: 1.00 8.42s, epoch [1044\\200]: train D loss: 0.0000, train F loss: 0.006827, acc 0.998200\n",
      "D_acc: 1.00 8.34s, epoch [1045\\200]: train D loss: 0.0000, train F loss: 0.006148, acc 0.998200\n",
      "D_acc: 1.00 8.23s, epoch [1046\\200]: train D loss: 0.0000, train F loss: 0.005682, acc 0.998200\n",
      "D_acc: 1.00 8.25s, epoch [1047\\200]: train D loss: 0.0000, train F loss: 0.005321, acc 0.998400\n",
      "D_acc: 1.00 8.37s, epoch [1048\\200]: train D loss: 0.0000, train F loss: 0.004978, acc 0.998500\n",
      "D_acc: 1.00 8.30s, epoch [1049\\200]: train D loss: 0.0000, train F loss: 0.004909, acc 0.998500\n",
      "D_acc: 1.00 8.24s, epoch [1050\\200]: train D loss: 0.0000, train F loss: 0.004658, acc 0.998600\n",
      "D_acc: 1.00 8.28s, epoch [1051\\200]: train D loss: 0.0000, train F loss: 0.004786, acc 0.998700\n",
      "D_acc: 1.00 8.24s, epoch [1052\\200]: train D loss: 0.0000, train F loss: 0.004171, acc 0.998700\n",
      "D_acc: 1.00 8.28s, epoch [1053\\200]: train D loss: 0.0000, train F loss: 0.004114, acc 0.998700\n",
      "D_acc: 1.00 8.17s, epoch [1054\\200]: train D loss: 0.0000, train F loss: 0.003966, acc 0.998700\n",
      "D_acc: 1.00 8.23s, epoch [1055\\200]: train D loss: 0.0000, train F loss: 0.003866, acc 0.998700\n",
      "D_acc: 1.00 8.23s, epoch [1056\\200]: train D loss: 0.0000, train F loss: 0.003681, acc 0.998700\n",
      "D_acc: 1.00 8.32s, epoch [1057\\200]: train D loss: 0.0000, train F loss: 0.003642, acc 0.998600\n",
      "D_acc: 1.00 8.32s, epoch [1058\\200]: train D loss: 0.0000, train F loss: 0.003606, acc 0.998600\n",
      "D_acc: 1.00 8.29s, epoch [1059\\200]: train D loss: 0.0000, train F loss: 0.003750, acc 0.998700\n",
      "D_acc: 1.00 8.15s, epoch [1060\\200]: train D loss: 0.0000, train F loss: 0.003476, acc 0.998600\n",
      "D_acc: 1.00 8.30s, epoch [1061\\200]: train D loss: 0.0000, train F loss: 0.003255, acc 0.998700\n",
      "D_acc: 1.00 8.33s, epoch [1062\\200]: train D loss: 0.0000, train F loss: 0.003361, acc 0.998800\n",
      "D_acc: 1.00 8.33s, epoch [1063\\200]: train D loss: 0.0000, train F loss: 0.003661, acc 0.998700\n",
      "D_acc: 1.00 8.22s, epoch [1064\\200]: train D loss: 0.0000, train F loss: 0.003179, acc 0.998700\n",
      "D_acc: 1.00 8.21s, epoch [1065\\200]: train D loss: 0.0000, train F loss: 0.002994, acc 0.998800\n",
      "D_acc: 1.00 8.37s, epoch [1066\\200]: train D loss: 0.0000, train F loss: 0.002894, acc 0.998900\n",
      "D_acc: 1.00 8.32s, epoch [1067\\200]: train D loss: 0.0000, train F loss: 0.002831, acc 0.998900\n",
      "D_acc: 1.00 8.50s, epoch [1068\\200]: train D loss: 0.0000, train F loss: 0.002945, acc 0.999000\n",
      "D_acc: 1.00 8.55s, epoch [1069\\200]: train D loss: 0.0000, train F loss: 0.002867, acc 0.998800\n",
      "D_acc: 1.00 8.53s, epoch [1070\\200]: train D loss: 0.0000, train F loss: 0.002823, acc 0.998800\n",
      "D_acc: 1.00 8.51s, epoch [1071\\200]: train D loss: 0.0000, train F loss: 0.002970, acc 0.998800\n",
      "D_acc: 1.00 8.38s, epoch [1072\\200]: train D loss: 0.0000, train F loss: 0.002817, acc 0.998800\n",
      "D_acc: 1.00 8.57s, epoch [1073\\200]: train D loss: 0.0000, train F loss: 0.002775, acc 0.998800\n",
      "D_acc: 1.00 8.88s, epoch [1074\\200]: train D loss: 0.0000, train F loss: 0.002711, acc 0.998900\n",
      "D_acc: 1.00 8.49s, epoch [1075\\200]: train D loss: 0.0000, train F loss: 0.002596, acc 0.999000\n",
      "D_acc: 1.00 8.02s, epoch [1076\\200]: train D loss: 0.0000, train F loss: 0.002799, acc 0.998800\n",
      "D_acc: 1.00 8.31s, epoch [1077\\200]: train D loss: 0.0000, train F loss: 0.002769, acc 0.998800\n",
      "D_acc: 1.00 8.38s, epoch [1078\\200]: train D loss: 0.0000, train F loss: 0.002758, acc 0.998700\n",
      "D_acc: 1.00 8.21s, epoch [1079\\200]: train D loss: 0.0000, train F loss: 0.002747, acc 0.998900\n",
      "D_acc: 1.00 8.25s, epoch [1080\\200]: train D loss: 0.0000, train F loss: 0.002606, acc 0.999000\n",
      "D_acc: 1.00 8.33s, epoch [1081\\200]: train D loss: 0.0000, train F loss: 0.002509, acc 0.999000\n",
      "D_acc: 1.00 8.42s, epoch [1082\\200]: train D loss: 0.0000, train F loss: 0.002616, acc 0.998900\n",
      "D_acc: 1.00 8.20s, epoch [1083\\200]: train D loss: 0.0000, train F loss: 0.002908, acc 0.998900\n",
      "D_acc: 1.00 8.25s, epoch [1084\\200]: train D loss: 0.0000, train F loss: 0.002489, acc 0.999100\n",
      "D_acc: 1.00 8.27s, epoch [1085\\200]: train D loss: 0.0000, train F loss: 0.002463, acc 0.999100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 1.00 8.20s, epoch [1086\\200]: train D loss: 0.0000, train F loss: 0.002540, acc 0.999100\n",
      "D_acc: 1.00 8.34s, epoch [1087\\200]: train D loss: 0.0000, train F loss: 0.002635, acc 0.999000\n",
      "D_acc: 1.00 8.22s, epoch [1088\\200]: train D loss: 0.0000, train F loss: 0.002431, acc 0.999100\n",
      "D_acc: 1.00 8.28s, epoch [1089\\200]: train D loss: 0.0000, train F loss: 0.002379, acc 0.999100\n",
      "D_acc: 1.00 8.30s, epoch [1090\\200]: train D loss: 0.0000, train F loss: 0.002647, acc 0.998900\n",
      "D_acc: 1.00 8.11s, epoch [1091\\200]: train D loss: 0.0000, train F loss: 0.002244, acc 0.999100\n",
      "D_acc: 1.00 8.23s, epoch [1092\\200]: train D loss: 0.0000, train F loss: 0.002450, acc 0.999000\n",
      "D_acc: 1.00 8.42s, epoch [1093\\200]: train D loss: 0.0000, train F loss: 0.002527, acc 0.999100\n",
      "D_acc: 1.00 8.40s, epoch [1094\\200]: train D loss: 0.0000, train F loss: 0.002160, acc 0.999100\n",
      "D_acc: 1.00 8.49s, epoch [1095\\200]: train D loss: 0.0000, train F loss: 0.002451, acc 0.999200\n",
      "D_acc: 1.00 8.32s, epoch [1096\\200]: train D loss: 0.0000, train F loss: 0.002406, acc 0.999200\n",
      "D_acc: 1.00 8.16s, epoch [1097\\200]: train D loss: 0.0000, train F loss: 0.002390, acc 0.999000\n",
      "D_acc: 1.00 8.27s, epoch [1098\\200]: train D loss: 0.0000, train F loss: 0.002320, acc 0.999100\n",
      "D_acc: 1.00 8.35s, epoch [1099\\200]: train D loss: 0.0000, train F loss: 0.002319, acc 0.999100\n",
      "D_acc: 1.00 8.16s, epoch [1100\\200]: train D loss: 0.0000, train F loss: 0.002299, acc 0.999100\n",
      "D_acc: 1.00 8.15s, epoch [1101\\200]: train D loss: 0.0000, train F loss: 0.002455, acc 0.999000\n",
      "D_acc: 1.00 8.16s, epoch [1102\\200]: train D loss: 0.0000, train F loss: 0.002264, acc 0.999100\n",
      "D_acc: 1.00 8.35s, epoch [1103\\200]: train D loss: 0.0000, train F loss: 0.002284, acc 0.999200\n",
      "D_acc: 1.00 8.75s, epoch [1104\\200]: train D loss: 0.0000, train F loss: 0.002430, acc 0.999000\n",
      "D_acc: 1.00 8.73s, epoch [1105\\200]: train D loss: 0.0000, train F loss: 0.002034, acc 0.999300\n",
      "D_acc: 1.00 8.19s, epoch [1106\\200]: train D loss: 0.0000, train F loss: 0.002022, acc 0.999200\n",
      "D_acc: 1.00 8.28s, epoch [1107\\200]: train D loss: 0.0000, train F loss: 0.002323, acc 0.999100\n",
      "D_acc: 1.00 8.17s, epoch [1108\\200]: train D loss: 0.0000, train F loss: 0.002732, acc 0.999000\n",
      "D_acc: 1.00 8.26s, epoch [1109\\200]: train D loss: 0.0000, train F loss: 0.002543, acc 0.999100\n",
      "D_acc: 1.00 8.35s, epoch [1110\\200]: train D loss: 0.0000, train F loss: 0.014081, acc 0.997800\n",
      "D_acc: 0.99 8.49s, epoch [1111\\200]: train D loss: 0.0474, train F loss: 0.273471, acc 0.942500\n",
      "D_acc: 0.99 8.41s, epoch [1112\\200]: train D loss: 0.0377, train F loss: 0.172948, acc 0.941700\n",
      "D_acc: 1.00 8.48s, epoch [1113\\200]: train D loss: 0.0233, train F loss: 0.056936, acc 0.979500\n",
      "D_acc: 1.00 8.48s, epoch [1114\\200]: train D loss: 0.0207, train F loss: 0.021104, acc 0.993600\n",
      "D_acc: 1.00 8.34s, epoch [1115\\200]: train D loss: 0.0227, train F loss: 0.009345, acc 0.997400\n",
      "D_acc: 0.99 8.53s, epoch [1116\\200]: train D loss: 0.0242, train F loss: 0.005579, acc 0.998100\n",
      "D_acc: 1.00 8.38s, epoch [1117\\200]: train D loss: 0.0195, train F loss: 0.004276, acc 0.998500\n",
      "D_acc: 1.00 8.35s, epoch [1118\\200]: train D loss: 0.0134, train F loss: 0.003255, acc 0.999100\n",
      "D_acc: 1.00 8.48s, epoch [1119\\200]: train D loss: 0.0187, train F loss: 0.002508, acc 0.999100\n",
      "D_acc: 1.00 8.48s, epoch [1120\\200]: train D loss: 0.0171, train F loss: 0.002395, acc 0.999200\n",
      "D_acc: 0.99 8.50s, epoch [1121\\200]: train D loss: 0.0308, train F loss: 0.001327, acc 0.999100\n",
      "D_acc: 0.98 8.40s, epoch [1122\\200]: train D loss: 0.0589, train F loss: -0.001036, acc 0.999000\n",
      "D_acc: 0.96 8.39s, epoch [1123\\200]: train D loss: 0.1187, train F loss: -0.005852, acc 0.999000\n",
      "D_acc: 0.94 8.27s, epoch [1124\\200]: train D loss: 0.1861, train F loss: -0.010325, acc 0.998200\n",
      "D_acc: 0.96 8.22s, epoch [1125\\200]: train D loss: 0.1316, train F loss: -0.002039, acc 0.997700\n",
      "D_acc: 0.95 8.24s, epoch [1126\\200]: train D loss: 0.1644, train F loss: 0.006567, acc 0.994600\n",
      "D_acc: 0.96 8.43s, epoch [1127\\200]: train D loss: 0.1272, train F loss: 0.006271, acc 0.995600\n",
      "D_acc: 0.96 8.34s, epoch [1128\\200]: train D loss: 0.1226, train F loss: -0.000520, acc 0.997300\n",
      "D_acc: 0.97 8.15s, epoch [1129\\200]: train D loss: 0.1102, train F loss: 0.000623, acc 0.998000\n",
      "D_acc: 0.97 8.15s, epoch [1130\\200]: train D loss: 0.1150, train F loss: -0.003504, acc 0.998300\n",
      "D_acc: 0.98 8.22s, epoch [1131\\200]: train D loss: 0.0909, train F loss: -0.002823, acc 0.998600\n",
      "D_acc: 0.98 8.43s, epoch [1132\\200]: train D loss: 0.0872, train F loss: -0.003140, acc 0.999000\n",
      "D_acc: 0.98 8.48s, epoch [1133\\200]: train D loss: 0.0861, train F loss: -0.003142, acc 0.999000\n",
      "D_acc: 0.98 8.42s, epoch [1134\\200]: train D loss: 0.0932, train F loss: -0.003902, acc 0.999000\n",
      "D_acc: 0.98 8.43s, epoch [1135\\200]: train D loss: 0.0820, train F loss: -0.002901, acc 0.999000\n",
      "D_acc: 0.98 8.37s, epoch [1136\\200]: train D loss: 0.0947, train F loss: -0.003896, acc 0.998900\n",
      "D_acc: 0.98 8.38s, epoch [1137\\200]: train D loss: 0.0744, train F loss: -0.002562, acc 0.998900\n",
      "D_acc: 0.99 8.31s, epoch [1138\\200]: train D loss: 0.0671, train F loss: -0.001842, acc 0.999000\n",
      "D_acc: 0.99 8.25s, epoch [1139\\200]: train D loss: 0.0683, train F loss: -0.001941, acc 0.999000\n",
      "D_acc: 0.98 8.18s, epoch [1140\\200]: train D loss: 0.0808, train F loss: -0.002820, acc 0.998900\n",
      "D_acc: 0.98 8.42s, epoch [1141\\200]: train D loss: 0.0744, train F loss: -0.002145, acc 0.999100\n",
      "D_acc: 0.99 8.48s, epoch [1142\\200]: train D loss: 0.0632, train F loss: -0.001342, acc 0.999000\n",
      "D_acc: 0.99 8.36s, epoch [1143\\200]: train D loss: 0.0611, train F loss: -0.001245, acc 0.999000\n",
      "D_acc: 0.99 8.78s, epoch [1144\\200]: train D loss: 0.0622, train F loss: -0.000945, acc 0.998900\n",
      "D_acc: 0.98 8.56s, epoch [1145\\200]: train D loss: 0.0809, train F loss: -0.002576, acc 0.999000\n",
      "D_acc: 0.99 8.86s, epoch [1146\\200]: train D loss: 0.0639, train F loss: -0.001561, acc 0.998900\n",
      "D_acc: 0.99 8.37s, epoch [1147\\200]: train D loss: 0.0601, train F loss: -0.000850, acc 0.999000\n",
      "D_acc: 0.98 8.57s, epoch [1148\\200]: train D loss: 0.0831, train F loss: -0.002752, acc 0.999000\n",
      "D_acc: 0.99 8.22s, epoch [1149\\200]: train D loss: 0.0781, train F loss: -0.002454, acc 0.998900\n",
      "D_acc: 0.99 8.63s, epoch [1150\\200]: train D loss: 0.0729, train F loss: -0.002014, acc 0.999000\n",
      "D_acc: 0.99 8.57s, epoch [1151\\200]: train D loss: 0.0767, train F loss: -0.002295, acc 0.998900\n",
      "D_acc: 0.98 8.26s, epoch [1152\\200]: train D loss: 0.0881, train F loss: -0.002867, acc 0.998900\n",
      "D_acc: 0.98 8.19s, epoch [1153\\200]: train D loss: 0.0931, train F loss: -0.003370, acc 0.999100\n",
      "D_acc: 0.98 8.20s, epoch [1154\\200]: train D loss: 0.0933, train F loss: -0.003484, acc 0.999000\n",
      "D_acc: 0.97 8.58s, epoch [1155\\200]: train D loss: 0.1234, train F loss: -0.005880, acc 0.998900\n",
      "D_acc: 0.97 8.32s, epoch [1156\\200]: train D loss: 0.1185, train F loss: -0.005372, acc 0.999000\n",
      "D_acc: 0.97 8.43s, epoch [1157\\200]: train D loss: 0.1246, train F loss: -0.005136, acc 0.998900\n",
      "D_acc: 0.97 8.22s, epoch [1158\\200]: train D loss: 0.1368, train F loss: -0.004850, acc 0.998900\n",
      "D_acc: 0.96 8.31s, epoch [1159\\200]: train D loss: 0.1470, train F loss: 0.016595, acc 0.992800\n",
      "D_acc: 0.96 8.37s, epoch [1160\\200]: train D loss: 0.1479, train F loss: 0.075305, acc 0.978600\n",
      "D_acc: 0.96 8.35s, epoch [1161\\200]: train D loss: 0.1276, train F loss: 0.153057, acc 0.955500\n",
      "D_acc: 0.98 8.41s, epoch [1162\\200]: train D loss: 0.0795, train F loss: 0.034245, acc 0.987500\n",
      "D_acc: 0.99 8.60s, epoch [1163\\200]: train D loss: 0.0529, train F loss: 0.006119, acc 0.996900\n",
      "D_acc: 0.99 8.11s, epoch [1164\\200]: train D loss: 0.0477, train F loss: 0.001780, acc 0.998700\n",
      "D_acc: 0.99 8.10s, epoch [1165\\200]: train D loss: 0.0569, train F loss: 0.000163, acc 0.998800\n",
      "D_acc: 0.99 8.48s, epoch [1166\\200]: train D loss: 0.0800, train F loss: -0.001630, acc 0.998900\n",
      "D_acc: 0.98 8.36s, epoch [1167\\200]: train D loss: 0.0774, train F loss: -0.002273, acc 0.999200\n",
      "D_acc: 0.99 8.30s, epoch [1168\\200]: train D loss: 0.0601, train F loss: -0.000989, acc 0.999100\n",
      "D_acc: 0.99 8.27s, epoch [1169\\200]: train D loss: 0.0629, train F loss: -0.000756, acc 0.999100\n",
      "D_acc: 0.99 8.35s, epoch [1170\\200]: train D loss: 0.0657, train F loss: -0.001129, acc 0.999000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.99 8.29s, epoch [1171\\200]: train D loss: 0.0659, train F loss: -0.001559, acc 0.999100\n",
      "D_acc: 0.99 8.19s, epoch [1172\\200]: train D loss: 0.0528, train F loss: -0.000597, acc 0.999100\n",
      "D_acc: 0.99 8.27s, epoch [1173\\200]: train D loss: 0.0454, train F loss: -0.000133, acc 0.999100\n",
      "D_acc: 0.99 8.21s, epoch [1174\\200]: train D loss: 0.0457, train F loss: -0.000200, acc 0.999100\n",
      "D_acc: 1.00 8.27s, epoch [1175\\200]: train D loss: 0.0438, train F loss: 0.001466, acc 0.998300\n",
      "D_acc: 0.99 8.78s, epoch [1176\\200]: train D loss: 0.0534, train F loss: 0.002799, acc 0.998500\n",
      "D_acc: 0.99 8.18s, epoch [1177\\200]: train D loss: 0.0508, train F loss: 0.000331, acc 0.998900\n",
      "D_acc: 0.99 8.28s, epoch [1178\\200]: train D loss: 0.0475, train F loss: 0.000278, acc 0.999000\n",
      "D_acc: 1.00 8.25s, epoch [1179\\200]: train D loss: 0.0360, train F loss: 0.000619, acc 0.998800\n",
      "D_acc: 1.00 8.21s, epoch [1180\\200]: train D loss: 0.0375, train F loss: 0.000658, acc 0.999000\n",
      "D_acc: 1.00 8.33s, epoch [1181\\200]: train D loss: 0.0367, train F loss: 0.000486, acc 0.999000\n",
      "D_acc: 0.99 8.29s, epoch [1182\\200]: train D loss: 0.0477, train F loss: 0.000200, acc 0.998800\n",
      "D_acc: 1.00 8.13s, epoch [1183\\200]: train D loss: 0.0381, train F loss: 0.000450, acc 0.999200\n",
      "D_acc: 1.00 8.20s, epoch [1184\\200]: train D loss: 0.0386, train F loss: 0.000468, acc 0.999100\n",
      "D_acc: 0.99 8.22s, epoch [1185\\200]: train D loss: 0.0513, train F loss: -0.000530, acc 0.999200\n",
      "D_acc: 0.99 8.27s, epoch [1186\\200]: train D loss: 0.0484, train F loss: -0.000105, acc 0.999000\n",
      "D_acc: 0.99 8.39s, epoch [1187\\200]: train D loss: 0.0480, train F loss: -0.000322, acc 0.999100\n",
      "D_acc: 0.99 8.37s, epoch [1188\\200]: train D loss: 0.0481, train F loss: -0.000126, acc 0.999100\n",
      "D_acc: 0.99 8.21s, epoch [1189\\200]: train D loss: 0.0638, train F loss: -0.001295, acc 0.999200\n",
      "D_acc: 0.99 8.25s, epoch [1190\\200]: train D loss: 0.0677, train F loss: -0.001295, acc 0.999000\n",
      "D_acc: 0.99 8.31s, epoch [1191\\200]: train D loss: 0.0740, train F loss: -0.002117, acc 0.999100\n",
      "D_acc: 0.98 8.28s, epoch [1192\\200]: train D loss: 0.0834, train F loss: -0.001877, acc 0.998900\n",
      "D_acc: 0.98 8.28s, epoch [1193\\200]: train D loss: 0.0978, train F loss: -0.002073, acc 0.998600\n",
      "D_acc: 0.98 8.14s, epoch [1194\\200]: train D loss: 0.1088, train F loss: -0.003679, acc 0.998800\n",
      "D_acc: 0.97 8.15s, epoch [1195\\200]: train D loss: 0.1408, train F loss: -0.007034, acc 0.998900\n",
      "D_acc: 0.96 8.18s, epoch [1196\\200]: train D loss: 0.1638, train F loss: 0.016948, acc 0.992200\n",
      "D_acc: 0.94 8.27s, epoch [1197\\200]: train D loss: 0.1843, train F loss: 0.027326, acc 0.986000\n",
      "D_acc: 0.96 8.18s, epoch [1198\\200]: train D loss: 0.1444, train F loss: 0.011888, acc 0.992800\n",
      "D_acc: 0.96 8.26s, epoch [1199\\200]: train D loss: 0.1371, train F loss: 0.009512, acc 0.994400\n",
      "D_acc: 0.97 8.22s, epoch [1200\\200]: train D loss: 0.1382, train F loss: 0.002104, acc 0.996200\n",
      "add : 5000 new source size: 10000\n",
      "D_acc: 0.95 8.16s, epoch [1201\\200]: train D loss: 0.1713, train F loss: 0.351387, acc 0.914700\n",
      "D_acc: 0.96 8.13s, epoch [1202\\200]: train D loss: 0.1422, train F loss: 0.118517, acc 0.958000\n",
      "D_acc: 0.97 8.58s, epoch [1203\\200]: train D loss: 0.1101, train F loss: 0.047328, acc 0.981500\n",
      "D_acc: 0.98 8.57s, epoch [1204\\200]: train D loss: 0.1074, train F loss: 0.020684, acc 0.991400\n",
      "D_acc: 0.98 8.19s, epoch [1205\\200]: train D loss: 0.0985, train F loss: 0.011202, acc 0.994900\n",
      "D_acc: 0.98 8.14s, epoch [1206\\200]: train D loss: 0.1016, train F loss: 0.006491, acc 0.996500\n",
      "D_acc: 0.98 8.12s, epoch [1207\\200]: train D loss: 0.0902, train F loss: 0.004324, acc 0.997000\n",
      "D_acc: 0.98 8.11s, epoch [1208\\200]: train D loss: 0.0943, train F loss: -0.000124, acc 0.998300\n",
      "D_acc: 0.98 8.17s, epoch [1209\\200]: train D loss: 0.0907, train F loss: -0.000316, acc 0.998500\n",
      "D_acc: 0.98 8.13s, epoch [1210\\200]: train D loss: 0.1045, train F loss: -0.002673, acc 0.998600\n",
      "D_acc: 0.98 8.15s, epoch [1211\\200]: train D loss: 0.0983, train F loss: -0.003282, acc 0.999000\n",
      "D_acc: 0.98 8.24s, epoch [1212\\200]: train D loss: 0.0956, train F loss: -0.003005, acc 0.998800\n",
      "D_acc: 0.98 8.21s, epoch [1213\\200]: train D loss: 0.0998, train F loss: -0.003453, acc 0.998800\n",
      "D_acc: 0.98 8.18s, epoch [1214\\200]: train D loss: 0.0979, train F loss: -0.002963, acc 0.998900\n",
      "D_acc: 0.98 8.11s, epoch [1215\\200]: train D loss: 0.0945, train F loss: -0.002740, acc 0.998700\n",
      "D_acc: 0.98 8.18s, epoch [1216\\200]: train D loss: 0.0887, train F loss: -0.002193, acc 0.998900\n",
      "D_acc: 0.98 8.14s, epoch [1217\\200]: train D loss: 0.1085, train F loss: -0.003306, acc 0.998800\n",
      "D_acc: 0.97 8.14s, epoch [1218\\200]: train D loss: 0.1290, train F loss: -0.005664, acc 0.998800\n",
      "D_acc: 0.98 8.17s, epoch [1219\\200]: train D loss: 0.1197, train F loss: -0.004811, acc 0.998800\n",
      "D_acc: 0.97 8.20s, epoch [1220\\200]: train D loss: 0.1469, train F loss: -0.006678, acc 0.999000\n",
      "D_acc: 0.96 8.23s, epoch [1221\\200]: train D loss: 0.1647, train F loss: -0.008518, acc 0.998900\n",
      "D_acc: 0.95 8.50s, epoch [1222\\200]: train D loss: 0.1787, train F loss: -0.010182, acc 0.998700\n",
      "D_acc: 0.95 8.43s, epoch [1223\\200]: train D loss: 0.1895, train F loss: -0.001833, acc 0.996700\n",
      "D_acc: 0.94 8.12s, epoch [1224\\200]: train D loss: 0.1981, train F loss: 0.013396, acc 0.990600\n",
      "D_acc: 0.95 8.22s, epoch [1225\\200]: train D loss: 0.1727, train F loss: 0.003513, acc 0.994600\n",
      "D_acc: 0.96 8.11s, epoch [1226\\200]: train D loss: 0.1550, train F loss: -0.005164, acc 0.997400\n",
      "D_acc: 0.96 8.51s, epoch [1227\\200]: train D loss: 0.1496, train F loss: -0.006643, acc 0.998400\n",
      "D_acc: 0.96 9.09s, epoch [1228\\200]: train D loss: 0.1542, train F loss: -0.007238, acc 0.998700\n",
      "D_acc: 0.96 8.34s, epoch [1229\\200]: train D loss: 0.1644, train F loss: -0.008479, acc 0.998800\n",
      "D_acc: 0.95 8.24s, epoch [1230\\200]: train D loss: 0.1850, train F loss: -0.009888, acc 0.998500\n",
      "D_acc: 0.94 8.84s, epoch [1231\\200]: train D loss: 0.2133, train F loss: -0.012385, acc 0.998300\n",
      "D_acc: 0.95 8.09s, epoch [1232\\200]: train D loss: 0.1870, train F loss: -0.010002, acc 0.998500\n",
      "D_acc: 0.94 8.19s, epoch [1233\\200]: train D loss: 0.2063, train F loss: -0.011957, acc 0.998800\n",
      "D_acc: 0.94 8.15s, epoch [1234\\200]: train D loss: 0.2121, train F loss: -0.013432, acc 0.998900\n",
      "D_acc: 0.94 8.13s, epoch [1235\\200]: train D loss: 0.2125, train F loss: -0.013148, acc 0.998800\n",
      "D_acc: 0.94 8.37s, epoch [1236\\200]: train D loss: 0.2140, train F loss: -0.013526, acc 0.999000\n",
      "D_acc: 0.94 8.55s, epoch [1237\\200]: train D loss: 0.2188, train F loss: -0.013916, acc 0.998800\n",
      "D_acc: 0.93 8.13s, epoch [1238\\200]: train D loss: 0.2317, train F loss: -0.015029, acc 0.998900\n",
      "D_acc: 0.93 8.30s, epoch [1239\\200]: train D loss: 0.2332, train F loss: -0.015207, acc 0.999000\n",
      "D_acc: 0.92 8.20s, epoch [1240\\200]: train D loss: 0.2482, train F loss: -0.008222, acc 0.996300\n",
      "D_acc: 0.91 8.28s, epoch [1241\\200]: train D loss: 0.2743, train F loss: 0.011615, acc 0.989200\n",
      "D_acc: 0.92 8.23s, epoch [1242\\200]: train D loss: 0.2557, train F loss: 0.021959, acc 0.987800\n",
      "D_acc: 0.92 8.37s, epoch [1243\\200]: train D loss: 0.2315, train F loss: -0.001651, acc 0.994800\n",
      "D_acc: 0.94 8.27s, epoch [1244\\200]: train D loss: 0.2051, train F loss: -0.009132, acc 0.997300\n",
      "D_acc: 0.95 8.20s, epoch [1245\\200]: train D loss: 0.1844, train F loss: -0.009436, acc 0.998300\n",
      "D_acc: 0.96 8.22s, epoch [1246\\200]: train D loss: 0.1682, train F loss: -0.008299, acc 0.998700\n",
      "D_acc: 0.96 8.16s, epoch [1247\\200]: train D loss: 0.1659, train F loss: -0.008664, acc 0.998900\n",
      "D_acc: 0.95 8.24s, epoch [1248\\200]: train D loss: 0.1856, train F loss: -0.010359, acc 0.999000\n",
      "D_acc: 0.94 8.16s, epoch [1249\\200]: train D loss: 0.2038, train F loss: -0.012230, acc 0.998900\n",
      "D_acc: 0.94 8.28s, epoch [1250\\200]: train D loss: 0.2123, train F loss: -0.013440, acc 0.998800\n",
      "D_acc: 0.94 8.29s, epoch [1251\\200]: train D loss: 0.2130, train F loss: -0.013561, acc 0.998800\n",
      "D_acc: 0.93 8.26s, epoch [1252\\200]: train D loss: 0.2281, train F loss: -0.014524, acc 0.998800\n",
      "D_acc: 0.93 8.21s, epoch [1253\\200]: train D loss: 0.2382, train F loss: -0.015332, acc 0.998500\n",
      "D_acc: 0.93 8.20s, epoch [1254\\200]: train D loss: 0.2299, train F loss: -0.013042, acc 0.998000\n",
      "D_acc: 0.93 8.18s, epoch [1255\\200]: train D loss: 0.2366, train F loss: -0.003473, acc 0.995900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.93 8.12s, epoch [1256\\200]: train D loss: 0.2409, train F loss: -0.009504, acc 0.996500\n",
      "D_acc: 0.93 8.31s, epoch [1257\\200]: train D loss: 0.2216, train F loss: -0.004197, acc 0.995500\n",
      "D_acc: 0.93 8.38s, epoch [1258\\200]: train D loss: 0.2282, train F loss: -0.007798, acc 0.996200\n",
      "D_acc: 0.93 8.21s, epoch [1259\\200]: train D loss: 0.2161, train F loss: -0.010176, acc 0.997300\n",
      "D_acc: 0.94 8.18s, epoch [1260\\200]: train D loss: 0.2036, train F loss: -0.007316, acc 0.997200\n",
      "D_acc: 0.94 8.30s, epoch [1261\\200]: train D loss: 0.2182, train F loss: -0.009542, acc 0.998000\n",
      "D_acc: 0.93 8.35s, epoch [1262\\200]: train D loss: 0.2176, train F loss: -0.012334, acc 0.998400\n",
      "D_acc: 0.94 8.17s, epoch [1263\\200]: train D loss: 0.2066, train F loss: -0.012291, acc 0.998600\n",
      "D_acc: 0.94 8.45s, epoch [1264\\200]: train D loss: 0.2053, train F loss: -0.012370, acc 0.998700\n",
      "D_acc: 0.94 8.26s, epoch [1265\\200]: train D loss: 0.2181, train F loss: -0.013512, acc 0.999000\n",
      "D_acc: 0.93 8.35s, epoch [1266\\200]: train D loss: 0.2327, train F loss: -0.014996, acc 0.999000\n",
      "D_acc: 0.93 8.18s, epoch [1267\\200]: train D loss: 0.2390, train F loss: -0.016258, acc 0.998800\n",
      "D_acc: 0.92 8.19s, epoch [1268\\200]: train D loss: 0.2530, train F loss: -0.017254, acc 0.998700\n",
      "D_acc: 0.92 8.20s, epoch [1269\\200]: train D loss: 0.2551, train F loss: -0.017486, acc 0.998800\n",
      "D_acc: 0.92 8.11s, epoch [1270\\200]: train D loss: 0.2620, train F loss: -0.017934, acc 0.998800\n",
      "D_acc: 0.91 8.34s, epoch [1271\\200]: train D loss: 0.2803, train F loss: -0.018631, acc 0.998400\n",
      "D_acc: 0.90 8.70s, epoch [1272\\200]: train D loss: 0.2812, train F loss: -0.009918, acc 0.995800\n",
      "D_acc: 0.91 8.68s, epoch [1273\\200]: train D loss: 0.2740, train F loss: -0.007223, acc 0.994800\n",
      "D_acc: 0.91 8.18s, epoch [1274\\200]: train D loss: 0.2640, train F loss: -0.010797, acc 0.996200\n",
      "D_acc: 0.92 8.27s, epoch [1275\\200]: train D loss: 0.2436, train F loss: -0.013822, acc 0.997900\n",
      "D_acc: 0.93 8.31s, epoch [1276\\200]: train D loss: 0.2343, train F loss: -0.013968, acc 0.998100\n",
      "D_acc: 0.93 8.06s, epoch [1277\\200]: train D loss: 0.2372, train F loss: -0.015187, acc 0.998600\n",
      "D_acc: 0.93 8.29s, epoch [1278\\200]: train D loss: 0.2393, train F loss: -0.014779, acc 0.998500\n",
      "D_acc: 0.93 8.30s, epoch [1279\\200]: train D loss: 0.2405, train F loss: -0.015411, acc 0.998600\n",
      "D_acc: 0.93 8.24s, epoch [1280\\200]: train D loss: 0.2357, train F loss: -0.013785, acc 0.998100\n",
      "D_acc: 0.93 8.23s, epoch [1281\\200]: train D loss: 0.2393, train F loss: -0.010439, acc 0.997200\n",
      "D_acc: 0.92 8.26s, epoch [1282\\200]: train D loss: 0.2555, train F loss: -0.010546, acc 0.996600\n",
      "D_acc: 0.92 8.24s, epoch [1283\\200]: train D loss: 0.2430, train F loss: -0.012885, acc 0.997800\n",
      "D_acc: 0.93 8.37s, epoch [1284\\200]: train D loss: 0.2370, train F loss: -0.013659, acc 0.998400\n",
      "D_acc: 0.93 8.20s, epoch [1285\\200]: train D loss: 0.2328, train F loss: -0.015278, acc 0.999000\n",
      "D_acc: 0.93 8.25s, epoch [1286\\200]: train D loss: 0.2365, train F loss: -0.015902, acc 0.999100\n",
      "D_acc: 0.93 8.23s, epoch [1287\\200]: train D loss: 0.2523, train F loss: -0.017027, acc 0.998700\n",
      "D_acc: 0.92 8.21s, epoch [1288\\200]: train D loss: 0.2610, train F loss: -0.018348, acc 0.999000\n",
      "D_acc: 0.91 8.19s, epoch [1289\\200]: train D loss: 0.2717, train F loss: -0.019528, acc 0.998800\n",
      "D_acc: 0.91 8.29s, epoch [1290\\200]: train D loss: 0.2802, train F loss: -0.020179, acc 0.999100\n",
      "D_acc: 0.90 8.29s, epoch [1291\\200]: train D loss: 0.2968, train F loss: -0.022035, acc 0.998900\n",
      "D_acc: 0.89 8.26s, epoch [1292\\200]: train D loss: 0.3077, train F loss: -0.023449, acc 0.998900\n",
      "D_acc: 0.89 8.43s, epoch [1293\\200]: train D loss: 0.3169, train F loss: -0.024713, acc 0.998900\n",
      "D_acc: 0.88 8.27s, epoch [1294\\200]: train D loss: 0.3296, train F loss: -0.025757, acc 0.999000\n",
      "D_acc: 0.88 8.63s, epoch [1295\\200]: train D loss: 0.3315, train F loss: -0.026113, acc 0.999000\n",
      "D_acc: 0.88 8.18s, epoch [1296\\200]: train D loss: 0.3279, train F loss: -0.025805, acc 0.998800\n",
      "D_acc: 0.87 8.30s, epoch [1297\\200]: train D loss: 0.3406, train F loss: -0.017310, acc 0.995900\n",
      "D_acc: 0.86 8.26s, epoch [1298\\200]: train D loss: 0.3509, train F loss: 0.009054, acc 0.989600\n",
      "D_acc: 0.87 8.24s, epoch [1299\\200]: train D loss: 0.3378, train F loss: -0.016180, acc 0.994400\n",
      "D_acc: 0.89 8.29s, epoch [1300\\200]: train D loss: 0.3030, train F loss: -0.023254, acc 0.999100\n",
      "D_acc: 0.91 8.17s, epoch [1301\\200]: train D loss: 0.2722, train F loss: -0.020705, acc 0.999000\n",
      "D_acc: 0.92 8.33s, epoch [1302\\200]: train D loss: 0.2589, train F loss: -0.018857, acc 0.999100\n",
      "D_acc: 0.92 8.14s, epoch [1303\\200]: train D loss: 0.2542, train F loss: -0.017968, acc 0.998800\n",
      "D_acc: 0.92 8.24s, epoch [1304\\200]: train D loss: 0.2599, train F loss: -0.018200, acc 0.999100\n",
      "D_acc: 0.91 8.12s, epoch [1305\\200]: train D loss: 0.2758, train F loss: -0.019538, acc 0.999000\n",
      "D_acc: 0.90 8.18s, epoch [1306\\200]: train D loss: 0.2877, train F loss: -0.020687, acc 0.998900\n",
      "D_acc: 0.90 8.27s, epoch [1307\\200]: train D loss: 0.2898, train F loss: -0.020859, acc 0.998700\n",
      "D_acc: 0.90 8.25s, epoch [1308\\200]: train D loss: 0.2919, train F loss: -0.021330, acc 0.998500\n",
      "D_acc: 0.90 8.38s, epoch [1309\\200]: train D loss: 0.2947, train F loss: -0.021659, acc 0.998800\n",
      "D_acc: 0.90 8.16s, epoch [1310\\200]: train D loss: 0.2973, train F loss: -0.021812, acc 0.999000\n",
      "D_acc: 0.90 8.25s, epoch [1311\\200]: train D loss: 0.2952, train F loss: -0.021767, acc 0.998800\n",
      "D_acc: 0.89 8.52s, epoch [1312\\200]: train D loss: 0.3108, train F loss: -0.022408, acc 0.998500\n",
      "D_acc: 0.87 8.22s, epoch [1313\\200]: train D loss: 0.3406, train F loss: -0.009569, acc 0.993900\n",
      "D_acc: 0.87 8.22s, epoch [1314\\200]: train D loss: 0.3395, train F loss: -0.004505, acc 0.993600\n",
      "D_acc: 0.87 8.25s, epoch [1315\\200]: train D loss: 0.3406, train F loss: -0.001651, acc 0.993100\n",
      "D_acc: 0.89 8.11s, epoch [1316\\200]: train D loss: 0.3020, train F loss: -0.016582, acc 0.996700\n",
      "D_acc: 0.91 8.11s, epoch [1317\\200]: train D loss: 0.2748, train F loss: -0.017839, acc 0.997900\n",
      "D_acc: 0.92 8.17s, epoch [1318\\200]: train D loss: 0.2516, train F loss: -0.017558, acc 0.999200\n",
      "D_acc: 0.93 8.25s, epoch [1319\\200]: train D loss: 0.2382, train F loss: -0.016037, acc 0.999100\n",
      "D_acc: 0.93 8.40s, epoch [1320\\200]: train D loss: 0.2345, train F loss: -0.016154, acc 0.999300\n",
      "D_acc: 0.93 8.42s, epoch [1321\\200]: train D loss: 0.2521, train F loss: -0.017171, acc 0.999000\n",
      "D_acc: 0.92 8.32s, epoch [1322\\200]: train D loss: 0.2507, train F loss: -0.017513, acc 0.999100\n",
      "D_acc: 0.92 8.18s, epoch [1323\\200]: train D loss: 0.2610, train F loss: -0.017714, acc 0.999000\n",
      "D_acc: 0.92 8.22s, epoch [1324\\200]: train D loss: 0.2673, train F loss: -0.018833, acc 0.999100\n",
      "D_acc: 0.91 8.25s, epoch [1325\\200]: train D loss: 0.2743, train F loss: -0.020068, acc 0.999200\n",
      "D_acc: 0.91 8.65s, epoch [1326\\200]: train D loss: 0.2829, train F loss: -0.020101, acc 0.998700\n",
      "D_acc: 0.91 8.38s, epoch [1327\\200]: train D loss: 0.2878, train F loss: -0.019132, acc 0.998400\n",
      "D_acc: 0.90 8.76s, epoch [1328\\200]: train D loss: 0.3001, train F loss: -0.016744, acc 0.997100\n",
      "D_acc: 0.90 8.18s, epoch [1329\\200]: train D loss: 0.2972, train F loss: -0.014591, acc 0.996900\n",
      "D_acc: 0.88 8.11s, epoch [1330\\200]: train D loss: 0.3169, train F loss: 0.035826, acc 0.987000\n",
      "D_acc: 0.90 8.19s, epoch [1331\\200]: train D loss: 0.2822, train F loss: -0.012619, acc 0.996200\n",
      "D_acc: 0.92 8.22s, epoch [1332\\200]: train D loss: 0.2529, train F loss: -0.012807, acc 0.997700\n",
      "D_acc: 0.93 8.25s, epoch [1333\\200]: train D loss: 0.2291, train F loss: -0.014725, acc 0.998900\n",
      "D_acc: 0.94 8.11s, epoch [1334\\200]: train D loss: 0.2086, train F loss: -0.013738, acc 0.999200\n",
      "D_acc: 0.94 8.20s, epoch [1335\\200]: train D loss: 0.2200, train F loss: -0.013951, acc 0.999200\n",
      "D_acc: 0.93 8.10s, epoch [1336\\200]: train D loss: 0.2489, train F loss: -0.015839, acc 0.998900\n",
      "D_acc: 0.93 8.24s, epoch [1337\\200]: train D loss: 0.2374, train F loss: -0.015508, acc 0.998900\n",
      "D_acc: 0.93 8.26s, epoch [1338\\200]: train D loss: 0.2353, train F loss: -0.016037, acc 0.999400\n",
      "D_acc: 0.93 8.15s, epoch [1339\\200]: train D loss: 0.2452, train F loss: -0.016561, acc 0.999000\n",
      "D_acc: 0.92 8.21s, epoch [1340\\200]: train D loss: 0.2657, train F loss: -0.012790, acc 0.996900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.91 8.30s, epoch [1341\\200]: train D loss: 0.2768, train F loss: -0.016997, acc 0.997700\n",
      "D_acc: 0.92 8.12s, epoch [1342\\200]: train D loss: 0.2516, train F loss: -0.017138, acc 0.998600\n",
      "D_acc: 0.93 8.22s, epoch [1343\\200]: train D loss: 0.2341, train F loss: -0.016169, acc 0.999000\n",
      "D_acc: 0.93 8.11s, epoch [1344\\200]: train D loss: 0.2487, train F loss: -0.016831, acc 0.999100\n",
      "D_acc: 0.92 8.12s, epoch [1345\\200]: train D loss: 0.2635, train F loss: -0.015605, acc 0.998200\n",
      "D_acc: 0.92 8.09s, epoch [1346\\200]: train D loss: 0.2519, train F loss: -0.015941, acc 0.998800\n",
      "D_acc: 0.93 8.17s, epoch [1347\\200]: train D loss: 0.2420, train F loss: -0.016820, acc 0.999300\n",
      "D_acc: 0.93 8.18s, epoch [1348\\200]: train D loss: 0.2437, train F loss: -0.014723, acc 0.998500\n",
      "D_acc: 0.92 8.18s, epoch [1349\\200]: train D loss: 0.2495, train F loss: -0.017138, acc 0.999200\n",
      "D_acc: 0.91 8.07s, epoch [1350\\200]: train D loss: 0.2780, train F loss: -0.019307, acc 0.999000\n",
      "D_acc: 0.91 8.17s, epoch [1351\\200]: train D loss: 0.2833, train F loss: -0.020812, acc 0.999400\n",
      "D_acc: 0.90 8.35s, epoch [1352\\200]: train D loss: 0.2937, train F loss: -0.022067, acc 0.999300\n",
      "D_acc: 0.90 8.25s, epoch [1353\\200]: train D loss: 0.3000, train F loss: -0.022608, acc 0.999500\n",
      "D_acc: 0.89 8.12s, epoch [1354\\200]: train D loss: 0.3058, train F loss: -0.023616, acc 0.999300\n",
      "D_acc: 0.89 8.15s, epoch [1355\\200]: train D loss: 0.3127, train F loss: -0.023792, acc 0.999300\n",
      "D_acc: 0.89 8.21s, epoch [1356\\200]: train D loss: 0.3180, train F loss: -0.024204, acc 0.999200\n",
      "D_acc: 0.88 8.36s, epoch [1357\\200]: train D loss: 0.3201, train F loss: -0.024517, acc 0.999200\n",
      "D_acc: 0.88 8.41s, epoch [1358\\200]: train D loss: 0.3215, train F loss: -0.022677, acc 0.998600\n",
      "D_acc: 0.88 8.19s, epoch [1359\\200]: train D loss: 0.3232, train F loss: -0.018883, acc 0.997000\n",
      "D_acc: 0.88 8.24s, epoch [1360\\200]: train D loss: 0.3284, train F loss: -0.023560, acc 0.998200\n",
      "D_acc: 0.88 8.33s, epoch [1361\\200]: train D loss: 0.3213, train F loss: -0.020959, acc 0.998000\n",
      "D_acc: 0.87 8.13s, epoch [1362\\200]: train D loss: 0.3293, train F loss: -0.013077, acc 0.995600\n",
      "D_acc: 0.87 8.19s, epoch [1363\\200]: train D loss: 0.3305, train F loss: -0.015165, acc 0.995700\n",
      "D_acc: 0.89 8.23s, epoch [1364\\200]: train D loss: 0.3067, train F loss: -0.021132, acc 0.998000\n",
      "D_acc: 0.90 8.18s, epoch [1365\\200]: train D loss: 0.2802, train F loss: -0.020230, acc 0.998600\n",
      "D_acc: 0.91 8.17s, epoch [1366\\200]: train D loss: 0.2648, train F loss: -0.016660, acc 0.998300\n",
      "D_acc: 0.92 8.21s, epoch [1367\\200]: train D loss: 0.2613, train F loss: -0.016676, acc 0.998500\n",
      "D_acc: 0.91 8.22s, epoch [1368\\200]: train D loss: 0.2722, train F loss: -0.012244, acc 0.997700\n",
      "D_acc: 0.91 8.09s, epoch [1369\\200]: train D loss: 0.2713, train F loss: -0.015077, acc 0.998100\n",
      "D_acc: 0.92 8.13s, epoch [1370\\200]: train D loss: 0.2558, train F loss: -0.015411, acc 0.998200\n",
      "D_acc: 0.92 8.17s, epoch [1371\\200]: train D loss: 0.2506, train F loss: -0.017762, acc 0.999200\n",
      "D_acc: 0.92 8.26s, epoch [1372\\200]: train D loss: 0.2529, train F loss: -0.016372, acc 0.999000\n",
      "D_acc: 0.92 8.12s, epoch [1373\\200]: train D loss: 0.2631, train F loss: -0.015474, acc 0.998500\n",
      "D_acc: 0.92 8.14s, epoch [1374\\200]: train D loss: 0.2662, train F loss: -0.015558, acc 0.998700\n",
      "D_acc: 0.92 8.52s, epoch [1375\\200]: train D loss: 0.2530, train F loss: -0.018111, acc 0.999400\n",
      "D_acc: 0.92 8.76s, epoch [1376\\200]: train D loss: 0.2614, train F loss: -0.018810, acc 0.999400\n",
      "D_acc: 0.91 8.77s, epoch [1377\\200]: train D loss: 0.2658, train F loss: -0.017512, acc 0.999000\n",
      "D_acc: 0.91 8.92s, epoch [1378\\200]: train D loss: 0.2659, train F loss: -0.016581, acc 0.998300\n",
      "D_acc: 0.91 8.87s, epoch [1379\\200]: train D loss: 0.2727, train F loss: -0.018200, acc 0.998900\n",
      "D_acc: 0.91 8.72s, epoch [1380\\200]: train D loss: 0.2626, train F loss: -0.017564, acc 0.999000\n",
      "D_acc: 0.92 9.02s, epoch [1381\\200]: train D loss: 0.2538, train F loss: -0.017574, acc 0.999300\n",
      "D_acc: 0.92 8.73s, epoch [1382\\200]: train D loss: 0.2668, train F loss: -0.017767, acc 0.998900\n",
      "D_acc: 0.91 8.25s, epoch [1383\\200]: train D loss: 0.2871, train F loss: -0.020650, acc 0.999300\n",
      "D_acc: 0.90 8.20s, epoch [1384\\200]: train D loss: 0.2882, train F loss: -0.021408, acc 0.999300\n",
      "D_acc: 0.91 8.28s, epoch [1385\\200]: train D loss: 0.2831, train F loss: -0.021049, acc 0.999200\n",
      "D_acc: 0.89 8.31s, epoch [1386\\200]: train D loss: 0.3096, train F loss: -0.022811, acc 0.999200\n",
      "D_acc: 0.89 8.25s, epoch [1387\\200]: train D loss: 0.3102, train F loss: -0.023427, acc 0.999300\n",
      "D_acc: 0.89 8.11s, epoch [1388\\200]: train D loss: 0.3034, train F loss: -0.022682, acc 0.999000\n",
      "D_acc: 0.89 8.19s, epoch [1389\\200]: train D loss: 0.3047, train F loss: -0.022747, acc 0.999100\n",
      "D_acc: 0.89 8.19s, epoch [1390\\200]: train D loss: 0.3061, train F loss: -0.023659, acc 0.999200\n",
      "D_acc: 0.89 8.16s, epoch [1391\\200]: train D loss: 0.3144, train F loss: -0.017804, acc 0.997900\n",
      "D_acc: 0.88 8.17s, epoch [1392\\200]: train D loss: 0.3325, train F loss: -0.013214, acc 0.995800\n",
      "D_acc: 0.87 8.29s, epoch [1393\\200]: train D loss: 0.3380, train F loss: -0.012469, acc 0.996800\n",
      "D_acc: 0.89 8.21s, epoch [1394\\200]: train D loss: 0.3047, train F loss: -0.014033, acc 0.996600\n",
      "D_acc: 0.90 8.20s, epoch [1395\\200]: train D loss: 0.2795, train F loss: -0.016792, acc 0.998300\n",
      "D_acc: 0.92 8.40s, epoch [1396\\200]: train D loss: 0.2599, train F loss: -0.016467, acc 0.998300\n",
      "D_acc: 0.92 8.86s, epoch [1397\\200]: train D loss: 0.2597, train F loss: -0.017220, acc 0.999000\n",
      "D_acc: 0.92 8.45s, epoch [1398\\200]: train D loss: 0.2563, train F loss: -0.018640, acc 0.999500\n",
      "D_acc: 0.91 8.30s, epoch [1399\\200]: train D loss: 0.2702, train F loss: -0.020107, acc 0.999800\n",
      "D_acc: 0.91 8.13s, epoch [1400\\200]: train D loss: 0.2790, train F loss: -0.020385, acc 0.999400\n",
      "add : 5000 new source size: 10000\n",
      "D_acc: 0.87 8.50s, epoch [1401\\200]: train D loss: 0.3593, train F loss: 0.424703, acc 0.908800\n",
      "D_acc: 0.88 8.18s, epoch [1402\\200]: train D loss: 0.3155, train F loss: 0.128495, acc 0.953700\n",
      "D_acc: 0.91 8.24s, epoch [1403\\200]: train D loss: 0.2655, train F loss: 0.037185, acc 0.982500\n",
      "D_acc: 0.93 8.39s, epoch [1404\\200]: train D loss: 0.2322, train F loss: 0.007920, acc 0.992900\n",
      "D_acc: 0.93 8.17s, epoch [1405\\200]: train D loss: 0.2164, train F loss: -0.001899, acc 0.996000\n",
      "D_acc: 0.94 8.12s, epoch [1406\\200]: train D loss: 0.2189, train F loss: -0.006624, acc 0.996900\n",
      "D_acc: 0.93 8.24s, epoch [1407\\200]: train D loss: 0.2256, train F loss: -0.008590, acc 0.998100\n",
      "D_acc: 0.93 8.11s, epoch [1408\\200]: train D loss: 0.2194, train F loss: -0.010748, acc 0.998200\n",
      "D_acc: 0.92 8.21s, epoch [1409\\200]: train D loss: 0.2408, train F loss: -0.013622, acc 0.998900\n",
      "D_acc: 0.93 8.34s, epoch [1410\\200]: train D loss: 0.2450, train F loss: -0.015749, acc 0.999000\n",
      "D_acc: 0.92 8.16s, epoch [1411\\200]: train D loss: 0.2562, train F loss: -0.015041, acc 0.998800\n",
      "D_acc: 0.92 8.18s, epoch [1412\\200]: train D loss: 0.2597, train F loss: 0.004043, acc 0.994400\n",
      "D_acc: 0.91 8.19s, epoch [1413\\200]: train D loss: 0.2706, train F loss: -0.009240, acc 0.997000\n",
      "D_acc: 0.92 8.16s, epoch [1414\\200]: train D loss: 0.2477, train F loss: -0.014140, acc 0.998300\n",
      "D_acc: 0.93 8.19s, epoch [1415\\200]: train D loss: 0.2339, train F loss: -0.014585, acc 0.998700\n",
      "D_acc: 0.93 8.24s, epoch [1416\\200]: train D loss: 0.2346, train F loss: -0.013454, acc 0.998600\n",
      "D_acc: 0.93 8.10s, epoch [1417\\200]: train D loss: 0.2389, train F loss: -0.013860, acc 0.998400\n",
      "D_acc: 0.91 8.32s, epoch [1418\\200]: train D loss: 0.2705, train F loss: -0.004714, acc 0.996900\n",
      "D_acc: 0.90 8.57s, epoch [1419\\200]: train D loss: 0.2874, train F loss: -0.009673, acc 0.995300\n",
      "D_acc: 0.92 8.41s, epoch [1420\\200]: train D loss: 0.2602, train F loss: -0.013352, acc 0.997500\n",
      "D_acc: 0.92 8.13s, epoch [1421\\200]: train D loss: 0.2529, train F loss: -0.013385, acc 0.998100\n",
      "D_acc: 0.92 8.13s, epoch [1422\\200]: train D loss: 0.2529, train F loss: -0.010917, acc 0.997800\n",
      "D_acc: 0.92 8.09s, epoch [1423\\200]: train D loss: 0.2540, train F loss: -0.015081, acc 0.998200\n",
      "D_acc: 0.92 8.72s, epoch [1424\\200]: train D loss: 0.2570, train F loss: -0.012044, acc 0.997000\n",
      "D_acc: 0.92 8.11s, epoch [1425\\200]: train D loss: 0.2516, train F loss: -0.015746, acc 0.998700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.92 8.12s, epoch [1426\\200]: train D loss: 0.2461, train F loss: -0.015597, acc 0.998900\n",
      "D_acc: 0.92 8.19s, epoch [1427\\200]: train D loss: 0.2494, train F loss: -0.015889, acc 0.998500\n",
      "D_acc: 0.92 8.65s, epoch [1428\\200]: train D loss: 0.2624, train F loss: -0.018096, acc 0.999300\n",
      "D_acc: 0.92 8.29s, epoch [1429\\200]: train D loss: 0.2567, train F loss: -0.017792, acc 0.999100\n",
      "D_acc: 0.92 8.27s, epoch [1430\\200]: train D loss: 0.2568, train F loss: -0.018709, acc 0.999600\n",
      "D_acc: 0.92 8.30s, epoch [1431\\200]: train D loss: 0.2614, train F loss: -0.018482, acc 0.999500\n",
      "D_acc: 0.92 8.30s, epoch [1432\\200]: train D loss: 0.2648, train F loss: -0.015784, acc 0.998500\n",
      "D_acc: 0.91 8.20s, epoch [1433\\200]: train D loss: 0.2703, train F loss: -0.012066, acc 0.997800\n",
      "D_acc: 0.91 8.13s, epoch [1434\\200]: train D loss: 0.2740, train F loss: -0.016159, acc 0.998300\n",
      "D_acc: 0.91 8.18s, epoch [1435\\200]: train D loss: 0.2638, train F loss: -0.017920, acc 0.998600\n",
      "D_acc: 0.92 8.19s, epoch [1436\\200]: train D loss: 0.2574, train F loss: -0.019317, acc 0.999600\n",
      "D_acc: 0.92 8.25s, epoch [1437\\200]: train D loss: 0.2587, train F loss: -0.019363, acc 0.999600\n",
      "D_acc: 0.90 8.14s, epoch [1438\\200]: train D loss: 0.2832, train F loss: -0.020836, acc 0.999300\n",
      "D_acc: 0.90 8.21s, epoch [1439\\200]: train D loss: 0.2925, train F loss: -0.022307, acc 0.999600\n",
      "D_acc: 0.90 8.18s, epoch [1440\\200]: train D loss: 0.2958, train F loss: -0.022450, acc 0.999200\n",
      "D_acc: 0.90 8.19s, epoch [1441\\200]: train D loss: 0.3014, train F loss: -0.020169, acc 0.998600\n",
      "D_acc: 0.90 8.19s, epoch [1442\\200]: train D loss: 0.2919, train F loss: -0.019916, acc 0.998600\n",
      "D_acc: 0.90 8.25s, epoch [1443\\200]: train D loss: 0.2995, train F loss: -0.016697, acc 0.997700\n",
      "D_acc: 0.89 8.28s, epoch [1444\\200]: train D loss: 0.3113, train F loss: -0.011215, acc 0.995100\n",
      "D_acc: 0.89 8.13s, epoch [1445\\200]: train D loss: 0.3032, train F loss: -0.012035, acc 0.996100\n",
      "D_acc: 0.90 8.18s, epoch [1446\\200]: train D loss: 0.2938, train F loss: -0.016078, acc 0.997300\n",
      "D_acc: 0.90 8.09s, epoch [1447\\200]: train D loss: 0.2804, train F loss: -0.015006, acc 0.998300\n",
      "D_acc: 0.91 8.16s, epoch [1448\\200]: train D loss: 0.2785, train F loss: -0.019518, acc 0.998800\n",
      "D_acc: 0.91 8.18s, epoch [1449\\200]: train D loss: 0.2737, train F loss: -0.020917, acc 0.999400\n",
      "D_acc: 0.91 8.19s, epoch [1450\\200]: train D loss: 0.2799, train F loss: -0.020408, acc 0.999000\n",
      "D_acc: 0.91 8.13s, epoch [1451\\200]: train D loss: 0.2787, train F loss: -0.016205, acc 0.998500\n",
      "D_acc: 0.91 8.09s, epoch [1452\\200]: train D loss: 0.2862, train F loss: -0.015029, acc 0.997400\n",
      "D_acc: 0.91 8.22s, epoch [1453\\200]: train D loss: 0.2775, train F loss: -0.015653, acc 0.998100\n",
      "D_acc: 0.91 8.20s, epoch [1454\\200]: train D loss: 0.2766, train F loss: -0.013430, acc 0.997300\n",
      "D_acc: 0.91 8.64s, epoch [1455\\200]: train D loss: 0.2761, train F loss: -0.018588, acc 0.998800\n",
      "D_acc: 0.91 8.10s, epoch [1456\\200]: train D loss: 0.2770, train F loss: -0.020268, acc 0.999100\n",
      "D_acc: 0.90 8.25s, epoch [1457\\200]: train D loss: 0.2877, train F loss: -0.020004, acc 0.998800\n",
      "D_acc: 0.90 8.27s, epoch [1458\\200]: train D loss: 0.2858, train F loss: -0.020995, acc 0.999200\n",
      "D_acc: 0.90 8.48s, epoch [1459\\200]: train D loss: 0.2916, train F loss: -0.020953, acc 0.998900\n",
      "D_acc: 0.89 8.40s, epoch [1460\\200]: train D loss: 0.2959, train F loss: -0.020992, acc 0.998500\n",
      "D_acc: 0.90 8.16s, epoch [1461\\200]: train D loss: 0.2861, train F loss: -0.021420, acc 0.999100\n",
      "D_acc: 0.90 8.18s, epoch [1462\\200]: train D loss: 0.2839, train F loss: -0.020270, acc 0.999300\n",
      "D_acc: 0.89 8.48s, epoch [1463\\200]: train D loss: 0.3147, train F loss: -0.010348, acc 0.996000\n",
      "D_acc: 0.88 8.09s, epoch [1464\\200]: train D loss: 0.3345, train F loss: -0.013577, acc 0.995200\n",
      "D_acc: 0.89 8.04s, epoch [1465\\200]: train D loss: 0.3019, train F loss: -0.021866, acc 0.999300\n",
      "D_acc: 0.91 8.24s, epoch [1466\\200]: train D loss: 0.2765, train F loss: -0.021882, acc 0.999300\n",
      "D_acc: 0.91 8.22s, epoch [1467\\200]: train D loss: 0.2735, train F loss: -0.020859, acc 0.999500\n",
      "D_acc: 0.91 8.29s, epoch [1468\\200]: train D loss: 0.2768, train F loss: -0.019507, acc 0.999000\n",
      "D_acc: 0.90 8.52s, epoch [1469\\200]: train D loss: 0.2985, train F loss: -0.015982, acc 0.996800\n",
      "D_acc: 0.89 8.59s, epoch [1470\\200]: train D loss: 0.3122, train F loss: -0.018566, acc 0.997400\n",
      "D_acc: 0.88 8.16s, epoch [1471\\200]: train D loss: 0.3223, train F loss: 0.004823, acc 0.993500\n",
      "D_acc: 0.88 8.51s, epoch [1472\\200]: train D loss: 0.3090, train F loss: -0.020413, acc 0.997800\n",
      "D_acc: 0.90 8.15s, epoch [1473\\200]: train D loss: 0.2747, train F loss: -0.018004, acc 0.999000\n",
      "D_acc: 0.91 8.22s, epoch [1474\\200]: train D loss: 0.2672, train F loss: -0.013764, acc 0.998700\n",
      "D_acc: 0.91 8.16s, epoch [1475\\200]: train D loss: 0.2597, train F loss: -0.019081, acc 0.999400\n",
      "D_acc: 0.91 8.13s, epoch [1476\\200]: train D loss: 0.2631, train F loss: -0.020246, acc 0.999700\n",
      "D_acc: 0.90 8.21s, epoch [1477\\200]: train D loss: 0.2866, train F loss: -0.021166, acc 0.999300\n",
      "D_acc: 0.90 8.15s, epoch [1478\\200]: train D loss: 0.2885, train F loss: -0.020757, acc 0.998900\n",
      "D_acc: 0.91 8.24s, epoch [1479\\200]: train D loss: 0.2752, train F loss: -0.020982, acc 0.999400\n",
      "D_acc: 0.90 8.28s, epoch [1480\\200]: train D loss: 0.2941, train F loss: -0.018735, acc 0.998600\n",
      "D_acc: 0.89 8.22s, epoch [1481\\200]: train D loss: 0.3210, train F loss: -0.013141, acc 0.997000\n",
      "D_acc: 0.87 8.20s, epoch [1482\\200]: train D loss: 0.3324, train F loss: -0.019494, acc 0.997200\n",
      "D_acc: 0.90 8.22s, epoch [1483\\200]: train D loss: 0.2839, train F loss: -0.021689, acc 0.999100\n",
      "D_acc: 0.91 8.16s, epoch [1484\\200]: train D loss: 0.2597, train F loss: -0.020389, acc 0.999600\n",
      "D_acc: 0.92 8.21s, epoch [1485\\200]: train D loss: 0.2452, train F loss: -0.018777, acc 0.999600\n",
      "D_acc: 0.91 8.19s, epoch [1486\\200]: train D loss: 0.2541, train F loss: -0.018979, acc 0.999700\n",
      "D_acc: 0.92 8.24s, epoch [1487\\200]: train D loss: 0.2459, train F loss: -0.018762, acc 0.999700\n",
      "D_acc: 0.91 8.15s, epoch [1488\\200]: train D loss: 0.2622, train F loss: -0.019904, acc 0.999800\n",
      "D_acc: 0.90 8.22s, epoch [1489\\200]: train D loss: 0.2830, train F loss: -0.021704, acc 0.999800\n",
      "D_acc: 0.89 8.17s, epoch [1490\\200]: train D loss: 0.3133, train F loss: -0.024725, acc 0.999900\n",
      "D_acc: 0.87 8.26s, epoch [1491\\200]: train D loss: 0.3443, train F loss: -0.027343, acc 0.999400\n",
      "D_acc: 0.87 8.22s, epoch [1492\\200]: train D loss: 0.3385, train F loss: -0.016055, acc 0.997900\n",
      "D_acc: 0.88 8.24s, epoch [1493\\200]: train D loss: 0.3167, train F loss: -0.019885, acc 0.998900\n",
      "D_acc: 0.87 8.17s, epoch [1494\\200]: train D loss: 0.3549, train F loss: 0.009639, acc 0.989300\n",
      "D_acc: 0.88 8.56s, epoch [1495\\200]: train D loss: 0.3221, train F loss: -0.019061, acc 0.997100\n",
      "D_acc: 0.90 8.59s, epoch [1496\\200]: train D loss: 0.2801, train F loss: -0.022222, acc 0.999300\n",
      "D_acc: 0.91 8.39s, epoch [1497\\200]: train D loss: 0.2546, train F loss: -0.020803, acc 0.999800\n",
      "D_acc: 0.92 8.25s, epoch [1498\\200]: train D loss: 0.2465, train F loss: -0.019071, acc 0.999500\n",
      "D_acc: 0.92 8.17s, epoch [1499\\200]: train D loss: 0.2568, train F loss: -0.019984, acc 0.999800\n",
      "D_acc: 0.91 8.25s, epoch [1500\\200]: train D loss: 0.2725, train F loss: -0.021055, acc 0.999900\n",
      "D_acc: 0.90 8.28s, epoch [1501\\200]: train D loss: 0.3018, train F loss: -0.023254, acc 0.999500\n",
      "D_acc: 0.89 8.12s, epoch [1502\\200]: train D loss: 0.3008, train F loss: -0.023780, acc 0.999600\n",
      "D_acc: 0.89 8.13s, epoch [1503\\200]: train D loss: 0.3049, train F loss: -0.023928, acc 0.999600\n",
      "D_acc: 0.89 8.08s, epoch [1504\\200]: train D loss: 0.3087, train F loss: -0.024658, acc 0.999600\n",
      "D_acc: 0.89 8.22s, epoch [1505\\200]: train D loss: 0.3100, train F loss: -0.023836, acc 0.999100\n",
      "D_acc: 0.89 8.22s, epoch [1506\\200]: train D loss: 0.3057, train F loss: -0.023156, acc 0.999200\n",
      "D_acc: 0.89 8.18s, epoch [1507\\200]: train D loss: 0.3083, train F loss: -0.014839, acc 0.996800\n",
      "D_acc: 0.88 8.15s, epoch [1508\\200]: train D loss: 0.3185, train F loss: -0.017735, acc 0.996800\n",
      "D_acc: 0.90 8.17s, epoch [1509\\200]: train D loss: 0.2803, train F loss: -0.022838, acc 0.999600\n",
      "D_acc: 0.90 8.32s, epoch [1510\\200]: train D loss: 0.2746, train F loss: -0.022235, acc 0.999700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.90 8.11s, epoch [1511\\200]: train D loss: 0.2788, train F loss: -0.021323, acc 0.999600\n",
      "D_acc: 0.90 8.21s, epoch [1512\\200]: train D loss: 0.2951, train F loss: -0.019166, acc 0.998900\n",
      "D_acc: 0.90 8.19s, epoch [1513\\200]: train D loss: 0.2962, train F loss: -0.023059, acc 0.999700\n",
      "D_acc: 0.89 8.16s, epoch [1514\\200]: train D loss: 0.3034, train F loss: -0.024358, acc 0.999900\n",
      "D_acc: 0.89 8.26s, epoch [1515\\200]: train D loss: 0.3077, train F loss: -0.024898, acc 0.999800\n",
      "D_acc: 0.88 8.28s, epoch [1516\\200]: train D loss: 0.3178, train F loss: -0.025588, acc 0.999700\n",
      "D_acc: 0.87 8.20s, epoch [1517\\200]: train D loss: 0.3364, train F loss: -0.027678, acc 0.999700\n",
      "D_acc: 0.87 8.18s, epoch [1518\\200]: train D loss: 0.3317, train F loss: -0.027705, acc 0.999900\n",
      "D_acc: 0.87 8.22s, epoch [1519\\200]: train D loss: 0.3432, train F loss: -0.028660, acc 0.999900\n",
      "D_acc: 0.86 8.43s, epoch [1520\\200]: train D loss: 0.3426, train F loss: -0.028846, acc 0.999800\n",
      "D_acc: 0.86 8.41s, epoch [1521\\200]: train D loss: 0.3536, train F loss: -0.029432, acc 0.999600\n",
      "D_acc: 0.87 8.40s, epoch [1522\\200]: train D loss: 0.3446, train F loss: -0.024760, acc 0.998700\n",
      "D_acc: 0.86 8.54s, epoch [1523\\200]: train D loss: 0.3698, train F loss: -0.019965, acc 0.996800\n",
      "D_acc: 0.86 8.99s, epoch [1524\\200]: train D loss: 0.3551, train F loss: -0.021859, acc 0.997100\n",
      "D_acc: 0.87 8.10s, epoch [1525\\200]: train D loss: 0.3319, train F loss: -0.024904, acc 0.998600\n",
      "D_acc: 0.88 8.42s, epoch [1526\\200]: train D loss: 0.3133, train F loss: -0.026029, acc 0.999600\n",
      "D_acc: 0.89 8.15s, epoch [1527\\200]: train D loss: 0.2936, train F loss: -0.025056, acc 0.999800\n",
      "D_acc: 0.89 8.04s, epoch [1528\\200]: train D loss: 0.2964, train F loss: -0.024596, acc 0.999700\n",
      "D_acc: 0.89 8.08s, epoch [1529\\200]: train D loss: 0.3184, train F loss: -0.021030, acc 0.998500\n",
      "D_acc: 0.87 8.40s, epoch [1530\\200]: train D loss: 0.3366, train F loss: -0.021760, acc 0.997400\n",
      "D_acc: 0.89 8.22s, epoch [1531\\200]: train D loss: 0.3088, train F loss: -0.021326, acc 0.998100\n",
      "D_acc: 0.88 8.12s, epoch [1532\\200]: train D loss: 0.3286, train F loss: -0.008818, acc 0.996400\n",
      "D_acc: 0.88 8.14s, epoch [1533\\200]: train D loss: 0.3305, train F loss: -0.018539, acc 0.997000\n",
      "D_acc: 0.89 8.34s, epoch [1534\\200]: train D loss: 0.2922, train F loss: -0.023647, acc 0.999600\n",
      "D_acc: 0.91 8.62s, epoch [1535\\200]: train D loss: 0.2722, train F loss: -0.022765, acc 0.999900\n",
      "D_acc: 0.90 8.26s, epoch [1536\\200]: train D loss: 0.2775, train F loss: -0.021604, acc 0.999600\n",
      "D_acc: 0.88 8.06s, epoch [1537\\200]: train D loss: 0.3282, train F loss: 0.006611, acc 0.994900\n",
      "D_acc: 0.88 8.49s, epoch [1538\\200]: train D loss: 0.3229, train F loss: -0.019051, acc 0.997200\n",
      "D_acc: 0.90 8.10s, epoch [1539\\200]: train D loss: 0.2840, train F loss: -0.021487, acc 0.999400\n",
      "D_acc: 0.91 8.08s, epoch [1540\\200]: train D loss: 0.2662, train F loss: -0.021910, acc 0.999800\n",
      "D_acc: 0.90 8.20s, epoch [1541\\200]: train D loss: 0.2825, train F loss: -0.021760, acc 0.999500\n",
      "D_acc: 0.89 8.09s, epoch [1542\\200]: train D loss: 0.2969, train F loss: -0.022652, acc 0.999600\n",
      "D_acc: 0.90 8.11s, epoch [1543\\200]: train D loss: 0.2865, train F loss: -0.021123, acc 0.999100\n",
      "D_acc: 0.89 8.16s, epoch [1544\\200]: train D loss: 0.2914, train F loss: -0.023269, acc 0.999400\n",
      "D_acc: 0.88 8.25s, epoch [1545\\200]: train D loss: 0.2996, train F loss: -0.022633, acc 0.999600\n",
      "D_acc: 0.89 8.12s, epoch [1546\\200]: train D loss: 0.3060, train F loss: -0.021535, acc 0.999100\n",
      "D_acc: 0.88 8.18s, epoch [1547\\200]: train D loss: 0.3060, train F loss: -0.024760, acc 0.999700\n",
      "D_acc: 0.88 8.20s, epoch [1548\\200]: train D loss: 0.3092, train F loss: -0.025143, acc 0.999800\n",
      "D_acc: 0.87 8.21s, epoch [1549\\200]: train D loss: 0.3287, train F loss: -0.026907, acc 0.999800\n",
      "D_acc: 0.86 8.10s, epoch [1550\\200]: train D loss: 0.3419, train F loss: -0.028631, acc 0.999900\n",
      "D_acc: 0.86 8.25s, epoch [1551\\200]: train D loss: 0.3543, train F loss: -0.029691, acc 0.999900\n",
      "D_acc: 0.85 8.15s, epoch [1552\\200]: train D loss: 0.3608, train F loss: -0.030599, acc 0.999800\n",
      "D_acc: 0.86 8.10s, epoch [1553\\200]: train D loss: 0.3454, train F loss: -0.029485, acc 0.999600\n",
      "D_acc: 0.86 8.18s, epoch [1554\\200]: train D loss: 0.3433, train F loss: -0.028801, acc 0.999700\n",
      "D_acc: 0.87 8.15s, epoch [1555\\200]: train D loss: 0.3492, train F loss: -0.025513, acc 0.998900\n",
      "D_acc: 0.85 8.20s, epoch [1556\\200]: train D loss: 0.3734, train F loss: -0.021958, acc 0.996600\n",
      "D_acc: 0.85 8.19s, epoch [1557\\200]: train D loss: 0.3658, train F loss: -0.022737, acc 0.996500\n",
      "D_acc: 0.87 8.39s, epoch [1558\\200]: train D loss: 0.3414, train F loss: -0.024853, acc 0.998100\n",
      "D_acc: 0.88 8.31s, epoch [1559\\200]: train D loss: 0.3132, train F loss: -0.025736, acc 0.999400\n",
      "D_acc: 0.89 8.17s, epoch [1560\\200]: train D loss: 0.3012, train F loss: -0.025024, acc 0.999500\n",
      "D_acc: 0.88 8.30s, epoch [1561\\200]: train D loss: 0.3054, train F loss: -0.025577, acc 0.999900\n",
      "D_acc: 0.88 8.09s, epoch [1562\\200]: train D loss: 0.3141, train F loss: -0.025832, acc 0.999700\n",
      "D_acc: 0.87 8.19s, epoch [1563\\200]: train D loss: 0.3255, train F loss: -0.026677, acc 0.999600\n",
      "D_acc: 0.87 8.21s, epoch [1564\\200]: train D loss: 0.3424, train F loss: -0.027489, acc 0.999700\n",
      "D_acc: 0.86 8.15s, epoch [1565\\200]: train D loss: 0.3501, train F loss: -0.026905, acc 0.999200\n",
      "D_acc: 0.86 8.21s, epoch [1566\\200]: train D loss: 0.3621, train F loss: -0.015564, acc 0.996400\n",
      "D_acc: 0.85 8.34s, epoch [1567\\200]: train D loss: 0.3708, train F loss: 0.000227, acc 0.993300\n",
      "D_acc: 0.87 8.21s, epoch [1568\\200]: train D loss: 0.3408, train F loss: -0.025631, acc 0.998500\n",
      "D_acc: 0.88 8.16s, epoch [1569\\200]: train D loss: 0.3094, train F loss: -0.025154, acc 0.999400\n",
      "D_acc: 0.89 8.08s, epoch [1570\\200]: train D loss: 0.2958, train F loss: -0.024095, acc 0.999700\n",
      "D_acc: 0.89 8.15s, epoch [1571\\200]: train D loss: 0.2989, train F loss: -0.024971, acc 0.999800\n",
      "D_acc: 0.88 8.19s, epoch [1572\\200]: train D loss: 0.3161, train F loss: -0.025886, acc 0.999700\n",
      "D_acc: 0.88 8.26s, epoch [1573\\200]: train D loss: 0.3238, train F loss: -0.021280, acc 0.998900\n",
      "D_acc: 0.88 8.11s, epoch [1574\\200]: train D loss: 0.3244, train F loss: -0.024651, acc 0.999200\n",
      "D_acc: 0.88 8.22s, epoch [1575\\200]: train D loss: 0.3224, train F loss: -0.026505, acc 0.999500\n",
      "D_acc: 0.88 8.35s, epoch [1576\\200]: train D loss: 0.3350, train F loss: -0.025715, acc 0.999100\n",
      "D_acc: 0.87 8.21s, epoch [1577\\200]: train D loss: 0.3381, train F loss: -0.023202, acc 0.998300\n",
      "D_acc: 0.87 8.27s, epoch [1578\\200]: train D loss: 0.3480, train F loss: -0.026178, acc 0.998600\n",
      "D_acc: 0.88 8.25s, epoch [1579\\200]: train D loss: 0.3292, train F loss: -0.023582, acc 0.997700\n",
      "D_acc: 0.89 8.17s, epoch [1580\\200]: train D loss: 0.3001, train F loss: -0.024072, acc 0.999200\n",
      "D_acc: 0.89 8.15s, epoch [1581\\200]: train D loss: 0.3015, train F loss: -0.024814, acc 0.999800\n",
      "D_acc: 0.87 8.24s, epoch [1582\\200]: train D loss: 0.3230, train F loss: -0.026779, acc 0.999900\n",
      "D_acc: 0.87 8.10s, epoch [1583\\200]: train D loss: 0.3424, train F loss: 0.004845, acc 0.997300\n",
      "D_acc: 0.85 8.32s, epoch [1584\\200]: train D loss: 0.3690, train F loss: -0.011833, acc 0.994700\n",
      "D_acc: 0.87 8.08s, epoch [1585\\200]: train D loss: 0.3325, train F loss: -0.026215, acc 0.998900\n",
      "D_acc: 0.90 8.23s, epoch [1586\\200]: train D loss: 0.2845, train F loss: -0.024669, acc 0.999900\n",
      "D_acc: 0.89 8.47s, epoch [1587\\200]: train D loss: 0.2869, train F loss: -0.023545, acc 0.999700\n",
      "D_acc: 0.89 8.54s, epoch [1588\\200]: train D loss: 0.2992, train F loss: -0.023614, acc 0.999300\n",
      "D_acc: 0.89 8.37s, epoch [1589\\200]: train D loss: 0.3049, train F loss: -0.022663, acc 0.999400\n",
      "D_acc: 0.89 8.26s, epoch [1590\\200]: train D loss: 0.3140, train F loss: -0.021234, acc 0.999200\n",
      "D_acc: 0.88 8.28s, epoch [1591\\200]: train D loss: 0.3290, train F loss: -0.023010, acc 0.998400\n",
      "D_acc: 0.88 8.33s, epoch [1592\\200]: train D loss: 0.3252, train F loss: -0.022947, acc 0.998700\n",
      "D_acc: 0.89 8.36s, epoch [1593\\200]: train D loss: 0.2929, train F loss: -0.024381, acc 0.999800\n",
      "D_acc: 0.89 8.15s, epoch [1594\\200]: train D loss: 0.2923, train F loss: -0.023881, acc 0.999600\n",
      "D_acc: 0.89 8.18s, epoch [1595\\200]: train D loss: 0.2975, train F loss: -0.023488, acc 0.999200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.88 8.13s, epoch [1596\\200]: train D loss: 0.3122, train F loss: -0.025339, acc 0.999900\n",
      "D_acc: 0.87 8.18s, epoch [1597\\200]: train D loss: 0.3392, train F loss: -0.027492, acc 0.999700\n",
      "D_acc: 0.85 8.10s, epoch [1598\\200]: train D loss: 0.3645, train F loss: -0.028931, acc 0.999200\n",
      "D_acc: 0.87 8.08s, epoch [1599\\200]: train D loss: 0.3426, train F loss: -0.027864, acc 0.999200\n",
      "D_acc: 0.87 8.21s, epoch [1600\\200]: train D loss: 0.3300, train F loss: -0.027486, acc 0.999500\n",
      "add : 5000 new source size: 10000\n",
      "D_acc: 0.82 8.18s, epoch [1601\\200]: train D loss: 0.4484, train F loss: 0.456923, acc 0.902000\n",
      "D_acc: 0.84 8.13s, epoch [1602\\200]: train D loss: 0.3758, train F loss: 0.125701, acc 0.953300\n",
      "D_acc: 0.86 8.06s, epoch [1603\\200]: train D loss: 0.3453, train F loss: 0.033775, acc 0.981300\n",
      "D_acc: 0.87 8.18s, epoch [1604\\200]: train D loss: 0.3285, train F loss: 0.003261, acc 0.989900\n",
      "D_acc: 0.87 8.06s, epoch [1605\\200]: train D loss: 0.3280, train F loss: -0.008825, acc 0.992900\n",
      "D_acc: 0.88 8.06s, epoch [1606\\200]: train D loss: 0.3076, train F loss: -0.014774, acc 0.996000\n",
      "D_acc: 0.88 8.37s, epoch [1607\\200]: train D loss: 0.3104, train F loss: -0.020128, acc 0.997600\n",
      "D_acc: 0.88 8.39s, epoch [1608\\200]: train D loss: 0.3005, train F loss: -0.021745, acc 0.998700\n",
      "D_acc: 0.88 8.15s, epoch [1609\\200]: train D loss: 0.3083, train F loss: -0.022996, acc 0.999200\n",
      "D_acc: 0.88 8.09s, epoch [1610\\200]: train D loss: 0.3207, train F loss: -0.024586, acc 0.999200\n",
      "D_acc: 0.88 8.13s, epoch [1611\\200]: train D loss: 0.3217, train F loss: -0.007378, acc 0.996300\n",
      "D_acc: 0.88 8.11s, epoch [1612\\200]: train D loss: 0.3147, train F loss: -0.019655, acc 0.997600\n",
      "D_acc: 0.89 8.20s, epoch [1613\\200]: train D loss: 0.2926, train F loss: -0.020886, acc 0.998300\n",
      "D_acc: 0.89 8.47s, epoch [1614\\200]: train D loss: 0.2821, train F loss: -0.018876, acc 0.998800\n",
      "D_acc: 0.90 8.15s, epoch [1615\\200]: train D loss: 0.2879, train F loss: -0.021301, acc 0.999200\n",
      "D_acc: 0.89 8.20s, epoch [1616\\200]: train D loss: 0.3135, train F loss: -0.022499, acc 0.998900\n",
      "D_acc: 0.88 8.20s, epoch [1617\\200]: train D loss: 0.3411, train F loss: -0.025146, acc 0.999200\n",
      "D_acc: 0.87 8.23s, epoch [1618\\200]: train D loss: 0.3396, train F loss: -0.022638, acc 0.998400\n",
      "D_acc: 0.88 8.14s, epoch [1619\\200]: train D loss: 0.3151, train F loss: -0.018285, acc 0.997200\n",
      "D_acc: 0.88 8.19s, epoch [1620\\200]: train D loss: 0.3085, train F loss: -0.018087, acc 0.997700\n",
      "D_acc: 0.89 8.28s, epoch [1621\\200]: train D loss: 0.2987, train F loss: -0.018994, acc 0.998000\n",
      "D_acc: 0.90 8.43s, epoch [1622\\200]: train D loss: 0.2920, train F loss: -0.019776, acc 0.998500\n",
      "D_acc: 0.90 8.30s, epoch [1623\\200]: train D loss: 0.2897, train F loss: -0.021215, acc 0.999100\n",
      "D_acc: 0.89 8.27s, epoch [1624\\200]: train D loss: 0.3055, train F loss: -0.023435, acc 0.999000\n",
      "D_acc: 0.89 8.30s, epoch [1625\\200]: train D loss: 0.3015, train F loss: -0.022841, acc 0.999100\n",
      "D_acc: 0.89 8.50s, epoch [1626\\200]: train D loss: 0.3058, train F loss: -0.024343, acc 0.999600\n",
      "D_acc: 0.89 8.12s, epoch [1627\\200]: train D loss: 0.3069, train F loss: -0.023937, acc 0.999300\n",
      "D_acc: 0.88 8.28s, epoch [1628\\200]: train D loss: 0.3054, train F loss: -0.023795, acc 0.999100\n",
      "D_acc: 0.88 8.32s, epoch [1629\\200]: train D loss: 0.3135, train F loss: -0.021702, acc 0.998400\n",
      "D_acc: 0.88 8.27s, epoch [1630\\200]: train D loss: 0.3173, train F loss: -0.018991, acc 0.997500\n",
      "D_acc: 0.88 8.22s, epoch [1631\\200]: train D loss: 0.3331, train F loss: -0.007941, acc 0.995700\n",
      "D_acc: 0.87 8.69s, epoch [1632\\200]: train D loss: 0.3264, train F loss: -0.015056, acc 0.996100\n",
      "D_acc: 0.88 8.43s, epoch [1633\\200]: train D loss: 0.3010, train F loss: -0.022191, acc 0.998700\n",
      "D_acc: 0.88 8.30s, epoch [1634\\200]: train D loss: 0.3032, train F loss: -0.017908, acc 0.998200\n",
      "D_acc: 0.89 8.21s, epoch [1635\\200]: train D loss: 0.2809, train F loss: -0.022626, acc 0.999500\n",
      "D_acc: 0.88 8.25s, epoch [1636\\200]: train D loss: 0.3154, train F loss: -0.025156, acc 0.999400\n",
      "D_acc: 0.87 8.55s, epoch [1637\\200]: train D loss: 0.3235, train F loss: -0.026481, acc 0.999700\n",
      "D_acc: 0.88 8.26s, epoch [1638\\200]: train D loss: 0.3190, train F loss: -0.023460, acc 0.999000\n",
      "D_acc: 0.87 8.14s, epoch [1639\\200]: train D loss: 0.3282, train F loss: -0.021175, acc 0.997400\n",
      "D_acc: 0.88 8.24s, epoch [1640\\200]: train D loss: 0.3199, train F loss: -0.020315, acc 0.998100\n",
      "D_acc: 0.88 8.22s, epoch [1641\\200]: train D loss: 0.3146, train F loss: -0.024531, acc 0.999200\n",
      "D_acc: 0.90 8.20s, epoch [1642\\200]: train D loss: 0.2846, train F loss: -0.023735, acc 0.999600\n",
      "D_acc: 0.89 8.14s, epoch [1643\\200]: train D loss: 0.2913, train F loss: -0.023470, acc 0.999500\n",
      "D_acc: 0.89 8.12s, epoch [1644\\200]: train D loss: 0.3024, train F loss: -0.022782, acc 0.999600\n",
      "D_acc: 0.89 8.08s, epoch [1645\\200]: train D loss: 0.3091, train F loss: -0.023930, acc 0.999400\n",
      "D_acc: 0.87 8.17s, epoch [1646\\200]: train D loss: 0.3293, train F loss: -0.025555, acc 0.999400\n",
      "D_acc: 0.88 8.24s, epoch [1647\\200]: train D loss: 0.3251, train F loss: -0.025758, acc 0.999300\n",
      "D_acc: 0.87 8.69s, epoch [1648\\200]: train D loss: 0.3314, train F loss: -0.026329, acc 0.999100\n",
      "D_acc: 0.87 8.16s, epoch [1649\\200]: train D loss: 0.3375, train F loss: -0.022974, acc 0.998400\n",
      "D_acc: 0.87 8.11s, epoch [1650\\200]: train D loss: 0.3406, train F loss: -0.021948, acc 0.997300\n",
      "D_acc: 0.87 8.17s, epoch [1651\\200]: train D loss: 0.3289, train F loss: -0.025364, acc 0.998400\n",
      "D_acc: 0.88 8.17s, epoch [1652\\200]: train D loss: 0.3152, train F loss: -0.020499, acc 0.997800\n",
      "D_acc: 0.88 8.26s, epoch [1653\\200]: train D loss: 0.3207, train F loss: -0.021331, acc 0.997700\n",
      "D_acc: 0.88 8.20s, epoch [1654\\200]: train D loss: 0.3186, train F loss: -0.022376, acc 0.998300\n",
      "D_acc: 0.87 8.15s, epoch [1655\\200]: train D loss: 0.3336, train F loss: -0.015293, acc 0.997100\n",
      "D_acc: 0.87 8.67s, epoch [1656\\200]: train D loss: 0.3316, train F loss: -0.021125, acc 0.997400\n",
      "D_acc: 0.88 8.48s, epoch [1657\\200]: train D loss: 0.3140, train F loss: -0.024051, acc 0.999000\n",
      "D_acc: 0.88 8.39s, epoch [1658\\200]: train D loss: 0.3044, train F loss: -0.023530, acc 0.999200\n",
      "D_acc: 0.88 8.15s, epoch [1659\\200]: train D loss: 0.3241, train F loss: -0.026311, acc 0.999800\n",
      "D_acc: 0.87 8.16s, epoch [1660\\200]: train D loss: 0.3298, train F loss: -0.027573, acc 0.999800\n",
      "D_acc: 0.85 8.11s, epoch [1661\\200]: train D loss: 0.3562, train F loss: -0.029484, acc 0.999900\n",
      "D_acc: 0.86 8.26s, epoch [1662\\200]: train D loss: 0.3491, train F loss: -0.009701, acc 0.998900\n",
      "D_acc: 0.82 8.35s, epoch [1663\\200]: train D loss: 0.4288, train F loss: 0.009909, acc 0.987600\n",
      "D_acc: 0.84 8.59s, epoch [1664\\200]: train D loss: 0.3775, train F loss: -0.022764, acc 0.995800\n",
      "D_acc: 0.87 8.29s, epoch [1665\\200]: train D loss: 0.3168, train F loss: -0.028615, acc 0.999900\n",
      "D_acc: 0.89 8.33s, epoch [1666\\200]: train D loss: 0.2805, train F loss: -0.024917, acc 0.999800\n",
      "D_acc: 0.89 8.20s, epoch [1667\\200]: train D loss: 0.2901, train F loss: -0.024168, acc 0.999900\n",
      "D_acc: 0.87 8.19s, epoch [1668\\200]: train D loss: 0.3280, train F loss: -0.004212, acc 0.996700\n",
      "D_acc: 0.88 8.22s, epoch [1669\\200]: train D loss: 0.3170, train F loss: -0.019014, acc 0.996400\n",
      "D_acc: 0.90 8.11s, epoch [1670\\200]: train D loss: 0.2777, train F loss: -0.020967, acc 0.999200\n",
      "D_acc: 0.91 8.12s, epoch [1671\\200]: train D loss: 0.2631, train F loss: -0.021533, acc 0.999800\n",
      "D_acc: 0.91 8.18s, epoch [1672\\200]: train D loss: 0.2644, train F loss: -0.020991, acc 0.999800\n",
      "D_acc: 0.89 8.10s, epoch [1673\\200]: train D loss: 0.3082, train F loss: -0.022849, acc 0.999400\n",
      "D_acc: 0.89 8.50s, epoch [1674\\200]: train D loss: 0.3117, train F loss: -0.023040, acc 0.998800\n",
      "D_acc: 0.89 8.41s, epoch [1675\\200]: train D loss: 0.2940, train F loss: -0.020844, acc 0.998500\n",
      "D_acc: 0.91 8.13s, epoch [1676\\200]: train D loss: 0.2672, train F loss: -0.021219, acc 0.999500\n",
      "D_acc: 0.90 8.43s, epoch [1677\\200]: train D loss: 0.2788, train F loss: -0.022053, acc 0.999800\n",
      "D_acc: 0.90 8.21s, epoch [1678\\200]: train D loss: 0.2848, train F loss: -0.021295, acc 0.999400\n",
      "D_acc: 0.88 8.20s, epoch [1679\\200]: train D loss: 0.3134, train F loss: -0.024616, acc 0.999600\n",
      "D_acc: 0.87 8.19s, epoch [1680\\200]: train D loss: 0.3220, train F loss: -0.025978, acc 0.999600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.88 8.21s, epoch [1681\\200]: train D loss: 0.3235, train F loss: -0.021221, acc 0.999100\n",
      "D_acc: 0.88 8.49s, epoch [1682\\200]: train D loss: 0.3098, train F loss: -0.025052, acc 0.999100\n",
      "D_acc: 0.88 8.16s, epoch [1683\\200]: train D loss: 0.3019, train F loss: -0.024201, acc 0.999300\n",
      "D_acc: 0.89 8.24s, epoch [1684\\200]: train D loss: 0.2985, train F loss: -0.024214, acc 0.999500\n",
      "D_acc: 0.87 8.18s, epoch [1685\\200]: train D loss: 0.3265, train F loss: -0.026265, acc 0.999600\n",
      "D_acc: 0.87 8.17s, epoch [1686\\200]: train D loss: 0.3387, train F loss: -0.027715, acc 0.999600\n",
      "D_acc: 0.86 8.18s, epoch [1687\\200]: train D loss: 0.3491, train F loss: -0.028770, acc 0.999700\n",
      "D_acc: 0.87 8.26s, epoch [1688\\200]: train D loss: 0.3332, train F loss: -0.027035, acc 0.999300\n",
      "D_acc: 0.86 8.23s, epoch [1689\\200]: train D loss: 0.3441, train F loss: -0.028360, acc 0.999700\n",
      "D_acc: 0.86 8.25s, epoch [1690\\200]: train D loss: 0.3488, train F loss: -0.029242, acc 0.999700\n",
      "D_acc: 0.86 8.28s, epoch [1691\\200]: train D loss: 0.3550, train F loss: -0.029003, acc 0.999500\n",
      "D_acc: 0.86 8.20s, epoch [1692\\200]: train D loss: 0.3578, train F loss: -0.028054, acc 0.998700\n",
      "D_acc: 0.86 8.29s, epoch [1693\\200]: train D loss: 0.3528, train F loss: -0.021415, acc 0.997400\n",
      "D_acc: 0.85 8.22s, epoch [1694\\200]: train D loss: 0.3502, train F loss: -0.022887, acc 0.997200\n",
      "D_acc: 0.85 8.18s, epoch [1695\\200]: train D loss: 0.3752, train F loss: -0.000065, acc 0.994700\n",
      "D_acc: 0.85 8.18s, epoch [1696\\200]: train D loss: 0.3645, train F loss: -0.020871, acc 0.996600\n",
      "D_acc: 0.86 8.15s, epoch [1697\\200]: train D loss: 0.3429, train F loss: -0.028685, acc 0.999200\n",
      "D_acc: 0.88 8.13s, epoch [1698\\200]: train D loss: 0.3006, train F loss: -0.026613, acc 0.999900\n",
      "D_acc: 0.88 8.20s, epoch [1699\\200]: train D loss: 0.3069, train F loss: -0.026424, acc 0.999600\n",
      "D_acc: 0.88 8.06s, epoch [1700\\200]: train D loss: 0.3184, train F loss: -0.024646, acc 0.999000\n",
      "D_acc: 0.87 8.18s, epoch [1701\\200]: train D loss: 0.3361, train F loss: -0.025865, acc 0.998900\n",
      "D_acc: 0.87 8.12s, epoch [1702\\200]: train D loss: 0.3228, train F loss: -0.024360, acc 0.998800\n",
      "D_acc: 0.86 8.27s, epoch [1703\\200]: train D loss: 0.3433, train F loss: -0.025658, acc 0.998600\n",
      "D_acc: 0.87 8.10s, epoch [1704\\200]: train D loss: 0.3457, train F loss: -0.027154, acc 0.999600\n",
      "D_acc: 0.87 8.09s, epoch [1705\\200]: train D loss: 0.3422, train F loss: -0.019862, acc 0.997600\n",
      "D_acc: 0.86 8.21s, epoch [1706\\200]: train D loss: 0.3488, train F loss: -0.011979, acc 0.995500\n",
      "D_acc: 0.87 8.17s, epoch [1707\\200]: train D loss: 0.3296, train F loss: -0.025205, acc 0.999000\n",
      "D_acc: 0.88 8.13s, epoch [1708\\200]: train D loss: 0.3144, train F loss: -0.025492, acc 0.999100\n",
      "D_acc: 0.86 8.13s, epoch [1709\\200]: train D loss: 0.3374, train F loss: -0.025975, acc 0.998900\n",
      "D_acc: 0.87 8.07s, epoch [1710\\200]: train D loss: 0.3362, train F loss: -0.020031, acc 0.998800\n",
      "D_acc: 0.87 8.13s, epoch [1711\\200]: train D loss: 0.3308, train F loss: -0.025541, acc 0.999000\n",
      "D_acc: 0.87 8.15s, epoch [1712\\200]: train D loss: 0.3241, train F loss: -0.025684, acc 0.999100\n",
      "D_acc: 0.87 8.25s, epoch [1713\\200]: train D loss: 0.3365, train F loss: -0.024929, acc 0.998700\n",
      "D_acc: 0.86 8.22s, epoch [1714\\200]: train D loss: 0.3472, train F loss: -0.028160, acc 0.999200\n",
      "D_acc: 0.86 8.35s, epoch [1715\\200]: train D loss: 0.3440, train F loss: -0.029174, acc 0.999500\n",
      "D_acc: 0.85 8.24s, epoch [1716\\200]: train D loss: 0.3715, train F loss: -0.031105, acc 0.999500\n",
      "D_acc: 0.84 8.20s, epoch [1717\\200]: train D loss: 0.3805, train F loss: -0.033056, acc 0.999800\n",
      "D_acc: 0.83 8.30s, epoch [1718\\200]: train D loss: 0.3885, train F loss: -0.032763, acc 0.999500\n",
      "D_acc: 0.85 8.11s, epoch [1719\\200]: train D loss: 0.3560, train F loss: -0.031253, acc 0.999500\n",
      "D_acc: 0.85 7.76s, epoch [1720\\200]: train D loss: 0.3586, train F loss: -0.030950, acc 0.999700\n",
      "D_acc: 0.85 7.79s, epoch [1721\\200]: train D loss: 0.3603, train F loss: -0.029643, acc 0.998900\n",
      "D_acc: 0.85 7.74s, epoch [1722\\200]: train D loss: 0.3535, train F loss: -0.026936, acc 0.997800\n",
      "D_acc: 0.85 7.83s, epoch [1723\\200]: train D loss: 0.3578, train F loss: -0.027316, acc 0.998000\n",
      "D_acc: 0.86 7.81s, epoch [1724\\200]: train D loss: 0.3539, train F loss: -0.028374, acc 0.998600\n",
      "D_acc: 0.87 7.75s, epoch [1725\\200]: train D loss: 0.3310, train F loss: -0.027297, acc 0.999200\n",
      "D_acc: 0.86 7.75s, epoch [1726\\200]: train D loss: 0.3398, train F loss: -0.026880, acc 0.999000\n",
      "D_acc: 0.86 7.74s, epoch [1727\\200]: train D loss: 0.3479, train F loss: -0.028018, acc 0.999000\n",
      "D_acc: 0.85 7.74s, epoch [1728\\200]: train D loss: 0.3700, train F loss: -0.011403, acc 0.996600\n",
      "D_acc: 0.85 7.72s, epoch [1729\\200]: train D loss: 0.3864, train F loss: -0.029414, acc 0.998800\n",
      "D_acc: 0.87 7.74s, epoch [1730\\200]: train D loss: 0.3385, train F loss: -0.030095, acc 0.999800\n",
      "D_acc: 0.88 7.76s, epoch [1731\\200]: train D loss: 0.3164, train F loss: -0.028067, acc 0.999900\n",
      "D_acc: 0.87 7.78s, epoch [1732\\200]: train D loss: 0.3295, train F loss: -0.023257, acc 0.999000\n",
      "D_acc: 0.86 7.71s, epoch [1733\\200]: train D loss: 0.3433, train F loss: -0.025644, acc 0.998300\n",
      "D_acc: 0.86 7.75s, epoch [1734\\200]: train D loss: 0.3503, train F loss: -0.025712, acc 0.998700\n",
      "D_acc: 0.85 7.70s, epoch [1735\\200]: train D loss: 0.3571, train F loss: -0.028812, acc 0.998800\n",
      "D_acc: 0.85 7.73s, epoch [1736\\200]: train D loss: 0.3565, train F loss: -0.029303, acc 0.999300\n",
      "D_acc: 0.86 7.75s, epoch [1737\\200]: train D loss: 0.3503, train F loss: -0.026459, acc 0.998400\n",
      "D_acc: 0.85 7.74s, epoch [1738\\200]: train D loss: 0.3631, train F loss: -0.027104, acc 0.998100\n",
      "D_acc: 0.85 7.73s, epoch [1739\\200]: train D loss: 0.3586, train F loss: -0.028699, acc 0.998800\n",
      "D_acc: 0.86 7.72s, epoch [1740\\200]: train D loss: 0.3565, train F loss: -0.028143, acc 0.999000\n",
      "D_acc: 0.85 7.79s, epoch [1741\\200]: train D loss: 0.3569, train F loss: -0.029607, acc 0.999100\n",
      "D_acc: 0.85 7.76s, epoch [1742\\200]: train D loss: 0.3635, train F loss: -0.031151, acc 0.999500\n",
      "D_acc: 0.84 7.74s, epoch [1743\\200]: train D loss: 0.3698, train F loss: -0.031324, acc 0.999400\n",
      "D_acc: 0.85 7.73s, epoch [1744\\200]: train D loss: 0.3704, train F loss: -0.031257, acc 0.999100\n",
      "D_acc: 0.84 7.74s, epoch [1745\\200]: train D loss: 0.3757, train F loss: -0.030375, acc 0.999000\n",
      "D_acc: 0.84 7.69s, epoch [1746\\200]: train D loss: 0.3860, train F loss: -0.025487, acc 0.997700\n",
      "D_acc: 0.83 7.73s, epoch [1747\\200]: train D loss: 0.4008, train F loss: -0.027693, acc 0.997200\n",
      "D_acc: 0.84 7.73s, epoch [1748\\200]: train D loss: 0.3759, train F loss: -0.028958, acc 0.998400\n",
      "D_acc: 0.86 7.76s, epoch [1749\\200]: train D loss: 0.3505, train F loss: -0.028463, acc 0.998700\n",
      "D_acc: 0.85 7.72s, epoch [1750\\200]: train D loss: 0.3556, train F loss: -0.030339, acc 0.999400\n",
      "D_acc: 0.86 7.71s, epoch [1751\\200]: train D loss: 0.3441, train F loss: -0.029845, acc 0.999300\n",
      "D_acc: 0.85 7.73s, epoch [1752\\200]: train D loss: 0.3655, train F loss: -0.031022, acc 0.999300\n",
      "D_acc: 0.85 7.72s, epoch [1753\\200]: train D loss: 0.3568, train F loss: -0.030429, acc 0.999500\n",
      "D_acc: 0.85 7.74s, epoch [1754\\200]: train D loss: 0.3696, train F loss: -0.029908, acc 0.999200\n",
      "D_acc: 0.85 7.72s, epoch [1755\\200]: train D loss: 0.3566, train F loss: -0.029398, acc 0.998800\n",
      "D_acc: 0.84 7.69s, epoch [1756\\200]: train D loss: 0.3651, train F loss: -0.030809, acc 0.999100\n",
      "D_acc: 0.86 7.76s, epoch [1757\\200]: train D loss: 0.3502, train F loss: -0.029828, acc 0.999700\n",
      "D_acc: 0.86 7.70s, epoch [1758\\200]: train D loss: 0.3483, train F loss: -0.030622, acc 0.999800\n",
      "D_acc: 0.86 7.72s, epoch [1759\\200]: train D loss: 0.3598, train F loss: -0.028401, acc 0.998600\n",
      "D_acc: 0.85 7.73s, epoch [1760\\200]: train D loss: 0.3729, train F loss: -0.028210, acc 0.998500\n",
      "D_acc: 0.86 7.69s, epoch [1761\\200]: train D loss: 0.3566, train F loss: -0.029053, acc 0.999000\n",
      "D_acc: 0.85 7.74s, epoch [1762\\200]: train D loss: 0.3580, train F loss: -0.028060, acc 0.998200\n",
      "D_acc: 0.86 7.73s, epoch [1763\\200]: train D loss: 0.3385, train F loss: -0.028543, acc 0.999400\n",
      "D_acc: 0.86 7.72s, epoch [1764\\200]: train D loss: 0.3484, train F loss: -0.030009, acc 0.999500\n",
      "D_acc: 0.86 7.72s, epoch [1765\\200]: train D loss: 0.3507, train F loss: -0.016596, acc 0.999400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.80 7.71s, epoch [1766\\200]: train D loss: 0.5005, train F loss: 0.238209, acc 0.988700\n",
      "D_acc: 0.79 7.72s, epoch [1767\\200]: train D loss: 0.4713, train F loss: -0.018421, acc 0.992800\n",
      "D_acc: 0.81 7.72s, epoch [1768\\200]: train D loss: 0.4200, train F loss: -0.037705, acc 0.999200\n",
      "D_acc: 0.84 7.69s, epoch [1769\\200]: train D loss: 0.3680, train F loss: -0.034314, acc 0.999900\n",
      "D_acc: 0.86 7.72s, epoch [1770\\200]: train D loss: 0.3386, train F loss: -0.030785, acc 0.999900\n",
      "D_acc: 0.87 7.74s, epoch [1771\\200]: train D loss: 0.3168, train F loss: -0.026336, acc 0.999200\n",
      "D_acc: 0.87 7.72s, epoch [1772\\200]: train D loss: 0.3249, train F loss: -0.025821, acc 0.999200\n",
      "D_acc: 0.87 7.72s, epoch [1773\\200]: train D loss: 0.3303, train F loss: -0.027436, acc 0.999600\n",
      "D_acc: 0.86 7.72s, epoch [1774\\200]: train D loss: 0.3454, train F loss: -0.028480, acc 0.999300\n",
      "D_acc: 0.86 7.71s, epoch [1775\\200]: train D loss: 0.3489, train F loss: -0.026902, acc 0.999100\n",
      "D_acc: 0.86 7.71s, epoch [1776\\200]: train D loss: 0.3372, train F loss: -0.015960, acc 0.998600\n",
      "D_acc: 0.87 7.68s, epoch [1777\\200]: train D loss: 0.3313, train F loss: -0.026235, acc 0.998500\n",
      "D_acc: 0.88 7.72s, epoch [1778\\200]: train D loss: 0.3104, train F loss: -0.025560, acc 0.999300\n",
      "D_acc: 0.88 7.75s, epoch [1779\\200]: train D loss: 0.3157, train F loss: -0.025615, acc 0.999100\n",
      "D_acc: 0.87 7.69s, epoch [1780\\200]: train D loss: 0.3285, train F loss: -0.027671, acc 0.999400\n",
      "D_acc: 0.85 7.76s, epoch [1781\\200]: train D loss: 0.3548, train F loss: -0.028481, acc 0.999000\n",
      "D_acc: 0.85 7.76s, epoch [1782\\200]: train D loss: 0.3593, train F loss: -0.029768, acc 0.999200\n",
      "D_acc: 0.84 7.75s, epoch [1783\\200]: train D loss: 0.3784, train F loss: -0.031414, acc 0.999400\n",
      "D_acc: 0.85 7.72s, epoch [1784\\200]: train D loss: 0.3663, train F loss: -0.031572, acc 0.999500\n",
      "D_acc: 0.85 7.73s, epoch [1785\\200]: train D loss: 0.3572, train F loss: -0.031023, acc 0.999800\n",
      "D_acc: 0.85 7.74s, epoch [1786\\200]: train D loss: 0.3651, train F loss: -0.031947, acc 0.999800\n",
      "D_acc: 0.83 7.74s, epoch [1787\\200]: train D loss: 0.3917, train F loss: -0.034500, acc 0.999800\n",
      "D_acc: 0.84 7.72s, epoch [1788\\200]: train D loss: 0.3731, train F loss: -0.032660, acc 0.999600\n",
      "D_acc: 0.85 7.73s, epoch [1789\\200]: train D loss: 0.3522, train F loss: -0.029781, acc 0.998900\n",
      "D_acc: 0.86 7.75s, epoch [1790\\200]: train D loss: 0.3503, train F loss: -0.028713, acc 0.999000\n",
      "D_acc: 0.86 7.84s, epoch [1791\\200]: train D loss: 0.3463, train F loss: -0.026628, acc 0.998100\n",
      "D_acc: 0.86 7.71s, epoch [1792\\200]: train D loss: 0.3394, train F loss: -0.026248, acc 0.998100\n",
      "D_acc: 0.87 7.74s, epoch [1793\\200]: train D loss: 0.3345, train F loss: -0.027206, acc 0.999100\n",
      "D_acc: 0.86 7.70s, epoch [1794\\200]: train D loss: 0.3459, train F loss: -0.028616, acc 0.999200\n",
      "D_acc: 0.85 7.74s, epoch [1795\\200]: train D loss: 0.3612, train F loss: -0.030394, acc 0.999300\n",
      "D_acc: 0.85 7.70s, epoch [1796\\200]: train D loss: 0.3590, train F loss: -0.031037, acc 0.999500\n",
      "D_acc: 0.86 7.68s, epoch [1797\\200]: train D loss: 0.3473, train F loss: -0.027581, acc 0.999100\n",
      "D_acc: 0.86 7.75s, epoch [1798\\200]: train D loss: 0.3398, train F loss: -0.026221, acc 0.998700\n",
      "D_acc: 0.86 7.76s, epoch [1799\\200]: train D loss: 0.3376, train F loss: -0.029066, acc 0.999600\n",
      "D_acc: 0.86 7.72s, epoch [1800\\200]: train D loss: 0.3451, train F loss: -0.028596, acc 0.999300\n",
      "add : 5000 new source size: 10000\n",
      "D_acc: 0.80 7.72s, epoch [1801\\200]: train D loss: 0.4813, train F loss: 0.380003, acc 0.916000\n",
      "D_acc: 0.82 7.69s, epoch [1802\\200]: train D loss: 0.4194, train F loss: 0.094561, acc 0.963300\n",
      "D_acc: 0.84 7.70s, epoch [1803\\200]: train D loss: 0.3813, train F loss: 0.017332, acc 0.986000\n",
      "D_acc: 0.84 7.72s, epoch [1804\\200]: train D loss: 0.3682, train F loss: -0.008138, acc 0.993100\n",
      "D_acc: 0.85 7.71s, epoch [1805\\200]: train D loss: 0.3565, train F loss: -0.017279, acc 0.996200\n",
      "D_acc: 0.85 7.72s, epoch [1806\\200]: train D loss: 0.3572, train F loss: -0.024154, acc 0.998000\n",
      "D_acc: 0.85 7.72s, epoch [1807\\200]: train D loss: 0.3565, train F loss: -0.025324, acc 0.998500\n",
      "D_acc: 0.84 7.72s, epoch [1808\\200]: train D loss: 0.3678, train F loss: -0.028996, acc 0.999000\n",
      "D_acc: 0.84 7.68s, epoch [1809\\200]: train D loss: 0.3752, train F loss: -0.030904, acc 0.999300\n",
      "D_acc: 0.84 7.71s, epoch [1810\\200]: train D loss: 0.3793, train F loss: -0.031422, acc 0.999400\n",
      "D_acc: 0.84 7.73s, epoch [1811\\200]: train D loss: 0.3670, train F loss: -0.031029, acc 0.999500\n",
      "D_acc: 0.84 7.67s, epoch [1812\\200]: train D loss: 0.3737, train F loss: -0.032069, acc 0.999500\n",
      "D_acc: 0.84 7.71s, epoch [1813\\200]: train D loss: 0.3756, train F loss: -0.032719, acc 0.999800\n",
      "D_acc: 0.84 7.75s, epoch [1814\\200]: train D loss: 0.3730, train F loss: -0.032057, acc 0.999800\n",
      "D_acc: 0.84 7.70s, epoch [1815\\200]: train D loss: 0.3788, train F loss: -0.032841, acc 0.999800\n",
      "D_acc: 0.83 7.71s, epoch [1816\\200]: train D loss: 0.3897, train F loss: -0.033982, acc 0.999800\n",
      "D_acc: 0.83 7.71s, epoch [1817\\200]: train D loss: 0.3880, train F loss: -0.033712, acc 0.999800\n",
      "D_acc: 0.83 7.72s, epoch [1818\\200]: train D loss: 0.3981, train F loss: -0.033359, acc 0.999100\n",
      "D_acc: 0.83 7.73s, epoch [1819\\200]: train D loss: 0.4000, train F loss: -0.028945, acc 0.997700\n",
      "D_acc: 0.83 7.72s, epoch [1820\\200]: train D loss: 0.3860, train F loss: -0.020079, acc 0.996100\n",
      "D_acc: 0.84 7.69s, epoch [1821\\200]: train D loss: 0.3773, train F loss: -0.018547, acc 0.995100\n",
      "D_acc: 0.84 7.76s, epoch [1822\\200]: train D loss: 0.3739, train F loss: -0.026377, acc 0.997400\n",
      "D_acc: 0.85 7.74s, epoch [1823\\200]: train D loss: 0.3610, train F loss: -0.022461, acc 0.997800\n",
      "D_acc: 0.86 7.72s, epoch [1824\\200]: train D loss: 0.3420, train F loss: -0.024625, acc 0.998000\n",
      "D_acc: 0.86 7.79s, epoch [1825\\200]: train D loss: 0.3364, train F loss: -0.027339, acc 0.999200\n",
      "D_acc: 0.87 7.70s, epoch [1826\\200]: train D loss: 0.3417, train F loss: -0.027666, acc 0.999200\n",
      "D_acc: 0.86 7.71s, epoch [1827\\200]: train D loss: 0.3471, train F loss: -0.029046, acc 0.999500\n",
      "D_acc: 0.87 7.66s, epoch [1828\\200]: train D loss: 0.3429, train F loss: -0.026346, acc 0.999100\n",
      "D_acc: 0.86 7.72s, epoch [1829\\200]: train D loss: 0.3557, train F loss: -0.027343, acc 0.998900\n",
      "D_acc: 0.85 7.72s, epoch [1830\\200]: train D loss: 0.3686, train F loss: -0.030038, acc 0.999300\n",
      "D_acc: 0.85 7.75s, epoch [1831\\200]: train D loss: 0.3566, train F loss: -0.024570, acc 0.999100\n",
      "D_acc: 0.84 7.72s, epoch [1832\\200]: train D loss: 0.3871, train F loss: -0.016516, acc 0.996000\n",
      "D_acc: 0.85 7.71s, epoch [1833\\200]: train D loss: 0.3637, train F loss: -0.026350, acc 0.998000\n",
      "D_acc: 0.86 7.69s, epoch [1834\\200]: train D loss: 0.3440, train F loss: -0.026915, acc 0.998700\n",
      "D_acc: 0.86 7.70s, epoch [1835\\200]: train D loss: 0.3354, train F loss: -0.027783, acc 0.998800\n",
      "D_acc: 0.86 7.71s, epoch [1836\\200]: train D loss: 0.3432, train F loss: -0.019851, acc 0.997800\n",
      "D_acc: 0.85 7.70s, epoch [1837\\200]: train D loss: 0.3566, train F loss: -0.015944, acc 0.995000\n",
      "D_acc: 0.85 7.71s, epoch [1838\\200]: train D loss: 0.3526, train F loss: -0.027085, acc 0.998200\n",
      "D_acc: 0.86 7.71s, epoch [1839\\200]: train D loss: 0.3442, train F loss: -0.029565, acc 0.999800\n",
      "D_acc: 0.85 7.69s, epoch [1840\\200]: train D loss: 0.3576, train F loss: -0.030654, acc 0.999600\n",
      "D_acc: 0.85 7.72s, epoch [1841\\200]: train D loss: 0.3537, train F loss: -0.029073, acc 0.999200\n",
      "D_acc: 0.85 7.71s, epoch [1842\\200]: train D loss: 0.3638, train F loss: -0.030906, acc 0.999600\n",
      "D_acc: 0.85 7.69s, epoch [1843\\200]: train D loss: 0.3601, train F loss: -0.031438, acc 0.999800\n",
      "D_acc: 0.84 7.71s, epoch [1844\\200]: train D loss: 0.3785, train F loss: -0.032597, acc 0.999600\n",
      "D_acc: 0.83 7.69s, epoch [1845\\200]: train D loss: 0.3961, train F loss: -0.034381, acc 0.999800\n",
      "D_acc: 0.83 7.72s, epoch [1846\\200]: train D loss: 0.3911, train F loss: -0.033434, acc 0.999500\n",
      "D_acc: 0.82 7.66s, epoch [1847\\200]: train D loss: 0.4118, train F loss: -0.004167, acc 0.995900\n",
      "D_acc: 0.82 7.73s, epoch [1848\\200]: train D loss: 0.4065, train F loss: -0.021725, acc 0.995400\n",
      "D_acc: 0.84 7.69s, epoch [1849\\200]: train D loss: 0.3748, train F loss: -0.029265, acc 0.997900\n",
      "D_acc: 0.86 7.72s, epoch [1850\\200]: train D loss: 0.3392, train F loss: -0.029828, acc 0.999600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.87 7.69s, epoch [1851\\200]: train D loss: 0.3224, train F loss: -0.028875, acc 0.999900\n",
      "D_acc: 0.88 7.71s, epoch [1852\\200]: train D loss: 0.3286, train F loss: -0.028562, acc 0.999800\n",
      "D_acc: 0.87 7.72s, epoch [1853\\200]: train D loss: 0.3308, train F loss: -0.028784, acc 0.999700\n",
      "D_acc: 0.85 7.66s, epoch [1854\\200]: train D loss: 0.3690, train F loss: -0.029796, acc 0.998900\n",
      "D_acc: 0.85 7.71s, epoch [1855\\200]: train D loss: 0.3600, train F loss: -0.029381, acc 0.998900\n",
      "D_acc: 0.85 7.69s, epoch [1856\\200]: train D loss: 0.3585, train F loss: -0.027732, acc 0.998800\n",
      "D_acc: 0.85 7.74s, epoch [1857\\200]: train D loss: 0.3498, train F loss: -0.027698, acc 0.998700\n",
      "D_acc: 0.85 7.69s, epoch [1858\\200]: train D loss: 0.3526, train F loss: -0.027752, acc 0.998300\n",
      "D_acc: 0.85 7.71s, epoch [1859\\200]: train D loss: 0.3586, train F loss: -0.028098, acc 0.998400\n",
      "D_acc: 0.85 7.70s, epoch [1860\\200]: train D loss: 0.3684, train F loss: -0.030484, acc 0.999000\n",
      "D_acc: 0.85 7.69s, epoch [1861\\200]: train D loss: 0.3481, train F loss: -0.030830, acc 0.999800\n",
      "D_acc: 0.84 7.69s, epoch [1862\\200]: train D loss: 0.3774, train F loss: -0.031689, acc 0.999400\n",
      "D_acc: 0.85 7.71s, epoch [1863\\200]: train D loss: 0.3683, train F loss: -0.032094, acc 0.999600\n",
      "D_acc: 0.84 7.71s, epoch [1864\\200]: train D loss: 0.3777, train F loss: -0.032761, acc 0.999600\n",
      "D_acc: 0.83 7.71s, epoch [1865\\200]: train D loss: 0.3868, train F loss: -0.033501, acc 0.999400\n",
      "D_acc: 0.84 7.71s, epoch [1866\\200]: train D loss: 0.3779, train F loss: -0.032331, acc 0.998900\n",
      "D_acc: 0.84 7.70s, epoch [1867\\200]: train D loss: 0.3758, train F loss: -0.031445, acc 0.999000\n",
      "D_acc: 0.84 7.69s, epoch [1868\\200]: train D loss: 0.3729, train F loss: -0.031567, acc 0.999200\n",
      "D_acc: 0.84 7.70s, epoch [1869\\200]: train D loss: 0.3787, train F loss: -0.031428, acc 0.998800\n",
      "D_acc: 0.84 7.71s, epoch [1870\\200]: train D loss: 0.3779, train F loss: -0.030941, acc 0.998800\n",
      "D_acc: 0.84 7.69s, epoch [1871\\200]: train D loss: 0.3788, train F loss: -0.031044, acc 0.998900\n",
      "D_acc: 0.84 7.70s, epoch [1872\\200]: train D loss: 0.3898, train F loss: -0.024706, acc 0.997000\n",
      "D_acc: 0.83 7.72s, epoch [1873\\200]: train D loss: 0.4066, train F loss: -0.007345, acc 0.995700\n",
      "D_acc: 0.82 7.73s, epoch [1874\\200]: train D loss: 0.4085, train F loss: -0.025074, acc 0.995200\n",
      "D_acc: 0.84 7.72s, epoch [1875\\200]: train D loss: 0.3774, train F loss: -0.032100, acc 0.999100\n",
      "D_acc: 0.85 7.71s, epoch [1876\\200]: train D loss: 0.3614, train F loss: -0.032447, acc 0.999600\n",
      "D_acc: 0.84 7.70s, epoch [1877\\200]: train D loss: 0.3720, train F loss: -0.032535, acc 0.999600\n",
      "D_acc: 0.84 7.69s, epoch [1878\\200]: train D loss: 0.3660, train F loss: -0.032670, acc 0.999700\n",
      "D_acc: 0.84 7.70s, epoch [1879\\200]: train D loss: 0.3781, train F loss: -0.032865, acc 0.999400\n",
      "D_acc: 0.84 7.71s, epoch [1880\\200]: train D loss: 0.3781, train F loss: -0.033663, acc 0.999800\n",
      "D_acc: 0.84 7.66s, epoch [1881\\200]: train D loss: 0.3697, train F loss: -0.031202, acc 0.999100\n",
      "D_acc: 0.84 7.71s, epoch [1882\\200]: train D loss: 0.3733, train F loss: -0.031721, acc 0.999000\n",
      "D_acc: 0.84 7.69s, epoch [1883\\200]: train D loss: 0.3722, train F loss: -0.032167, acc 0.999500\n",
      "D_acc: 0.84 7.73s, epoch [1884\\200]: train D loss: 0.3787, train F loss: -0.031594, acc 0.999000\n",
      "D_acc: 0.84 7.66s, epoch [1885\\200]: train D loss: 0.3718, train F loss: -0.032954, acc 0.999300\n",
      "D_acc: 0.83 7.71s, epoch [1886\\200]: train D loss: 0.3937, train F loss: -0.034329, acc 0.999500\n",
      "D_acc: 0.84 7.71s, epoch [1887\\200]: train D loss: 0.3863, train F loss: -0.034363, acc 0.999500\n",
      "D_acc: 0.83 7.71s, epoch [1888\\200]: train D loss: 0.3833, train F loss: -0.033707, acc 0.999800\n",
      "D_acc: 0.83 7.69s, epoch [1889\\200]: train D loss: 0.3948, train F loss: -0.032653, acc 0.999100\n",
      "D_acc: 0.83 7.71s, epoch [1890\\200]: train D loss: 0.3952, train F loss: -0.033528, acc 0.999300\n",
      "D_acc: 0.83 7.76s, epoch [1891\\200]: train D loss: 0.3921, train F loss: -0.033510, acc 0.999200\n",
      "D_acc: 0.83 7.69s, epoch [1892\\200]: train D loss: 0.3824, train F loss: -0.034085, acc 0.999500\n",
      "D_acc: 0.83 7.71s, epoch [1893\\200]: train D loss: 0.3873, train F loss: -0.033650, acc 0.999200\n",
      "D_acc: 0.83 7.70s, epoch [1894\\200]: train D loss: 0.3984, train F loss: -0.033765, acc 0.999300\n",
      "D_acc: 0.83 7.69s, epoch [1895\\200]: train D loss: 0.3969, train F loss: -0.032018, acc 0.998300\n",
      "D_acc: 0.82 7.71s, epoch [1896\\200]: train D loss: 0.4138, train F loss: -0.025988, acc 0.997300\n",
      "D_acc: 0.82 7.70s, epoch [1897\\200]: train D loss: 0.4000, train F loss: -0.030236, acc 0.997500\n",
      "D_acc: 0.84 7.69s, epoch [1898\\200]: train D loss: 0.3747, train F loss: -0.031714, acc 0.998600\n",
      "D_acc: 0.84 7.72s, epoch [1899\\200]: train D loss: 0.3695, train F loss: -0.031397, acc 0.998900\n",
      "D_acc: 0.84 7.72s, epoch [1900\\200]: train D loss: 0.3716, train F loss: -0.032091, acc 0.999300\n",
      "D_acc: 0.85 7.70s, epoch [1901\\200]: train D loss: 0.3611, train F loss: -0.032070, acc 0.999600\n",
      "D_acc: 0.84 7.71s, epoch [1902\\200]: train D loss: 0.3717, train F loss: -0.032552, acc 0.999600\n",
      "D_acc: 0.84 7.71s, epoch [1903\\200]: train D loss: 0.3784, train F loss: -0.033729, acc 0.999600\n",
      "D_acc: 0.83 7.70s, epoch [1904\\200]: train D loss: 0.4010, train F loss: -0.035170, acc 0.999700\n",
      "D_acc: 0.83 7.68s, epoch [1905\\200]: train D loss: 0.4016, train F loss: -0.035991, acc 0.999800\n",
      "D_acc: 0.82 7.70s, epoch [1906\\200]: train D loss: 0.4073, train F loss: -0.035924, acc 0.999700\n",
      "D_acc: 0.82 7.71s, epoch [1907\\200]: train D loss: 0.4030, train F loss: -0.036182, acc 0.999700\n",
      "D_acc: 0.83 7.73s, epoch [1908\\200]: train D loss: 0.3900, train F loss: -0.033422, acc 0.999200\n",
      "D_acc: 0.83 7.72s, epoch [1909\\200]: train D loss: 0.3950, train F loss: -0.035425, acc 0.999700\n",
      "D_acc: 0.83 7.71s, epoch [1910\\200]: train D loss: 0.3960, train F loss: -0.035293, acc 0.999700\n",
      "D_acc: 0.83 7.70s, epoch [1911\\200]: train D loss: 0.4003, train F loss: -0.035943, acc 0.999900\n",
      "D_acc: 0.82 7.72s, epoch [1912\\200]: train D loss: 0.4188, train F loss: -0.035716, acc 0.999200\n",
      "D_acc: 0.82 7.71s, epoch [1913\\200]: train D loss: 0.4065, train F loss: -0.036083, acc 0.999500\n",
      "D_acc: 0.83 7.71s, epoch [1914\\200]: train D loss: 0.3962, train F loss: -0.035428, acc 0.999200\n",
      "D_acc: 0.83 7.74s, epoch [1915\\200]: train D loss: 0.3832, train F loss: -0.034841, acc 0.999700\n",
      "D_acc: 0.83 7.74s, epoch [1916\\200]: train D loss: 0.3971, train F loss: -0.031820, acc 0.998400\n",
      "D_acc: 0.83 7.67s, epoch [1917\\200]: train D loss: 0.4015, train F loss: -0.033910, acc 0.998900\n",
      "D_acc: 0.83 7.70s, epoch [1918\\200]: train D loss: 0.3978, train F loss: -0.032134, acc 0.998800\n",
      "D_acc: 0.83 7.72s, epoch [1919\\200]: train D loss: 0.3979, train F loss: -0.030405, acc 0.997900\n",
      "D_acc: 0.83 7.71s, epoch [1920\\200]: train D loss: 0.3997, train F loss: -0.032589, acc 0.998300\n",
      "D_acc: 0.83 7.70s, epoch [1921\\200]: train D loss: 0.3890, train F loss: -0.032500, acc 0.998300\n",
      "D_acc: 0.83 7.71s, epoch [1922\\200]: train D loss: 0.3823, train F loss: -0.028993, acc 0.998400\n",
      "D_acc: 0.84 7.70s, epoch [1923\\200]: train D loss: 0.3801, train F loss: -0.025190, acc 0.998000\n",
      "D_acc: 0.84 7.67s, epoch [1924\\200]: train D loss: 0.3712, train F loss: -0.033079, acc 0.999600\n",
      "D_acc: 0.84 7.74s, epoch [1925\\200]: train D loss: 0.3835, train F loss: -0.033540, acc 0.999500\n",
      "D_acc: 0.84 7.70s, epoch [1926\\200]: train D loss: 0.3775, train F loss: -0.032777, acc 0.999500\n",
      "D_acc: 0.83 7.67s, epoch [1927\\200]: train D loss: 0.3979, train F loss: -0.019297, acc 0.995300\n",
      "D_acc: 0.84 7.70s, epoch [1928\\200]: train D loss: 0.3855, train F loss: -0.031057, acc 0.998500\n",
      "D_acc: 0.85 7.72s, epoch [1929\\200]: train D loss: 0.3675, train F loss: -0.032675, acc 0.999700\n",
      "D_acc: 0.84 7.70s, epoch [1930\\200]: train D loss: 0.3827, train F loss: -0.033881, acc 0.999700\n",
      "D_acc: 0.83 7.68s, epoch [1931\\200]: train D loss: 0.3896, train F loss: -0.033236, acc 0.999300\n",
      "D_acc: 0.83 7.70s, epoch [1932\\200]: train D loss: 0.3906, train F loss: -0.034713, acc 0.999600\n",
      "D_acc: 0.83 7.72s, epoch [1933\\200]: train D loss: 0.4032, train F loss: -0.025955, acc 0.997800\n",
      "D_acc: 0.83 7.74s, epoch [1934\\200]: train D loss: 0.4040, train F loss: -0.028451, acc 0.996900\n",
      "D_acc: 0.84 7.72s, epoch [1935\\200]: train D loss: 0.3702, train F loss: -0.031297, acc 0.998900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.84 7.72s, epoch [1936\\200]: train D loss: 0.3639, train F loss: -0.032364, acc 0.999600\n",
      "D_acc: 0.83 7.66s, epoch [1937\\200]: train D loss: 0.3807, train F loss: -0.033985, acc 0.999500\n",
      "D_acc: 0.82 7.66s, epoch [1938\\200]: train D loss: 0.3960, train F loss: -0.035079, acc 0.999400\n",
      "D_acc: 0.82 7.71s, epoch [1939\\200]: train D loss: 0.4052, train F loss: -0.036719, acc 0.999700\n",
      "D_acc: 0.82 7.70s, epoch [1940\\200]: train D loss: 0.4071, train F loss: -0.036535, acc 0.999600\n",
      "D_acc: 0.82 7.70s, epoch [1941\\200]: train D loss: 0.4106, train F loss: -0.037291, acc 0.999600\n",
      "D_acc: 0.82 7.78s, epoch [1942\\200]: train D loss: 0.4099, train F loss: -0.037237, acc 0.999700\n",
      "D_acc: 0.82 7.71s, epoch [1943\\200]: train D loss: 0.4075, train F loss: -0.037433, acc 0.999900\n",
      "D_acc: 0.81 7.70s, epoch [1944\\200]: train D loss: 0.4209, train F loss: -0.038451, acc 0.999900\n",
      "D_acc: 0.81 7.70s, epoch [1945\\200]: train D loss: 0.4291, train F loss: -0.034137, acc 0.998300\n",
      "D_acc: 0.82 7.70s, epoch [1946\\200]: train D loss: 0.4100, train F loss: -0.031081, acc 0.998000\n",
      "D_acc: 0.82 7.69s, epoch [1947\\200]: train D loss: 0.3978, train F loss: -0.031133, acc 0.998000\n",
      "D_acc: 0.83 7.69s, epoch [1948\\200]: train D loss: 0.3800, train F loss: -0.033140, acc 0.998900\n",
      "D_acc: 0.84 7.72s, epoch [1949\\200]: train D loss: 0.3795, train F loss: -0.032693, acc 0.999200\n",
      "D_acc: 0.82 7.69s, epoch [1950\\200]: train D loss: 0.4417, train F loss: 0.054506, acc 0.994700\n",
      "D_acc: 0.79 7.71s, epoch [1951\\200]: train D loss: 0.4737, train F loss: 0.055426, acc 0.987400\n",
      "D_acc: 0.81 7.70s, epoch [1952\\200]: train D loss: 0.4289, train F loss: -0.026808, acc 0.996600\n",
      "D_acc: 0.82 7.71s, epoch [1953\\200]: train D loss: 0.4062, train F loss: -0.036811, acc 0.999400\n",
      "D_acc: 0.83 7.67s, epoch [1954\\200]: train D loss: 0.3754, train F loss: -0.034103, acc 0.999600\n",
      "D_acc: 0.84 7.72s, epoch [1955\\200]: train D loss: 0.3641, train F loss: -0.032407, acc 0.999600\n",
      "D_acc: 0.84 7.70s, epoch [1956\\200]: train D loss: 0.3628, train F loss: -0.032647, acc 0.999800\n",
      "D_acc: 0.84 7.70s, epoch [1957\\200]: train D loss: 0.3657, train F loss: -0.033147, acc 0.999900\n",
      "D_acc: 0.84 7.68s, epoch [1958\\200]: train D loss: 0.3741, train F loss: -0.031763, acc 0.999100\n",
      "D_acc: 0.85 7.74s, epoch [1959\\200]: train D loss: 0.3660, train F loss: -0.031941, acc 0.999600\n",
      "D_acc: 0.85 7.71s, epoch [1960\\200]: train D loss: 0.3560, train F loss: -0.031511, acc 0.999800\n",
      "D_acc: 0.86 7.66s, epoch [1961\\200]: train D loss: 0.3532, train F loss: -0.030704, acc 0.999500\n",
      "D_acc: 0.86 7.72s, epoch [1962\\200]: train D loss: 0.3530, train F loss: -0.030179, acc 0.999200\n",
      "D_acc: 0.85 7.71s, epoch [1963\\200]: train D loss: 0.3686, train F loss: -0.029468, acc 0.998700\n",
      "D_acc: 0.84 7.70s, epoch [1964\\200]: train D loss: 0.3758, train F loss: -0.031089, acc 0.998800\n",
      "D_acc: 0.83 7.70s, epoch [1965\\200]: train D loss: 0.3824, train F loss: -0.032106, acc 0.999200\n",
      "D_acc: 0.83 7.69s, epoch [1966\\200]: train D loss: 0.3978, train F loss: -0.034103, acc 0.999300\n",
      "D_acc: 0.83 7.69s, epoch [1967\\200]: train D loss: 0.3954, train F loss: -0.035430, acc 0.999600\n",
      "D_acc: 0.82 7.72s, epoch [1968\\200]: train D loss: 0.4034, train F loss: -0.036518, acc 0.999700\n",
      "D_acc: 0.83 7.70s, epoch [1969\\200]: train D loss: 0.3932, train F loss: -0.035988, acc 0.999900\n",
      "D_acc: 0.82 7.70s, epoch [1970\\200]: train D loss: 0.4010, train F loss: -0.036394, acc 0.999900\n",
      "D_acc: 0.82 7.71s, epoch [1971\\200]: train D loss: 0.4064, train F loss: -0.037136, acc 0.999800\n",
      "D_acc: 0.82 7.69s, epoch [1972\\200]: train D loss: 0.4076, train F loss: -0.037206, acc 0.999800\n",
      "D_acc: 0.82 7.71s, epoch [1973\\200]: train D loss: 0.4108, train F loss: -0.036672, acc 0.999800\n",
      "D_acc: 0.82 7.68s, epoch [1974\\200]: train D loss: 0.3999, train F loss: -0.035705, acc 0.999400\n",
      "D_acc: 0.83 7.72s, epoch [1975\\200]: train D loss: 0.3942, train F loss: -0.036174, acc 0.999600\n",
      "D_acc: 0.83 7.74s, epoch [1976\\200]: train D loss: 0.3912, train F loss: -0.035682, acc 0.999700\n",
      "D_acc: 0.83 7.71s, epoch [1977\\200]: train D loss: 0.3938, train F loss: -0.033077, acc 0.998900\n",
      "D_acc: 0.83 7.68s, epoch [1978\\200]: train D loss: 0.3939, train F loss: -0.031857, acc 0.999100\n",
      "D_acc: 0.83 7.70s, epoch [1979\\200]: train D loss: 0.3946, train F loss: -0.036144, acc 0.999900\n",
      "D_acc: 0.83 7.72s, epoch [1980\\200]: train D loss: 0.3976, train F loss: -0.036401, acc 0.999700\n",
      "D_acc: 0.82 7.71s, epoch [1981\\200]: train D loss: 0.4068, train F loss: -0.037329, acc 0.999800\n",
      "D_acc: 0.82 7.68s, epoch [1982\\200]: train D loss: 0.4070, train F loss: -0.037441, acc 0.999600\n",
      "D_acc: 0.82 7.70s, epoch [1983\\200]: train D loss: 0.4097, train F loss: -0.036490, acc 0.999300\n",
      "D_acc: 0.82 7.71s, epoch [1984\\200]: train D loss: 0.4036, train F loss: -0.036844, acc 0.999300\n",
      "D_acc: 0.82 7.74s, epoch [1985\\200]: train D loss: 0.3992, train F loss: -0.036875, acc 0.999700\n",
      "D_acc: 0.82 7.70s, epoch [1986\\200]: train D loss: 0.4078, train F loss: -0.036499, acc 0.999300\n",
      "D_acc: 0.82 7.70s, epoch [1987\\200]: train D loss: 0.4160, train F loss: -0.035563, acc 0.998800\n",
      "D_acc: 0.82 7.67s, epoch [1988\\200]: train D loss: 0.4121, train F loss: -0.032988, acc 0.998400\n",
      "D_acc: 0.82 7.68s, epoch [1989\\200]: train D loss: 0.4035, train F loss: -0.031196, acc 0.998000\n",
      "D_acc: 0.83 7.71s, epoch [1990\\200]: train D loss: 0.4011, train F loss: -0.033522, acc 0.998800\n",
      "D_acc: 0.83 7.72s, epoch [1991\\200]: train D loss: 0.3966, train F loss: -0.035957, acc 0.999400\n",
      "D_acc: 0.83 7.70s, epoch [1992\\200]: train D loss: 0.3891, train F loss: -0.034657, acc 0.999400\n",
      "D_acc: 0.83 7.75s, epoch [1993\\200]: train D loss: 0.3918, train F loss: -0.035307, acc 0.999700\n",
      "D_acc: 0.84 7.73s, epoch [1994\\200]: train D loss: 0.3827, train F loss: -0.033863, acc 0.999300\n",
      "D_acc: 0.84 7.68s, epoch [1995\\200]: train D loss: 0.3954, train F loss: -0.032954, acc 0.998400\n",
      "D_acc: 0.83 7.71s, epoch [1996\\200]: train D loss: 0.3988, train F loss: -0.034192, acc 0.999200\n",
      "D_acc: 0.83 7.69s, epoch [1997\\200]: train D loss: 0.3909, train F loss: -0.034654, acc 0.999300\n",
      "D_acc: 0.82 7.72s, epoch [1998\\200]: train D loss: 0.3939, train F loss: -0.035464, acc 0.999600\n",
      "D_acc: 0.82 7.70s, epoch [1999\\200]: train D loss: 0.3957, train F loss: -0.034652, acc 0.999200\n",
      "D_acc: 0.82 7.71s, epoch [2000\\200]: train D loss: 0.4086, train F loss: -0.035630, acc 0.999400\n",
      "add : 5000 new source size: 10000\n",
      "D_acc: 0.78 7.76s, epoch [2001\\200]: train D loss: 0.5103, train F loss: 0.383947, acc 0.922500\n",
      "D_acc: 0.81 7.70s, epoch [2002\\200]: train D loss: 0.4270, train F loss: 0.094591, acc 0.962800\n",
      "D_acc: 0.81 7.72s, epoch [2003\\200]: train D loss: 0.4155, train F loss: 0.015339, acc 0.984600\n",
      "D_acc: 0.82 7.72s, epoch [2004\\200]: train D loss: 0.4039, train F loss: -0.009636, acc 0.992000\n",
      "D_acc: 0.83 7.71s, epoch [2005\\200]: train D loss: 0.3847, train F loss: -0.018800, acc 0.995300\n",
      "D_acc: 0.84 7.69s, epoch [2006\\200]: train D loss: 0.3685, train F loss: -0.022610, acc 0.996500\n",
      "D_acc: 0.84 7.69s, epoch [2007\\200]: train D loss: 0.3674, train F loss: -0.023767, acc 0.997700\n",
      "D_acc: 0.85 7.71s, epoch [2008\\200]: train D loss: 0.3631, train F loss: -0.026827, acc 0.998400\n",
      "D_acc: 0.84 7.73s, epoch [2009\\200]: train D loss: 0.3791, train F loss: -0.028001, acc 0.998400\n",
      "D_acc: 0.84 7.70s, epoch [2010\\200]: train D loss: 0.3769, train F loss: -0.029374, acc 0.998900\n",
      "D_acc: 0.84 7.70s, epoch [2011\\200]: train D loss: 0.3870, train F loss: -0.030540, acc 0.999000\n",
      "D_acc: 0.84 7.71s, epoch [2012\\200]: train D loss: 0.3729, train F loss: -0.030830, acc 0.999100\n",
      "D_acc: 0.84 7.70s, epoch [2013\\200]: train D loss: 0.3758, train F loss: -0.031087, acc 0.999000\n",
      "D_acc: 0.84 7.70s, epoch [2014\\200]: train D loss: 0.3795, train F loss: -0.032090, acc 0.999500\n",
      "D_acc: 0.84 7.72s, epoch [2015\\200]: train D loss: 0.3778, train F loss: -0.032072, acc 0.999500\n",
      "D_acc: 0.84 7.68s, epoch [2016\\200]: train D loss: 0.3846, train F loss: -0.032903, acc 0.999300\n",
      "D_acc: 0.84 7.72s, epoch [2017\\200]: train D loss: 0.3895, train F loss: -0.032176, acc 0.998800\n",
      "D_acc: 0.83 7.74s, epoch [2018\\200]: train D loss: 0.3936, train F loss: -0.033589, acc 0.999300\n",
      "D_acc: 0.83 7.69s, epoch [2019\\200]: train D loss: 0.3958, train F loss: -0.033226, acc 0.999400\n",
      "D_acc: 0.83 7.71s, epoch [2020\\200]: train D loss: 0.3956, train F loss: -0.030670, acc 0.998200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.83 7.70s, epoch [2021\\200]: train D loss: 0.3958, train F loss: -0.030707, acc 0.997700\n",
      "D_acc: 0.83 7.67s, epoch [2022\\200]: train D loss: 0.3915, train F loss: -0.030907, acc 0.997900\n",
      "D_acc: 0.82 7.70s, epoch [2023\\200]: train D loss: 0.4084, train F loss: -0.028524, acc 0.997200\n",
      "D_acc: 0.83 7.71s, epoch [2024\\200]: train D loss: 0.4000, train F loss: -0.027438, acc 0.997300\n",
      "D_acc: 0.83 7.70s, epoch [2025\\200]: train D loss: 0.3884, train F loss: -0.021283, acc 0.995800\n",
      "D_acc: 0.84 7.68s, epoch [2026\\200]: train D loss: 0.3698, train F loss: -0.025961, acc 0.997200\n",
      "D_acc: 0.85 7.74s, epoch [2027\\200]: train D loss: 0.3549, train F loss: -0.025918, acc 0.998600\n",
      "D_acc: 0.85 7.70s, epoch [2028\\200]: train D loss: 0.3595, train F loss: -0.028921, acc 0.999300\n",
      "D_acc: 0.84 7.66s, epoch [2029\\200]: train D loss: 0.3788, train F loss: -0.031276, acc 0.998500\n",
      "D_acc: 0.83 7.69s, epoch [2030\\200]: train D loss: 0.3834, train F loss: -0.032963, acc 0.999200\n",
      "D_acc: 0.84 7.68s, epoch [2031\\200]: train D loss: 0.3792, train F loss: -0.032492, acc 0.999200\n",
      "D_acc: 0.83 7.71s, epoch [2032\\200]: train D loss: 0.3838, train F loss: -0.033229, acc 0.999500\n",
      "D_acc: 0.83 7.71s, epoch [2033\\200]: train D loss: 0.3954, train F loss: -0.035172, acc 0.999500\n",
      "D_acc: 0.82 7.69s, epoch [2034\\200]: train D loss: 0.3982, train F loss: -0.033772, acc 0.999400\n",
      "D_acc: 0.82 7.75s, epoch [2035\\200]: train D loss: 0.3959, train F loss: -0.034896, acc 0.999400\n",
      "D_acc: 0.82 7.69s, epoch [2036\\200]: train D loss: 0.4047, train F loss: -0.029128, acc 0.997900\n",
      "D_acc: 0.82 7.68s, epoch [2037\\200]: train D loss: 0.4042, train F loss: -0.018792, acc 0.995500\n",
      "D_acc: 0.82 7.70s, epoch [2038\\200]: train D loss: 0.3986, train F loss: -0.026599, acc 0.996800\n",
      "D_acc: 0.83 7.71s, epoch [2039\\200]: train D loss: 0.3864, train F loss: -0.028993, acc 0.997500\n",
      "D_acc: 0.84 7.67s, epoch [2040\\200]: train D loss: 0.3719, train F loss: -0.032412, acc 0.999200\n",
      "D_acc: 0.85 7.72s, epoch [2041\\200]: train D loss: 0.3649, train F loss: -0.032769, acc 0.999600\n",
      "D_acc: 0.83 7.69s, epoch [2042\\200]: train D loss: 0.3933, train F loss: -0.033907, acc 0.999400\n",
      "D_acc: 0.82 7.71s, epoch [2043\\200]: train D loss: 0.4102, train F loss: -0.036417, acc 0.999700\n",
      "D_acc: 0.81 7.71s, epoch [2044\\200]: train D loss: 0.4126, train F loss: -0.037265, acc 0.999800\n",
      "D_acc: 0.82 7.70s, epoch [2045\\200]: train D loss: 0.4007, train F loss: -0.035240, acc 0.999500\n",
      "D_acc: 0.82 7.72s, epoch [2046\\200]: train D loss: 0.3976, train F loss: -0.033503, acc 0.999100\n",
      "D_acc: 0.83 7.66s, epoch [2047\\200]: train D loss: 0.3929, train F loss: -0.035166, acc 0.999700\n",
      "D_acc: 0.82 7.67s, epoch [2048\\200]: train D loss: 0.3990, train F loss: -0.035852, acc 0.999500\n",
      "D_acc: 0.82 7.71s, epoch [2049\\200]: train D loss: 0.4064, train F loss: -0.036206, acc 0.999700\n",
      "D_acc: 0.82 7.71s, epoch [2050\\200]: train D loss: 0.4041, train F loss: -0.036819, acc 0.999700\n",
      "D_acc: 0.81 7.66s, epoch [2051\\200]: train D loss: 0.4113, train F loss: -0.036687, acc 0.999000\n",
      "D_acc: 0.81 7.67s, epoch [2052\\200]: train D loss: 0.4103, train F loss: -0.033634, acc 0.998900\n",
      "D_acc: 0.81 7.73s, epoch [2053\\200]: train D loss: 0.4205, train F loss: -0.035548, acc 0.998800\n",
      "D_acc: 0.81 7.71s, epoch [2054\\200]: train D loss: 0.4230, train F loss: -0.033565, acc 0.998100\n",
      "D_acc: 0.82 7.66s, epoch [2055\\200]: train D loss: 0.4162, train F loss: -0.032712, acc 0.997800\n",
      "D_acc: 0.83 7.75s, epoch [2056\\200]: train D loss: 0.4039, train F loss: -0.033728, acc 0.998000\n",
      "D_acc: 0.83 7.69s, epoch [2057\\200]: train D loss: 0.3916, train F loss: -0.034317, acc 0.998800\n",
      "D_acc: 0.82 7.71s, epoch [2058\\200]: train D loss: 0.3944, train F loss: -0.034489, acc 0.998900\n",
      "D_acc: 0.82 7.68s, epoch [2059\\200]: train D loss: 0.3916, train F loss: -0.034715, acc 0.999000\n",
      "D_acc: 0.83 7.71s, epoch [2060\\200]: train D loss: 0.3936, train F loss: -0.033164, acc 0.998500\n",
      "D_acc: 0.83 7.75s, epoch [2061\\200]: train D loss: 0.3946, train F loss: -0.032986, acc 0.998400\n",
      "D_acc: 0.83 7.67s, epoch [2062\\200]: train D loss: 0.3946, train F loss: -0.033149, acc 0.999100\n",
      "D_acc: 0.82 7.71s, epoch [2063\\200]: train D loss: 0.4015, train F loss: -0.032842, acc 0.998600\n",
      "D_acc: 0.83 7.70s, epoch [2064\\200]: train D loss: 0.4072, train F loss: -0.034452, acc 0.998600\n",
      "D_acc: 0.82 7.72s, epoch [2065\\200]: train D loss: 0.3986, train F loss: -0.036122, acc 0.999400\n",
      "D_acc: 0.82 7.69s, epoch [2066\\200]: train D loss: 0.4037, train F loss: -0.035980, acc 0.999700\n",
      "D_acc: 0.82 7.72s, epoch [2067\\200]: train D loss: 0.4072, train F loss: -0.034104, acc 0.998200\n",
      "D_acc: 0.83 7.70s, epoch [2068\\200]: train D loss: 0.3994, train F loss: -0.033585, acc 0.998900\n",
      "D_acc: 0.83 7.68s, epoch [2069\\200]: train D loss: 0.3922, train F loss: -0.033517, acc 0.999000\n",
      "D_acc: 0.83 7.74s, epoch [2070\\200]: train D loss: 0.3945, train F loss: -0.033526, acc 0.998600\n",
      "D_acc: 0.82 7.70s, epoch [2071\\200]: train D loss: 0.4075, train F loss: -0.035104, acc 0.999200\n",
      "D_acc: 0.81 7.67s, epoch [2072\\200]: train D loss: 0.4122, train F loss: -0.036027, acc 0.999400\n",
      "D_acc: 0.82 7.69s, epoch [2073\\200]: train D loss: 0.4052, train F loss: -0.034223, acc 0.999000\n",
      "D_acc: 0.81 7.72s, epoch [2074\\200]: train D loss: 0.4215, train F loss: -0.036664, acc 0.999300\n",
      "D_acc: 0.81 7.69s, epoch [2075\\200]: train D loss: 0.4178, train F loss: -0.033105, acc 0.999300\n",
      "D_acc: 0.82 7.70s, epoch [2076\\200]: train D loss: 0.4053, train F loss: -0.036238, acc 0.999200\n",
      "D_acc: 0.82 7.71s, epoch [2077\\200]: train D loss: 0.3929, train F loss: -0.035734, acc 0.999200\n",
      "D_acc: 0.82 7.72s, epoch [2078\\200]: train D loss: 0.4055, train F loss: -0.036305, acc 0.999300\n",
      "D_acc: 0.82 7.75s, epoch [2079\\200]: train D loss: 0.4097, train F loss: -0.037563, acc 0.999800\n",
      "D_acc: 0.81 7.70s, epoch [2080\\200]: train D loss: 0.4148, train F loss: -0.037408, acc 0.999500\n",
      "D_acc: 0.82 7.72s, epoch [2081\\200]: train D loss: 0.4053, train F loss: -0.037426, acc 0.999600\n",
      "D_acc: 0.81 7.66s, epoch [2082\\200]: train D loss: 0.4200, train F loss: -0.038267, acc 0.999500\n",
      "D_acc: 0.81 7.71s, epoch [2083\\200]: train D loss: 0.4320, train F loss: -0.039291, acc 0.999600\n",
      "D_acc: 0.80 7.70s, epoch [2084\\200]: train D loss: 0.4327, train F loss: -0.039626, acc 0.999500\n",
      "D_acc: 0.80 7.71s, epoch [2085\\200]: train D loss: 0.4294, train F loss: -0.039552, acc 0.999600\n",
      "D_acc: 0.80 7.66s, epoch [2086\\200]: train D loss: 0.4391, train F loss: -0.039845, acc 0.999500\n",
      "D_acc: 0.80 7.74s, epoch [2087\\200]: train D loss: 0.4250, train F loss: -0.038725, acc 0.999600\n",
      "D_acc: 0.81 7.71s, epoch [2088\\200]: train D loss: 0.4146, train F loss: -0.037838, acc 0.999500\n",
      "D_acc: 0.82 7.67s, epoch [2089\\200]: train D loss: 0.4143, train F loss: -0.036457, acc 0.999100\n",
      "D_acc: 0.81 7.71s, epoch [2090\\200]: train D loss: 0.4187, train F loss: -0.034465, acc 0.998600\n",
      "D_acc: 0.81 7.69s, epoch [2091\\200]: train D loss: 0.4278, train F loss: -0.036304, acc 0.998500\n",
      "D_acc: 0.81 7.72s, epoch [2092\\200]: train D loss: 0.4183, train F loss: -0.034577, acc 0.998200\n",
      "D_acc: 0.82 7.66s, epoch [2093\\200]: train D loss: 0.4075, train F loss: -0.035619, acc 0.999200\n",
      "D_acc: 0.81 7.71s, epoch [2094\\200]: train D loss: 0.4195, train F loss: -0.035686, acc 0.998300\n",
      "D_acc: 0.82 7.69s, epoch [2095\\200]: train D loss: 0.4135, train F loss: -0.035123, acc 0.998900\n",
      "D_acc: 0.82 7.75s, epoch [2096\\200]: train D loss: 0.4199, train F loss: -0.029750, acc 0.997400\n",
      "D_acc: 0.81 7.69s, epoch [2097\\200]: train D loss: 0.4241, train F loss: -0.025819, acc 0.997400\n",
      "D_acc: 0.81 7.71s, epoch [2098\\200]: train D loss: 0.4145, train F loss: -0.032360, acc 0.998100\n",
      "D_acc: 0.82 7.71s, epoch [2099\\200]: train D loss: 0.3953, train F loss: -0.034326, acc 0.999400\n",
      "D_acc: 0.83 7.67s, epoch [2100\\200]: train D loss: 0.3912, train F loss: -0.033778, acc 0.999200\n",
      "D_acc: 0.83 7.71s, epoch [2101\\200]: train D loss: 0.3841, train F loss: -0.031514, acc 0.998600\n",
      "D_acc: 0.83 7.70s, epoch [2102\\200]: train D loss: 0.3910, train F loss: -0.033158, acc 0.998400\n",
      "D_acc: 0.82 7.73s, epoch [2103\\200]: train D loss: 0.4138, train F loss: -0.034958, acc 0.998500\n",
      "D_acc: 0.82 7.70s, epoch [2104\\200]: train D loss: 0.4070, train F loss: -0.035436, acc 0.999200\n",
      "D_acc: 0.82 7.73s, epoch [2105\\200]: train D loss: 0.4037, train F loss: -0.036712, acc 0.999500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.82 7.71s, epoch [2106\\200]: train D loss: 0.4111, train F loss: -0.037565, acc 0.999800\n",
      "D_acc: 0.82 7.70s, epoch [2107\\200]: train D loss: 0.4135, train F loss: -0.037798, acc 0.999500\n",
      "D_acc: 0.81 7.73s, epoch [2108\\200]: train D loss: 0.4179, train F loss: -0.037406, acc 0.999600\n",
      "D_acc: 0.81 7.65s, epoch [2109\\200]: train D loss: 0.4132, train F loss: -0.037567, acc 0.999400\n",
      "D_acc: 0.81 7.66s, epoch [2110\\200]: train D loss: 0.4168, train F loss: -0.037949, acc 0.999600\n",
      "D_acc: 0.81 7.69s, epoch [2111\\200]: train D loss: 0.4108, train F loss: -0.037767, acc 0.999700\n",
      "D_acc: 0.80 7.72s, epoch [2112\\200]: train D loss: 0.4240, train F loss: -0.038505, acc 0.999500\n",
      "D_acc: 0.80 7.73s, epoch [2113\\200]: train D loss: 0.4280, train F loss: -0.038088, acc 0.999400\n",
      "D_acc: 0.81 7.72s, epoch [2114\\200]: train D loss: 0.4234, train F loss: -0.036741, acc 0.998800\n",
      "D_acc: 0.81 7.72s, epoch [2115\\200]: train D loss: 0.4207, train F loss: -0.035558, acc 0.998700\n",
      "D_acc: 0.81 7.66s, epoch [2116\\200]: train D loss: 0.4237, train F loss: -0.032204, acc 0.997900\n",
      "D_acc: 0.81 7.67s, epoch [2117\\200]: train D loss: 0.4202, train F loss: -0.037205, acc 0.999200\n",
      "D_acc: 0.82 7.71s, epoch [2118\\200]: train D loss: 0.4092, train F loss: -0.037712, acc 0.999700\n",
      "D_acc: 0.82 7.72s, epoch [2119\\200]: train D loss: 0.4090, train F loss: -0.035247, acc 0.998800\n",
      "D_acc: 0.81 7.69s, epoch [2120\\200]: train D loss: 0.4217, train F loss: -0.034149, acc 0.998500\n",
      "D_acc: 0.82 7.72s, epoch [2121\\200]: train D loss: 0.4157, train F loss: -0.036637, acc 0.999000\n",
      "D_acc: 0.82 7.74s, epoch [2122\\200]: train D loss: 0.4125, train F loss: -0.037268, acc 0.999200\n",
      "D_acc: 0.82 7.68s, epoch [2123\\200]: train D loss: 0.4098, train F loss: -0.036440, acc 0.999500\n",
      "D_acc: 0.81 7.70s, epoch [2124\\200]: train D loss: 0.4163, train F loss: -0.037764, acc 0.999700\n",
      "D_acc: 0.80 7.69s, epoch [2125\\200]: train D loss: 0.4294, train F loss: -0.034377, acc 0.998000\n",
      "D_acc: 0.80 7.71s, epoch [2126\\200]: train D loss: 0.4419, train F loss: -0.033507, acc 0.997400\n",
      "D_acc: 0.79 7.68s, epoch [2127\\200]: train D loss: 0.4893, train F loss: 0.018006, acc 0.993000\n",
      "D_acc: 0.77 7.71s, epoch [2128\\200]: train D loss: 0.4829, train F loss: -0.033092, acc 0.996300\n",
      "D_acc: 0.79 7.70s, epoch [2129\\200]: train D loss: 0.4375, train F loss: -0.041460, acc 0.999800\n",
      "D_acc: 0.81 7.74s, epoch [2130\\200]: train D loss: 0.4173, train F loss: -0.039752, acc 0.999800\n",
      "D_acc: 0.81 7.75s, epoch [2131\\200]: train D loss: 0.4135, train F loss: -0.038588, acc 0.999800\n",
      "D_acc: 0.81 7.70s, epoch [2132\\200]: train D loss: 0.4191, train F loss: -0.038832, acc 0.999700\n",
      "D_acc: 0.81 7.69s, epoch [2133\\200]: train D loss: 0.4174, train F loss: -0.038054, acc 0.999700\n",
      "D_acc: 0.81 7.70s, epoch [2134\\200]: train D loss: 0.4194, train F loss: -0.039164, acc 0.999600\n",
      "D_acc: 0.81 7.73s, epoch [2135\\200]: train D loss: 0.4167, train F loss: -0.039094, acc 0.999900\n",
      "D_acc: 0.81 7.67s, epoch [2136\\200]: train D loss: 0.4235, train F loss: -0.038301, acc 0.999700\n",
      "D_acc: 0.80 7.71s, epoch [2137\\200]: train D loss: 0.4341, train F loss: -0.039525, acc 0.999600\n",
      "D_acc: 0.80 7.70s, epoch [2138\\200]: train D loss: 0.4302, train F loss: -0.039774, acc 0.999800\n",
      "D_acc: 0.81 7.73s, epoch [2139\\200]: train D loss: 0.4229, train F loss: -0.039385, acc 0.999600\n",
      "D_acc: 0.81 7.71s, epoch [2140\\200]: train D loss: 0.4227, train F loss: -0.039785, acc 0.999900\n",
      "D_acc: 0.81 7.70s, epoch [2141\\200]: train D loss: 0.4244, train F loss: -0.039509, acc 0.999800\n",
      "D_acc: 0.80 7.71s, epoch [2142\\200]: train D loss: 0.4280, train F loss: -0.040110, acc 0.999800\n",
      "D_acc: 0.80 7.70s, epoch [2143\\200]: train D loss: 0.4399, train F loss: -0.041110, acc 0.999800\n",
      "D_acc: 0.79 7.71s, epoch [2144\\200]: train D loss: 0.4519, train F loss: -0.016051, acc 0.998000\n",
      "D_acc: 0.79 7.73s, epoch [2145\\200]: train D loss: 0.4640, train F loss: -0.019726, acc 0.993900\n",
      "D_acc: 0.80 7.67s, epoch [2146\\200]: train D loss: 0.4395, train F loss: -0.035343, acc 0.998100\n",
      "D_acc: 0.81 7.73s, epoch [2147\\200]: train D loss: 0.4088, train F loss: -0.038530, acc 0.999800\n",
      "D_acc: 0.83 7.74s, epoch [2148\\200]: train D loss: 0.3883, train F loss: -0.036908, acc 0.999900\n",
      "D_acc: 0.82 7.70s, epoch [2149\\200]: train D loss: 0.4027, train F loss: -0.037041, acc 0.999500\n",
      "D_acc: 0.82 7.71s, epoch [2150\\200]: train D loss: 0.4090, train F loss: -0.037051, acc 0.999200\n",
      "D_acc: 0.81 7.70s, epoch [2151\\200]: train D loss: 0.4197, train F loss: -0.038005, acc 0.999200\n",
      "D_acc: 0.81 7.69s, epoch [2152\\200]: train D loss: 0.4168, train F loss: -0.038050, acc 0.999300\n",
      "D_acc: 0.81 7.69s, epoch [2153\\200]: train D loss: 0.4167, train F loss: -0.037657, acc 0.999400\n",
      "D_acc: 0.81 7.72s, epoch [2154\\200]: train D loss: 0.4257, train F loss: -0.037879, acc 0.999200\n",
      "D_acc: 0.80 7.71s, epoch [2155\\200]: train D loss: 0.4232, train F loss: -0.038575, acc 0.999200\n",
      "D_acc: 0.81 7.71s, epoch [2156\\200]: train D loss: 0.4210, train F loss: -0.038439, acc 0.999300\n",
      "D_acc: 0.81 7.75s, epoch [2157\\200]: train D loss: 0.4242, train F loss: -0.038306, acc 0.999200\n",
      "D_acc: 0.80 7.66s, epoch [2158\\200]: train D loss: 0.4263, train F loss: -0.038497, acc 0.999100\n",
      "D_acc: 0.81 7.68s, epoch [2159\\200]: train D loss: 0.4254, train F loss: -0.038285, acc 0.999600\n",
      "D_acc: 0.80 7.69s, epoch [2160\\200]: train D loss: 0.4295, train F loss: -0.040576, acc 0.999800\n",
      "D_acc: 0.80 7.73s, epoch [2161\\200]: train D loss: 0.4347, train F loss: -0.040019, acc 0.999600\n",
      "D_acc: 0.80 7.69s, epoch [2162\\200]: train D loss: 0.4288, train F loss: -0.040017, acc 0.999500\n",
      "D_acc: 0.80 7.70s, epoch [2163\\200]: train D loss: 0.4285, train F loss: -0.040017, acc 0.999500\n",
      "D_acc: 0.80 7.71s, epoch [2164\\200]: train D loss: 0.4282, train F loss: -0.039805, acc 0.999500\n",
      "D_acc: 0.80 7.74s, epoch [2165\\200]: train D loss: 0.4282, train F loss: -0.038646, acc 0.998800\n",
      "D_acc: 0.80 7.73s, epoch [2166\\200]: train D loss: 0.4325, train F loss: -0.037708, acc 0.998700\n",
      "D_acc: 0.81 7.72s, epoch [2167\\200]: train D loss: 0.4303, train F loss: -0.038732, acc 0.999300\n",
      "D_acc: 0.80 7.69s, epoch [2168\\200]: train D loss: 0.4326, train F loss: -0.035991, acc 0.998600\n",
      "D_acc: 0.81 7.71s, epoch [2169\\200]: train D loss: 0.4228, train F loss: -0.038846, acc 0.999500\n",
      "D_acc: 0.81 7.72s, epoch [2170\\200]: train D loss: 0.4190, train F loss: -0.037094, acc 0.999000\n",
      "D_acc: 0.81 7.70s, epoch [2171\\200]: train D loss: 0.4211, train F loss: -0.038297, acc 0.999200\n",
      "D_acc: 0.81 7.67s, epoch [2172\\200]: train D loss: 0.4203, train F loss: -0.037380, acc 0.999500\n",
      "D_acc: 0.81 7.77s, epoch [2173\\200]: train D loss: 0.4381, train F loss: -0.039339, acc 0.999500\n",
      "D_acc: 0.80 7.75s, epoch [2174\\200]: train D loss: 0.4325, train F loss: -0.039360, acc 0.999600\n",
      "D_acc: 0.80 7.71s, epoch [2175\\200]: train D loss: 0.4292, train F loss: -0.037862, acc 0.999100\n",
      "D_acc: 0.81 7.71s, epoch [2176\\200]: train D loss: 0.4197, train F loss: -0.039651, acc 0.999800\n",
      "D_acc: 0.81 7.69s, epoch [2177\\200]: train D loss: 0.4216, train F loss: -0.038524, acc 0.999500\n",
      "D_acc: 0.80 7.72s, epoch [2178\\200]: train D loss: 0.4280, train F loss: -0.038364, acc 0.999100\n",
      "D_acc: 0.80 7.70s, epoch [2179\\200]: train D loss: 0.4350, train F loss: -0.039363, acc 0.999400\n",
      "D_acc: 0.80 7.68s, epoch [2180\\200]: train D loss: 0.4297, train F loss: -0.037538, acc 0.999100\n",
      "D_acc: 0.80 7.71s, epoch [2181\\200]: train D loss: 0.4327, train F loss: -0.037496, acc 0.998600\n",
      "D_acc: 0.80 7.70s, epoch [2182\\200]: train D loss: 0.4293, train F loss: -0.038991, acc 0.999300\n",
      "D_acc: 0.81 7.76s, epoch [2183\\200]: train D loss: 0.4239, train F loss: -0.038470, acc 0.999600\n",
      "D_acc: 0.80 7.87s, epoch [2184\\200]: train D loss: 0.4328, train F loss: -0.039082, acc 0.999300\n",
      "D_acc: 0.80 7.72s, epoch [2185\\200]: train D loss: 0.4264, train F loss: -0.038770, acc 0.999400\n",
      "D_acc: 0.81 7.73s, epoch [2186\\200]: train D loss: 0.4211, train F loss: -0.038122, acc 0.999200\n",
      "D_acc: 0.81 7.71s, epoch [2187\\200]: train D loss: 0.4200, train F loss: -0.038209, acc 0.999500\n",
      "D_acc: 0.80 7.71s, epoch [2188\\200]: train D loss: 0.4316, train F loss: -0.039354, acc 0.999400\n",
      "D_acc: 0.80 7.70s, epoch [2189\\200]: train D loss: 0.4432, train F loss: -0.040876, acc 0.999400\n",
      "D_acc: 0.80 7.70s, epoch [2190\\200]: train D loss: 0.4301, train F loss: -0.039612, acc 0.999200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.80 7.75s, epoch [2191\\200]: train D loss: 0.4319, train F loss: -0.039266, acc 0.999400\n",
      "D_acc: 0.81 7.68s, epoch [2192\\200]: train D loss: 0.4281, train F loss: -0.038754, acc 0.999200\n",
      "D_acc: 0.81 7.71s, epoch [2193\\200]: train D loss: 0.4232, train F loss: -0.040074, acc 0.999900\n",
      "D_acc: 0.80 7.71s, epoch [2194\\200]: train D loss: 0.4298, train F loss: -0.039814, acc 0.999700\n",
      "D_acc: 0.80 7.70s, epoch [2195\\200]: train D loss: 0.4315, train F loss: -0.040057, acc 0.999800\n",
      "D_acc: 0.80 7.68s, epoch [2196\\200]: train D loss: 0.4389, train F loss: -0.040685, acc 0.999600\n",
      "D_acc: 0.80 7.71s, epoch [2197\\200]: train D loss: 0.4397, train F loss: -0.038540, acc 0.998700\n",
      "D_acc: 0.80 7.72s, epoch [2198\\200]: train D loss: 0.4352, train F loss: -0.037377, acc 0.998600\n",
      "D_acc: 0.80 7.67s, epoch [2199\\200]: train D loss: 0.4310, train F loss: -0.038019, acc 0.998400\n",
      "D_acc: 0.80 7.72s, epoch [2200\\200]: train D loss: 0.4358, train F loss: -0.040172, acc 0.999300\n",
      "add : 5000 new source size: 10000\n",
      "D_acc: 0.78 7.69s, epoch [2201\\200]: train D loss: 0.5199, train F loss: 0.345932, acc 0.924800\n",
      "D_acc: 0.77 7.69s, epoch [2202\\200]: train D loss: 0.4837, train F loss: 0.085018, acc 0.963300\n",
      "D_acc: 0.79 7.69s, epoch [2203\\200]: train D loss: 0.4504, train F loss: 0.015995, acc 0.983100\n",
      "D_acc: 0.79 7.71s, epoch [2204\\200]: train D loss: 0.4311, train F loss: -0.010444, acc 0.992200\n",
      "D_acc: 0.80 7.68s, epoch [2205\\200]: train D loss: 0.4234, train F loss: -0.018088, acc 0.994100\n",
      "D_acc: 0.81 7.70s, epoch [2206\\200]: train D loss: 0.4044, train F loss: -0.019717, acc 0.995700\n",
      "D_acc: 0.81 7.74s, epoch [2207\\200]: train D loss: 0.4069, train F loss: -0.008469, acc 0.995800\n",
      "D_acc: 0.80 7.68s, epoch [2208\\200]: train D loss: 0.4457, train F loss: -0.017763, acc 0.993600\n",
      "D_acc: 0.81 7.70s, epoch [2209\\200]: train D loss: 0.4092, train F loss: -0.028779, acc 0.996800\n",
      "D_acc: 0.82 7.71s, epoch [2210\\200]: train D loss: 0.3957, train F loss: -0.031903, acc 0.998000\n",
      "D_acc: 0.82 7.70s, epoch [2211\\200]: train D loss: 0.3958, train F loss: -0.032348, acc 0.998100\n",
      "D_acc: 0.81 7.68s, epoch [2212\\200]: train D loss: 0.4088, train F loss: -0.033682, acc 0.998500\n",
      "D_acc: 0.81 7.70s, epoch [2213\\200]: train D loss: 0.4205, train F loss: -0.036004, acc 0.999000\n",
      "D_acc: 0.80 7.71s, epoch [2214\\200]: train D loss: 0.4252, train F loss: -0.036752, acc 0.998900\n",
      "D_acc: 0.80 7.70s, epoch [2215\\200]: train D loss: 0.4289, train F loss: -0.037164, acc 0.998900\n",
      "D_acc: 0.80 7.73s, epoch [2216\\200]: train D loss: 0.4314, train F loss: -0.037559, acc 0.998900\n",
      "D_acc: 0.80 7.71s, epoch [2217\\200]: train D loss: 0.4317, train F loss: -0.037089, acc 0.999000\n",
      "D_acc: 0.80 7.68s, epoch [2218\\200]: train D loss: 0.4333, train F loss: -0.037393, acc 0.999100\n",
      "D_acc: 0.79 7.71s, epoch [2219\\200]: train D loss: 0.4425, train F loss: -0.039049, acc 0.999100\n",
      "D_acc: 0.79 7.71s, epoch [2220\\200]: train D loss: 0.4459, train F loss: -0.039128, acc 0.999100\n",
      "D_acc: 0.79 7.71s, epoch [2221\\200]: train D loss: 0.4416, train F loss: -0.039482, acc 0.999000\n",
      "D_acc: 0.79 7.68s, epoch [2222\\200]: train D loss: 0.4423, train F loss: -0.038214, acc 0.998900\n",
      "D_acc: 0.80 7.70s, epoch [2223\\200]: train D loss: 0.4349, train F loss: -0.038640, acc 0.998700\n",
      "D_acc: 0.80 7.71s, epoch [2224\\200]: train D loss: 0.4361, train F loss: -0.037919, acc 0.998500\n",
      "D_acc: 0.80 7.73s, epoch [2225\\200]: train D loss: 0.4380, train F loss: -0.037951, acc 0.998600\n",
      "D_acc: 0.80 7.69s, epoch [2226\\200]: train D loss: 0.4368, train F loss: -0.037322, acc 0.998400\n",
      "D_acc: 0.80 7.69s, epoch [2227\\200]: train D loss: 0.4370, train F loss: -0.034803, acc 0.997300\n",
      "D_acc: 0.80 7.72s, epoch [2228\\200]: train D loss: 0.4423, train F loss: -0.018198, acc 0.994600\n",
      "D_acc: 0.79 7.67s, epoch [2229\\200]: train D loss: 0.4627, train F loss: -0.007765, acc 0.989900\n",
      "D_acc: 0.80 7.73s, epoch [2230\\200]: train D loss: 0.4440, train F loss: -0.031108, acc 0.996400\n",
      "D_acc: 0.81 7.71s, epoch [2231\\200]: train D loss: 0.4178, train F loss: -0.035534, acc 0.998500\n",
      "D_acc: 0.81 7.68s, epoch [2232\\200]: train D loss: 0.4111, train F loss: -0.036136, acc 0.998800\n",
      "D_acc: 0.81 7.70s, epoch [2233\\200]: train D loss: 0.4129, train F loss: -0.036482, acc 0.999100\n",
      "D_acc: 0.81 7.74s, epoch [2234\\200]: train D loss: 0.4198, train F loss: -0.035603, acc 0.998700\n",
      "D_acc: 0.81 7.69s, epoch [2235\\200]: train D loss: 0.4260, train F loss: -0.037241, acc 0.999000\n",
      "D_acc: 0.81 7.71s, epoch [2236\\200]: train D loss: 0.4195, train F loss: -0.036082, acc 0.998700\n",
      "D_acc: 0.81 7.71s, epoch [2237\\200]: train D loss: 0.4210, train F loss: -0.037215, acc 0.999200\n",
      "D_acc: 0.81 7.71s, epoch [2238\\200]: train D loss: 0.4226, train F loss: -0.037645, acc 0.999400\n",
      "D_acc: 0.81 7.68s, epoch [2239\\200]: train D loss: 0.4203, train F loss: -0.037495, acc 0.999100\n",
      "D_acc: 0.81 7.70s, epoch [2240\\200]: train D loss: 0.4261, train F loss: -0.036938, acc 0.999200\n",
      "D_acc: 0.81 7.70s, epoch [2241\\200]: train D loss: 0.4191, train F loss: -0.037194, acc 0.999100\n",
      "D_acc: 0.81 7.73s, epoch [2242\\200]: train D loss: 0.4218, train F loss: -0.035556, acc 0.998600\n",
      "D_acc: 0.81 7.74s, epoch [2243\\200]: train D loss: 0.4298, train F loss: -0.035556, acc 0.998200\n",
      "D_acc: 0.81 7.71s, epoch [2244\\200]: train D loss: 0.4274, train F loss: -0.028955, acc 0.996900\n",
      "D_acc: 0.82 7.69s, epoch [2245\\200]: train D loss: 0.4188, train F loss: -0.033968, acc 0.998300\n",
      "D_acc: 0.81 7.70s, epoch [2246\\200]: train D loss: 0.4189, train F loss: -0.035102, acc 0.998300\n",
      "D_acc: 0.82 7.72s, epoch [2247\\200]: train D loss: 0.4130, train F loss: -0.036065, acc 0.999100\n",
      "D_acc: 0.81 7.68s, epoch [2248\\200]: train D loss: 0.4198, train F loss: -0.035314, acc 0.998500\n",
      "D_acc: 0.81 7.68s, epoch [2249\\200]: train D loss: 0.4226, train F loss: -0.030799, acc 0.997500\n",
      "D_acc: 0.81 7.69s, epoch [2250\\200]: train D loss: 0.4254, train F loss: -0.027981, acc 0.996400\n",
      "D_acc: 0.82 7.75s, epoch [2251\\200]: train D loss: 0.4185, train F loss: -0.035538, acc 0.998500\n",
      "D_acc: 0.81 7.66s, epoch [2252\\200]: train D loss: 0.4229, train F loss: -0.034487, acc 0.998400\n",
      "D_acc: 0.81 7.72s, epoch [2253\\200]: train D loss: 0.4266, train F loss: -0.037332, acc 0.998800\n",
      "D_acc: 0.81 7.70s, epoch [2254\\200]: train D loss: 0.4182, train F loss: -0.037697, acc 0.999200\n",
      "D_acc: 0.81 7.70s, epoch [2255\\200]: train D loss: 0.4168, train F loss: -0.036040, acc 0.998500\n",
      "D_acc: 0.81 7.66s, epoch [2256\\200]: train D loss: 0.4141, train F loss: -0.037079, acc 0.999200\n",
      "D_acc: 0.81 7.70s, epoch [2257\\200]: train D loss: 0.4160, train F loss: -0.037485, acc 0.998900\n",
      "D_acc: 0.81 7.70s, epoch [2258\\200]: train D loss: 0.4236, train F loss: -0.038111, acc 0.999300\n",
      "D_acc: 0.80 7.70s, epoch [2259\\200]: train D loss: 0.4413, train F loss: -0.039401, acc 0.999200\n",
      "D_acc: 0.80 7.69s, epoch [2260\\200]: train D loss: 0.4323, train F loss: -0.039239, acc 0.999300\n",
      "D_acc: 0.80 7.72s, epoch [2261\\200]: train D loss: 0.4301, train F loss: -0.036878, acc 0.998700\n",
      "D_acc: 0.80 7.75s, epoch [2262\\200]: train D loss: 0.4299, train F loss: -0.037469, acc 0.998600\n",
      "D_acc: 0.80 7.72s, epoch [2263\\200]: train D loss: 0.4297, train F loss: -0.035491, acc 0.998100\n",
      "D_acc: 0.80 7.73s, epoch [2264\\200]: train D loss: 0.4374, train F loss: -0.038477, acc 0.998700\n",
      "D_acc: 0.80 7.72s, epoch [2265\\200]: train D loss: 0.4357, train F loss: -0.037440, acc 0.998100\n",
      "D_acc: 0.80 7.74s, epoch [2266\\200]: train D loss: 0.4381, train F loss: -0.031063, acc 0.996800\n",
      "D_acc: 0.80 7.75s, epoch [2267\\200]: train D loss: 0.4315, train F loss: -0.034678, acc 0.997700\n",
      "D_acc: 0.80 7.74s, epoch [2268\\200]: train D loss: 0.4372, train F loss: -0.037499, acc 0.998600\n",
      "D_acc: 0.81 7.79s, epoch [2269\\200]: train D loss: 0.4280, train F loss: -0.037112, acc 0.998300\n",
      "D_acc: 0.81 7.76s, epoch [2270\\200]: train D loss: 0.4277, train F loss: -0.036226, acc 0.998600\n",
      "D_acc: 0.81 7.71s, epoch [2271\\200]: train D loss: 0.4179, train F loss: -0.038182, acc 0.999500\n",
      "D_acc: 0.81 7.75s, epoch [2272\\200]: train D loss: 0.4212, train F loss: -0.036686, acc 0.999100\n",
      "D_acc: 0.81 7.74s, epoch [2273\\200]: train D loss: 0.4221, train F loss: -0.038244, acc 0.999100\n",
      "D_acc: 0.81 7.76s, epoch [2274\\200]: train D loss: 0.4247, train F loss: -0.038549, acc 0.999400\n",
      "D_acc: 0.81 7.71s, epoch [2275\\200]: train D loss: 0.4241, train F loss: -0.037941, acc 0.999100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.80 7.73s, epoch [2276\\200]: train D loss: 0.4328, train F loss: -0.038495, acc 0.999100\n",
      "D_acc: 0.80 7.76s, epoch [2277\\200]: train D loss: 0.4306, train F loss: -0.039234, acc 0.999400\n",
      "D_acc: 0.80 7.79s, epoch [2278\\200]: train D loss: 0.4340, train F loss: -0.038842, acc 0.998900\n",
      "D_acc: 0.80 7.71s, epoch [2279\\200]: train D loss: 0.4298, train F loss: -0.038313, acc 0.998800\n",
      "D_acc: 0.80 7.73s, epoch [2280\\200]: train D loss: 0.4396, train F loss: -0.039021, acc 0.998800\n",
      "D_acc: 0.79 7.74s, epoch [2281\\200]: train D loss: 0.4424, train F loss: -0.040108, acc 0.999300\n",
      "D_acc: 0.79 7.76s, epoch [2282\\200]: train D loss: 0.4401, train F loss: -0.039783, acc 0.999300\n",
      "D_acc: 0.79 7.74s, epoch [2283\\200]: train D loss: 0.4408, train F loss: -0.040182, acc 0.999200\n",
      "D_acc: 0.79 7.73s, epoch [2284\\200]: train D loss: 0.4393, train F loss: -0.038968, acc 0.998800\n",
      "D_acc: 0.80 7.73s, epoch [2285\\200]: train D loss: 0.4359, train F loss: -0.039558, acc 0.998800\n",
      "D_acc: 0.80 7.74s, epoch [2286\\200]: train D loss: 0.4331, train F loss: -0.037579, acc 0.998100\n",
      "D_acc: 0.80 7.83s, epoch [2287\\200]: train D loss: 0.4379, train F loss: -0.038150, acc 0.998400\n",
      "D_acc: 0.80 7.75s, epoch [2288\\200]: train D loss: 0.4320, train F loss: -0.034075, acc 0.998300\n",
      "D_acc: 0.80 7.68s, epoch [2289\\200]: train D loss: 0.4355, train F loss: -0.037167, acc 0.998200\n",
      "D_acc: 0.80 7.74s, epoch [2290\\200]: train D loss: 0.4362, train F loss: -0.034355, acc 0.998000\n",
      "D_acc: 0.80 7.74s, epoch [2291\\200]: train D loss: 0.4395, train F loss: -0.037308, acc 0.998500\n",
      "D_acc: 0.80 7.73s, epoch [2292\\200]: train D loss: 0.4277, train F loss: -0.038209, acc 0.998900\n",
      "D_acc: 0.80 7.72s, epoch [2293\\200]: train D loss: 0.4253, train F loss: -0.037109, acc 0.999100\n",
      "D_acc: 0.80 7.72s, epoch [2294\\200]: train D loss: 0.4283, train F loss: -0.038671, acc 0.999200\n",
      "D_acc: 0.80 7.76s, epoch [2295\\200]: train D loss: 0.4330, train F loss: -0.037443, acc 0.999200\n",
      "D_acc: 0.80 7.70s, epoch [2296\\200]: train D loss: 0.4436, train F loss: -0.039811, acc 0.999400\n",
      "D_acc: 0.80 7.75s, epoch [2297\\200]: train D loss: 0.4323, train F loss: -0.039159, acc 0.999300\n",
      "D_acc: 0.80 7.76s, epoch [2298\\200]: train D loss: 0.4260, train F loss: -0.039340, acc 0.999400\n",
      "D_acc: 0.80 7.73s, epoch [2299\\200]: train D loss: 0.4265, train F loss: -0.039704, acc 0.999600\n",
      "D_acc: 0.80 7.73s, epoch [2300\\200]: train D loss: 0.4376, train F loss: -0.039961, acc 0.999400\n",
      "D_acc: 0.79 7.75s, epoch [2301\\200]: train D loss: 0.4611, train F loss: -0.003432, acc 0.994800\n",
      "D_acc: 0.77 7.74s, epoch [2302\\200]: train D loss: 0.4894, train F loss: -0.015606, acc 0.992300\n",
      "D_acc: 0.79 7.75s, epoch [2303\\200]: train D loss: 0.4522, train F loss: -0.037445, acc 0.998200\n",
      "D_acc: 0.80 7.77s, epoch [2304\\200]: train D loss: 0.4228, train F loss: -0.038878, acc 0.999400\n",
      "D_acc: 0.81 7.75s, epoch [2305\\200]: train D loss: 0.4211, train F loss: -0.038700, acc 0.999500\n",
      "D_acc: 0.81 7.73s, epoch [2306\\200]: train D loss: 0.4248, train F loss: -0.039135, acc 0.999600\n",
      "D_acc: 0.80 7.74s, epoch [2307\\200]: train D loss: 0.4359, train F loss: -0.040050, acc 0.999400\n",
      "D_acc: 0.80 7.72s, epoch [2308\\200]: train D loss: 0.4384, train F loss: -0.039849, acc 0.999300\n",
      "D_acc: 0.80 7.76s, epoch [2309\\200]: train D loss: 0.4351, train F loss: -0.040207, acc 0.999600\n",
      "D_acc: 0.80 7.76s, epoch [2310\\200]: train D loss: 0.4374, train F loss: -0.039834, acc 0.999300\n",
      "D_acc: 0.79 7.76s, epoch [2311\\200]: train D loss: 0.4409, train F loss: -0.040297, acc 0.999500\n",
      "D_acc: 0.79 7.70s, epoch [2312\\200]: train D loss: 0.4461, train F loss: -0.041318, acc 0.999600\n",
      "D_acc: 0.79 7.78s, epoch [2313\\200]: train D loss: 0.4461, train F loss: -0.041498, acc 0.999400\n",
      "D_acc: 0.79 7.74s, epoch [2314\\200]: train D loss: 0.4428, train F loss: -0.040447, acc 0.999200\n",
      "D_acc: 0.79 7.76s, epoch [2315\\200]: train D loss: 0.4451, train F loss: -0.040298, acc 0.998900\n",
      "D_acc: 0.79 7.73s, epoch [2316\\200]: train D loss: 0.4475, train F loss: -0.038180, acc 0.998500\n",
      "D_acc: 0.79 7.73s, epoch [2317\\200]: train D loss: 0.4541, train F loss: -0.040944, acc 0.999000\n",
      "D_acc: 0.79 7.74s, epoch [2318\\200]: train D loss: 0.4421, train F loss: -0.040664, acc 0.999000\n",
      "D_acc: 0.79 7.76s, epoch [2319\\200]: train D loss: 0.4422, train F loss: -0.040353, acc 0.999300\n",
      "D_acc: 0.79 7.74s, epoch [2320\\200]: train D loss: 0.4425, train F loss: -0.040820, acc 0.999500\n",
      "D_acc: 0.79 7.74s, epoch [2321\\200]: train D loss: 0.4415, train F loss: -0.040784, acc 0.999500\n",
      "D_acc: 0.79 7.76s, epoch [2322\\200]: train D loss: 0.4489, train F loss: -0.041186, acc 0.999400\n",
      "D_acc: 0.79 7.79s, epoch [2323\\200]: train D loss: 0.4561, train F loss: -0.040664, acc 0.998900\n",
      "D_acc: 0.79 7.74s, epoch [2324\\200]: train D loss: 0.4553, train F loss: -0.041496, acc 0.999400\n",
      "D_acc: 0.79 7.75s, epoch [2325\\200]: train D loss: 0.4534, train F loss: -0.040468, acc 0.998900\n",
      "D_acc: 0.79 7.70s, epoch [2326\\200]: train D loss: 0.4512, train F loss: -0.041356, acc 0.999400\n",
      "D_acc: 0.79 7.75s, epoch [2327\\200]: train D loss: 0.4495, train F loss: -0.040483, acc 0.998900\n",
      "D_acc: 0.79 7.75s, epoch [2328\\200]: train D loss: 0.4463, train F loss: -0.038628, acc 0.998700\n",
      "D_acc: 0.79 7.76s, epoch [2329\\200]: train D loss: 0.4535, train F loss: -0.039232, acc 0.998300\n",
      "D_acc: 0.79 7.71s, epoch [2330\\200]: train D loss: 0.4542, train F loss: -0.039042, acc 0.998700\n",
      "D_acc: 0.79 7.78s, epoch [2331\\200]: train D loss: 0.4517, train F loss: -0.041851, acc 0.999500\n",
      "D_acc: 0.79 7.71s, epoch [2332\\200]: train D loss: 0.4535, train F loss: -0.039299, acc 0.998700\n",
      "D_acc: 0.79 7.75s, epoch [2333\\200]: train D loss: 0.4539, train F loss: -0.041339, acc 0.999200\n",
      "D_acc: 0.79 7.72s, epoch [2334\\200]: train D loss: 0.4530, train F loss: -0.039845, acc 0.998800\n",
      "D_acc: 0.79 7.74s, epoch [2335\\200]: train D loss: 0.4562, train F loss: -0.041261, acc 0.999000\n",
      "D_acc: 0.79 7.74s, epoch [2336\\200]: train D loss: 0.4502, train F loss: -0.040723, acc 0.999000\n",
      "D_acc: 0.79 7.74s, epoch [2337\\200]: train D loss: 0.4493, train F loss: -0.040607, acc 0.999000\n",
      "D_acc: 0.79 7.74s, epoch [2338\\200]: train D loss: 0.4494, train F loss: -0.038569, acc 0.998500\n",
      "D_acc: 0.79 7.75s, epoch [2339\\200]: train D loss: 0.4597, train F loss: -0.037119, acc 0.997700\n",
      "D_acc: 0.79 7.78s, epoch [2340\\200]: train D loss: 0.4561, train F loss: -0.038180, acc 0.998200\n",
      "D_acc: 0.79 7.75s, epoch [2341\\200]: train D loss: 0.4511, train F loss: -0.038834, acc 0.998100\n",
      "D_acc: 0.79 7.73s, epoch [2342\\200]: train D loss: 0.4462, train F loss: -0.039961, acc 0.999000\n",
      "D_acc: 0.79 7.73s, epoch [2343\\200]: train D loss: 0.4503, train F loss: -0.039294, acc 0.998700\n",
      "D_acc: 0.79 7.74s, epoch [2344\\200]: train D loss: 0.4446, train F loss: -0.040645, acc 0.999300\n",
      "D_acc: 0.79 7.75s, epoch [2345\\200]: train D loss: 0.4485, train F loss: -0.039141, acc 0.998600\n",
      "D_acc: 0.79 7.76s, epoch [2346\\200]: train D loss: 0.4465, train F loss: -0.040019, acc 0.998800\n",
      "D_acc: 0.79 7.74s, epoch [2347\\200]: train D loss: 0.4503, train F loss: -0.041322, acc 0.999200\n",
      "D_acc: 0.79 7.77s, epoch [2348\\200]: train D loss: 0.4467, train F loss: -0.040392, acc 0.999200\n",
      "D_acc: 0.79 7.74s, epoch [2349\\200]: train D loss: 0.4499, train F loss: -0.040480, acc 0.999100\n",
      "D_acc: 0.79 7.74s, epoch [2350\\200]: train D loss: 0.4494, train F loss: -0.040289, acc 0.998700\n",
      "D_acc: 0.79 7.75s, epoch [2351\\200]: train D loss: 0.4549, train F loss: -0.038539, acc 0.998400\n",
      "D_acc: 0.79 7.73s, epoch [2352\\200]: train D loss: 0.4525, train F loss: -0.035109, acc 0.998400\n",
      "D_acc: 0.78 7.74s, epoch [2353\\200]: train D loss: 0.4579, train F loss: -0.034339, acc 0.996700\n",
      "D_acc: 0.79 7.75s, epoch [2354\\200]: train D loss: 0.4518, train F loss: -0.037111, acc 0.998300\n",
      "D_acc: 0.79 7.76s, epoch [2355\\200]: train D loss: 0.4405, train F loss: -0.039817, acc 0.999100\n",
      "D_acc: 0.80 8.03s, epoch [2356\\200]: train D loss: 0.4347, train F loss: -0.040044, acc 0.999300\n",
      "D_acc: 0.79 8.14s, epoch [2357\\200]: train D loss: 0.4442, train F loss: -0.040752, acc 0.999500\n",
      "D_acc: 0.79 8.15s, epoch [2358\\200]: train D loss: 0.4520, train F loss: -0.041505, acc 0.999400\n",
      "D_acc: 0.79 7.74s, epoch [2359\\200]: train D loss: 0.4555, train F loss: -0.041086, acc 0.999100\n",
      "D_acc: 0.79 7.75s, epoch [2360\\200]: train D loss: 0.4552, train F loss: -0.040430, acc 0.998900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.79 7.72s, epoch [2361\\200]: train D loss: 0.4516, train F loss: -0.042069, acc 0.999400\n",
      "D_acc: 0.79 7.68s, epoch [2362\\200]: train D loss: 0.4502, train F loss: -0.041777, acc 0.999300\n",
      "D_acc: 0.78 7.72s, epoch [2363\\200]: train D loss: 0.4583, train F loss: -0.042264, acc 0.999400\n",
      "D_acc: 0.78 7.70s, epoch [2364\\200]: train D loss: 0.4622, train F loss: -0.039791, acc 0.998700\n",
      "D_acc: 0.78 7.69s, epoch [2365\\200]: train D loss: 0.4927, train F loss: -0.038709, acc 0.997600\n",
      "D_acc: 0.78 7.75s, epoch [2366\\200]: train D loss: 0.4646, train F loss: -0.042365, acc 0.998900\n",
      "D_acc: 0.79 7.70s, epoch [2367\\200]: train D loss: 0.4507, train F loss: -0.039819, acc 0.998500\n",
      "D_acc: 0.79 7.68s, epoch [2368\\200]: train D loss: 0.4481, train F loss: -0.041687, acc 0.999400\n",
      "D_acc: 0.79 7.70s, epoch [2369\\200]: train D loss: 0.4494, train F loss: -0.039663, acc 0.998700\n",
      "D_acc: 0.79 7.74s, epoch [2370\\200]: train D loss: 0.4473, train F loss: -0.041260, acc 0.999400\n",
      "D_acc: 0.79 7.66s, epoch [2371\\200]: train D loss: 0.4615, train F loss: -0.042078, acc 0.999400\n",
      "D_acc: 0.79 7.70s, epoch [2372\\200]: train D loss: 0.4499, train F loss: -0.042056, acc 0.999500\n",
      "D_acc: 0.79 7.71s, epoch [2373\\200]: train D loss: 0.4551, train F loss: -0.041419, acc 0.999200\n",
      "D_acc: 0.79 7.72s, epoch [2374\\200]: train D loss: 0.4541, train F loss: -0.039590, acc 0.998700\n",
      "D_acc: 0.79 7.74s, epoch [2375\\200]: train D loss: 0.4520, train F loss: -0.041347, acc 0.999300\n",
      "D_acc: 0.79 7.73s, epoch [2376\\200]: train D loss: 0.4574, train F loss: -0.041759, acc 0.999100\n",
      "D_acc: 0.78 7.69s, epoch [2377\\200]: train D loss: 0.4654, train F loss: -0.042049, acc 0.998800\n",
      "D_acc: 0.78 7.71s, epoch [2378\\200]: train D loss: 0.4596, train F loss: -0.043153, acc 0.999600\n",
      "D_acc: 0.78 7.70s, epoch [2379\\200]: train D loss: 0.4586, train F loss: -0.042519, acc 0.999100\n",
      "D_acc: 0.78 7.74s, epoch [2380\\200]: train D loss: 0.4499, train F loss: -0.041207, acc 0.999300\n",
      "D_acc: 0.79 7.72s, epoch [2381\\200]: train D loss: 0.4578, train F loss: -0.040494, acc 0.998500\n",
      "D_acc: 0.78 7.72s, epoch [2382\\200]: train D loss: 0.4607, train F loss: -0.042199, acc 0.999000\n",
      "D_acc: 0.78 7.69s, epoch [2383\\200]: train D loss: 0.4592, train F loss: -0.040420, acc 0.998700\n",
      "D_acc: 0.78 7.75s, epoch [2384\\200]: train D loss: 0.4581, train F loss: -0.039872, acc 0.998600\n",
      "D_acc: 0.79 7.73s, epoch [2385\\200]: train D loss: 0.4553, train F loss: -0.041477, acc 0.999200\n",
      "D_acc: 0.79 7.73s, epoch [2386\\200]: train D loss: 0.4539, train F loss: -0.042173, acc 0.999300\n",
      "D_acc: 0.79 7.72s, epoch [2387\\200]: train D loss: 0.4532, train F loss: -0.041141, acc 0.999200\n",
      "D_acc: 0.78 7.70s, epoch [2388\\200]: train D loss: 0.4612, train F loss: -0.041718, acc 0.999100\n",
      "D_acc: 0.78 7.72s, epoch [2389\\200]: train D loss: 0.4582, train F loss: -0.042540, acc 0.999300\n",
      "D_acc: 0.78 7.70s, epoch [2390\\200]: train D loss: 0.4575, train F loss: -0.043028, acc 0.999500\n",
      "D_acc: 0.78 7.72s, epoch [2391\\200]: train D loss: 0.4620, train F loss: -0.042676, acc 0.999000\n",
      "D_acc: 0.78 7.76s, epoch [2392\\200]: train D loss: 0.4593, train F loss: -0.042544, acc 0.999200\n",
      "D_acc: 0.78 7.73s, epoch [2393\\200]: train D loss: 0.4653, train F loss: -0.041755, acc 0.998500\n",
      "D_acc: 0.78 7.72s, epoch [2394\\200]: train D loss: 0.4716, train F loss: -0.041590, acc 0.998700\n",
      "D_acc: 0.78 7.72s, epoch [2395\\200]: train D loss: 0.4712, train F loss: -0.039052, acc 0.998200\n",
      "D_acc: 0.78 7.70s, epoch [2396\\200]: train D loss: 0.4620, train F loss: -0.041216, acc 0.998700\n",
      "D_acc: 0.78 7.75s, epoch [2397\\200]: train D loss: 0.4607, train F loss: -0.042247, acc 0.998900\n",
      "D_acc: 0.78 7.70s, epoch [2398\\200]: train D loss: 0.4571, train F loss: -0.042890, acc 0.999500\n",
      "D_acc: 0.78 7.72s, epoch [2399\\200]: train D loss: 0.4569, train F loss: -0.042833, acc 0.999500\n",
      "D_acc: 0.78 7.73s, epoch [2400\\200]: train D loss: 0.4593, train F loss: -0.042999, acc 0.999600\n",
      "add : 5000 new source size: 10000\n",
      "D_acc: 0.76 7.74s, epoch [2401\\200]: train D loss: 0.5555, train F loss: 0.380995, acc 0.920900\n",
      "D_acc: 0.76 7.71s, epoch [2402\\200]: train D loss: 0.5274, train F loss: 0.096134, acc 0.960000\n",
      "D_acc: 0.76 7.69s, epoch [2403\\200]: train D loss: 0.4996, train F loss: 0.019039, acc 0.980500\n",
      "D_acc: 0.76 7.73s, epoch [2404\\200]: train D loss: 0.4889, train F loss: -0.008926, acc 0.988400\n",
      "D_acc: 0.76 7.72s, epoch [2405\\200]: train D loss: 0.4828, train F loss: -0.018000, acc 0.991700\n",
      "D_acc: 0.77 7.71s, epoch [2406\\200]: train D loss: 0.4803, train F loss: -0.025548, acc 0.993000\n",
      "D_acc: 0.77 7.71s, epoch [2407\\200]: train D loss: 0.4773, train F loss: -0.029649, acc 0.995000\n",
      "D_acc: 0.77 7.69s, epoch [2408\\200]: train D loss: 0.4744, train F loss: -0.030729, acc 0.995600\n",
      "D_acc: 0.77 7.75s, epoch [2409\\200]: train D loss: 0.4696, train F loss: -0.033213, acc 0.996100\n",
      "D_acc: 0.77 7.71s, epoch [2410\\200]: train D loss: 0.4699, train F loss: -0.034815, acc 0.996500\n",
      "D_acc: 0.77 7.71s, epoch [2411\\200]: train D loss: 0.4679, train F loss: -0.034966, acc 0.996400\n",
      "D_acc: 0.78 7.71s, epoch [2412\\200]: train D loss: 0.4665, train F loss: -0.035922, acc 0.996500\n",
      "D_acc: 0.78 7.72s, epoch [2413\\200]: train D loss: 0.4641, train F loss: -0.036407, acc 0.996900\n",
      "D_acc: 0.78 7.72s, epoch [2414\\200]: train D loss: 0.4662, train F loss: -0.035237, acc 0.996700\n",
      "D_acc: 0.78 7.67s, epoch [2415\\200]: train D loss: 0.4634, train F loss: -0.035923, acc 0.996900\n",
      "D_acc: 0.78 7.69s, epoch [2416\\200]: train D loss: 0.4638, train F loss: -0.037315, acc 0.997400\n",
      "D_acc: 0.78 7.70s, epoch [2417\\200]: train D loss: 0.4667, train F loss: -0.037938, acc 0.997500\n",
      "D_acc: 0.77 7.76s, epoch [2418\\200]: train D loss: 0.4750, train F loss: -0.038974, acc 0.997500\n",
      "D_acc: 0.77 7.71s, epoch [2419\\200]: train D loss: 0.4790, train F loss: -0.039345, acc 0.997300\n",
      "D_acc: 0.78 7.71s, epoch [2420\\200]: train D loss: 0.4769, train F loss: -0.039534, acc 0.997400\n",
      "D_acc: 0.78 7.72s, epoch [2421\\200]: train D loss: 0.4708, train F loss: -0.038941, acc 0.997400\n",
      "D_acc: 0.78 7.71s, epoch [2422\\200]: train D loss: 0.4673, train F loss: -0.038752, acc 0.997400\n",
      "D_acc: 0.78 7.70s, epoch [2423\\200]: train D loss: 0.4713, train F loss: -0.038379, acc 0.997100\n",
      "D_acc: 0.78 7.71s, epoch [2424\\200]: train D loss: 0.4720, train F loss: -0.037873, acc 0.996600\n",
      "D_acc: 0.78 7.66s, epoch [2425\\200]: train D loss: 0.4681, train F loss: -0.036531, acc 0.997100\n",
      "D_acc: 0.78 7.69s, epoch [2426\\200]: train D loss: 0.4722, train F loss: -0.039041, acc 0.997300\n",
      "D_acc: 0.78 7.74s, epoch [2427\\200]: train D loss: 0.4712, train F loss: -0.037023, acc 0.996800\n",
      "D_acc: 0.78 7.72s, epoch [2428\\200]: train D loss: 0.4704, train F loss: -0.033201, acc 0.995300\n",
      "D_acc: 0.78 7.72s, epoch [2429\\200]: train D loss: 0.4765, train F loss: -0.020755, acc 0.992600\n",
      "D_acc: 0.78 7.69s, epoch [2430\\200]: train D loss: 0.4758, train F loss: -0.022069, acc 0.992700\n",
      "D_acc: 0.78 7.71s, epoch [2431\\200]: train D loss: 0.4759, train F loss: 0.007744, acc 0.990200\n",
      "D_acc: 0.78 7.71s, epoch [2432\\200]: train D loss: 0.4679, train F loss: -0.024135, acc 0.993200\n",
      "D_acc: 0.79 7.73s, epoch [2433\\200]: train D loss: 0.4551, train F loss: -0.038121, acc 0.998500\n",
      "D_acc: 0.79 7.67s, epoch [2434\\200]: train D loss: 0.4473, train F loss: -0.040016, acc 0.999400\n",
      "D_acc: 0.79 7.73s, epoch [2435\\200]: train D loss: 0.4433, train F loss: -0.040215, acc 0.999400\n",
      "D_acc: 0.79 7.75s, epoch [2436\\200]: train D loss: 0.4457, train F loss: -0.040663, acc 0.999400\n",
      "D_acc: 0.79 7.69s, epoch [2437\\200]: train D loss: 0.4523, train F loss: -0.041369, acc 0.999500\n",
      "D_acc: 0.79 7.71s, epoch [2438\\200]: train D loss: 0.4573, train F loss: -0.041836, acc 0.999400\n",
      "D_acc: 0.78 7.71s, epoch [2439\\200]: train D loss: 0.4638, train F loss: -0.042478, acc 0.999300\n",
      "D_acc: 0.78 7.69s, epoch [2440\\200]: train D loss: 0.4691, train F loss: -0.043242, acc 0.999300\n",
      "D_acc: 0.78 7.71s, epoch [2441\\200]: train D loss: 0.4689, train F loss: -0.043036, acc 0.999200\n",
      "D_acc: 0.78 7.70s, epoch [2442\\200]: train D loss: 0.4595, train F loss: -0.042203, acc 0.999300\n",
      "D_acc: 0.78 7.70s, epoch [2443\\200]: train D loss: 0.4565, train F loss: -0.041882, acc 0.999300\n",
      "D_acc: 0.78 7.69s, epoch [2444\\200]: train D loss: 0.4606, train F loss: -0.042362, acc 0.999300\n",
      "D_acc: 0.78 7.75s, epoch [2445\\200]: train D loss: 0.4640, train F loss: -0.042615, acc 0.999400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.78 7.68s, epoch [2446\\200]: train D loss: 0.4667, train F loss: -0.042580, acc 0.999300\n",
      "D_acc: 0.78 7.70s, epoch [2447\\200]: train D loss: 0.4630, train F loss: -0.042580, acc 0.999200\n",
      "D_acc: 0.78 7.72s, epoch [2448\\200]: train D loss: 0.4658, train F loss: -0.042685, acc 0.999100\n",
      "D_acc: 0.78 7.70s, epoch [2449\\200]: train D loss: 0.4629, train F loss: -0.041946, acc 0.999100\n",
      "D_acc: 0.78 7.69s, epoch [2450\\200]: train D loss: 0.4625, train F loss: -0.041941, acc 0.999100\n",
      "D_acc: 0.78 7.70s, epoch [2451\\200]: train D loss: 0.4650, train F loss: -0.042305, acc 0.999100\n",
      "D_acc: 0.78 7.74s, epoch [2452\\200]: train D loss: 0.4693, train F loss: -0.042590, acc 0.999200\n",
      "D_acc: 0.78 7.67s, epoch [2453\\200]: train D loss: 0.4692, train F loss: -0.042873, acc 0.999100\n",
      "D_acc: 0.78 7.75s, epoch [2454\\200]: train D loss: 0.4653, train F loss: -0.041819, acc 0.999000\n",
      "D_acc: 0.78 7.71s, epoch [2455\\200]: train D loss: 0.4731, train F loss: -0.036900, acc 0.997600\n",
      "D_acc: 0.77 7.68s, epoch [2456\\200]: train D loss: 0.4788, train F loss: -0.031919, acc 0.995800\n",
      "D_acc: 0.78 7.71s, epoch [2457\\200]: train D loss: 0.4731, train F loss: -0.029528, acc 0.994700\n",
      "D_acc: 0.78 7.71s, epoch [2458\\200]: train D loss: 0.4723, train F loss: -0.026780, acc 0.994700\n",
      "D_acc: 0.78 7.73s, epoch [2459\\200]: train D loss: 0.4651, train F loss: -0.029487, acc 0.995800\n",
      "D_acc: 0.79 7.70s, epoch [2460\\200]: train D loss: 0.4549, train F loss: -0.031629, acc 0.996700\n",
      "D_acc: 0.79 7.71s, epoch [2461\\200]: train D loss: 0.4492, train F loss: -0.038273, acc 0.998800\n",
      "D_acc: 0.79 7.71s, epoch [2462\\200]: train D loss: 0.4404, train F loss: -0.040724, acc 0.999700\n",
      "D_acc: 0.80 7.75s, epoch [2463\\200]: train D loss: 0.4349, train F loss: -0.040582, acc 0.999700\n",
      "D_acc: 0.80 7.72s, epoch [2464\\200]: train D loss: 0.4403, train F loss: -0.040941, acc 0.999700\n",
      "D_acc: 0.79 7.68s, epoch [2465\\200]: train D loss: 0.4548, train F loss: -0.042381, acc 0.999700\n",
      "D_acc: 0.78 7.72s, epoch [2466\\200]: train D loss: 0.4609, train F loss: -0.042898, acc 0.999700\n",
      "D_acc: 0.79 7.71s, epoch [2467\\200]: train D loss: 0.4551, train F loss: -0.042545, acc 0.999600\n",
      "D_acc: 0.79 7.68s, epoch [2468\\200]: train D loss: 0.4578, train F loss: -0.042779, acc 0.999700\n",
      "D_acc: 0.79 7.71s, epoch [2469\\200]: train D loss: 0.4538, train F loss: -0.042453, acc 0.999700\n",
      "D_acc: 0.79 7.71s, epoch [2470\\200]: train D loss: 0.4533, train F loss: -0.042510, acc 0.999700\n",
      "D_acc: 0.79 7.73s, epoch [2471\\200]: train D loss: 0.4560, train F loss: -0.042393, acc 0.999500\n",
      "D_acc: 0.79 7.75s, epoch [2472\\200]: train D loss: 0.4587, train F loss: -0.041317, acc 0.999300\n",
      "D_acc: 0.79 7.71s, epoch [2473\\200]: train D loss: 0.4613, train F loss: -0.040551, acc 0.998900\n",
      "D_acc: 0.79 7.70s, epoch [2474\\200]: train D loss: 0.4602, train F loss: -0.035704, acc 0.997600\n",
      "D_acc: 0.78 7.72s, epoch [2475\\200]: train D loss: 0.4728, train F loss: -0.029967, acc 0.996200\n",
      "D_acc: 0.78 7.72s, epoch [2476\\200]: train D loss: 0.4701, train F loss: -0.028578, acc 0.995400\n",
      "D_acc: 0.79 7.70s, epoch [2477\\200]: train D loss: 0.4518, train F loss: -0.035243, acc 0.997900\n",
      "D_acc: 0.79 7.69s, epoch [2478\\200]: train D loss: 0.4439, train F loss: -0.037958, acc 0.998600\n",
      "D_acc: 0.79 7.73s, epoch [2479\\200]: train D loss: 0.4375, train F loss: -0.032377, acc 0.999100\n",
      "D_acc: 0.80 7.68s, epoch [2480\\200]: train D loss: 0.4346, train F loss: -0.036109, acc 0.998600\n",
      "D_acc: 0.80 7.76s, epoch [2481\\200]: train D loss: 0.4348, train F loss: -0.038277, acc 0.999200\n",
      "D_acc: 0.80 7.69s, epoch [2482\\200]: train D loss: 0.4304, train F loss: -0.040257, acc 0.999700\n",
      "D_acc: 0.80 7.69s, epoch [2483\\200]: train D loss: 0.4397, train F loss: -0.040645, acc 0.999600\n",
      "D_acc: 0.79 7.71s, epoch [2484\\200]: train D loss: 0.4565, train F loss: -0.041918, acc 0.999600\n",
      "D_acc: 0.79 7.69s, epoch [2485\\200]: train D loss: 0.4430, train F loss: -0.040845, acc 0.999400\n",
      "D_acc: 0.79 7.71s, epoch [2486\\200]: train D loss: 0.4543, train F loss: -0.041124, acc 0.999500\n",
      "D_acc: 0.79 7.74s, epoch [2487\\200]: train D loss: 0.4492, train F loss: -0.041581, acc 0.999400\n",
      "D_acc: 0.79 7.71s, epoch [2488\\200]: train D loss: 0.4579, train F loss: -0.041822, acc 0.999500\n",
      "D_acc: 0.79 7.69s, epoch [2489\\200]: train D loss: 0.4562, train F loss: -0.042135, acc 0.999600\n",
      "D_acc: 0.79 7.76s, epoch [2490\\200]: train D loss: 0.4508, train F loss: -0.040967, acc 0.999300\n",
      "D_acc: 0.79 7.72s, epoch [2491\\200]: train D loss: 0.4532, train F loss: -0.041758, acc 0.999400\n",
      "D_acc: 0.79 7.74s, epoch [2492\\200]: train D loss: 0.4491, train F loss: -0.041403, acc 0.999500\n",
      "D_acc: 0.79 7.73s, epoch [2493\\200]: train D loss: 0.4490, train F loss: -0.041303, acc 0.999300\n",
      "D_acc: 0.79 7.69s, epoch [2494\\200]: train D loss: 0.4537, train F loss: -0.041472, acc 0.999300\n",
      "D_acc: 0.79 7.73s, epoch [2495\\200]: train D loss: 0.4667, train F loss: -0.038750, acc 0.997900\n",
      "D_acc: 0.78 7.71s, epoch [2496\\200]: train D loss: 0.4705, train F loss: -0.035743, acc 0.996900\n",
      "D_acc: 0.79 7.73s, epoch [2497\\200]: train D loss: 0.4593, train F loss: -0.033332, acc 0.997600\n",
      "D_acc: 0.79 7.74s, epoch [2498\\200]: train D loss: 0.4493, train F loss: -0.038333, acc 0.998000\n",
      "D_acc: 0.80 7.73s, epoch [2499\\200]: train D loss: 0.4443, train F loss: -0.040399, acc 0.998600\n",
      "D_acc: 0.80 7.73s, epoch [2500\\200]: train D loss: 0.4406, train F loss: -0.040542, acc 0.999300\n",
      "D_acc: 0.80 7.75s, epoch [2501\\200]: train D loss: 0.4419, train F loss: -0.040731, acc 0.999400\n",
      "D_acc: 0.80 7.74s, epoch [2502\\200]: train D loss: 0.4458, train F loss: -0.040913, acc 0.999100\n",
      "D_acc: 0.79 7.73s, epoch [2503\\200]: train D loss: 0.4481, train F loss: -0.040027, acc 0.999200\n",
      "D_acc: 0.79 7.73s, epoch [2504\\200]: train D loss: 0.4517, train F loss: -0.042324, acc 0.999700\n",
      "D_acc: 0.79 7.73s, epoch [2505\\200]: train D loss: 0.4562, train F loss: -0.042013, acc 0.999500\n",
      "D_acc: 0.79 7.70s, epoch [2506\\200]: train D loss: 0.4504, train F loss: -0.039116, acc 0.999300\n",
      "D_acc: 0.79 7.72s, epoch [2507\\200]: train D loss: 0.4480, train F loss: -0.040998, acc 0.999200\n",
      "D_acc: 0.80 7.76s, epoch [2508\\200]: train D loss: 0.4443, train F loss: -0.041082, acc 0.999400\n",
      "D_acc: 0.80 7.71s, epoch [2509\\200]: train D loss: 0.4450, train F loss: -0.041212, acc 0.999600\n",
      "D_acc: 0.79 7.72s, epoch [2510\\200]: train D loss: 0.4492, train F loss: -0.040057, acc 0.998900\n",
      "D_acc: 0.79 7.68s, epoch [2511\\200]: train D loss: 0.4546, train F loss: -0.040523, acc 0.999000\n",
      "D_acc: 0.79 7.74s, epoch [2512\\200]: train D loss: 0.4605, train F loss: -0.041464, acc 0.998900\n",
      "D_acc: 0.79 7.73s, epoch [2513\\200]: train D loss: 0.4527, train F loss: -0.039055, acc 0.999200\n",
      "D_acc: 0.80 7.68s, epoch [2514\\200]: train D loss: 0.4434, train F loss: -0.041801, acc 0.999600\n",
      "D_acc: 0.80 7.73s, epoch [2515\\200]: train D loss: 0.4397, train F loss: -0.040868, acc 0.999400\n",
      "D_acc: 0.80 7.72s, epoch [2516\\200]: train D loss: 0.4451, train F loss: -0.037573, acc 0.998700\n",
      "D_acc: 0.79 7.79s, epoch [2517\\200]: train D loss: 0.4473, train F loss: -0.041207, acc 0.999200\n",
      "D_acc: 0.79 7.72s, epoch [2518\\200]: train D loss: 0.4493, train F loss: -0.040920, acc 0.999200\n",
      "D_acc: 0.79 7.73s, epoch [2519\\200]: train D loss: 0.4573, train F loss: -0.041585, acc 0.999000\n",
      "D_acc: 0.78 7.74s, epoch [2520\\200]: train D loss: 0.4558, train F loss: -0.042125, acc 0.999400\n",
      "D_acc: 0.79 7.71s, epoch [2521\\200]: train D loss: 0.4504, train F loss: -0.040601, acc 0.999300\n",
      "D_acc: 0.79 7.73s, epoch [2522\\200]: train D loss: 0.4515, train F loss: -0.036164, acc 0.998000\n",
      "D_acc: 0.79 7.74s, epoch [2523\\200]: train D loss: 0.4502, train F loss: -0.040748, acc 0.999300\n",
      "D_acc: 0.79 7.72s, epoch [2524\\200]: train D loss: 0.4599, train F loss: -0.041063, acc 0.999200\n",
      "D_acc: 0.78 7.72s, epoch [2525\\200]: train D loss: 0.4609, train F loss: -0.040380, acc 0.999000\n",
      "D_acc: 0.78 7.75s, epoch [2526\\200]: train D loss: 0.4762, train F loss: -0.038510, acc 0.997500\n",
      "D_acc: 0.78 7.71s, epoch [2527\\200]: train D loss: 0.4736, train F loss: -0.037517, acc 0.997900\n",
      "D_acc: 0.79 7.72s, epoch [2528\\200]: train D loss: 0.4554, train F loss: -0.039571, acc 0.998900\n",
      "D_acc: 0.79 7.74s, epoch [2529\\200]: train D loss: 0.4462, train F loss: -0.041544, acc 0.999300\n",
      "D_acc: 0.80 7.72s, epoch [2530\\200]: train D loss: 0.4372, train F loss: -0.040506, acc 0.999400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.79 7.71s, epoch [2531\\200]: train D loss: 0.4462, train F loss: -0.039434, acc 0.998800\n",
      "D_acc: 0.79 7.72s, epoch [2532\\200]: train D loss: 0.4494, train F loss: -0.040744, acc 0.999200\n",
      "D_acc: 0.79 7.72s, epoch [2533\\200]: train D loss: 0.4454, train F loss: -0.040539, acc 0.999200\n",
      "D_acc: 0.80 7.73s, epoch [2534\\200]: train D loss: 0.4434, train F loss: -0.040989, acc 0.999300\n",
      "D_acc: 0.79 7.76s, epoch [2535\\200]: train D loss: 0.4447, train F loss: -0.039953, acc 0.999000\n",
      "D_acc: 0.79 7.73s, epoch [2536\\200]: train D loss: 0.4417, train F loss: -0.038604, acc 0.998500\n",
      "D_acc: 0.79 7.72s, epoch [2537\\200]: train D loss: 0.4439, train F loss: -0.038640, acc 0.998800\n",
      "D_acc: 0.80 7.70s, epoch [2538\\200]: train D loss: 0.4422, train F loss: -0.039809, acc 0.999100\n",
      "D_acc: 0.79 7.71s, epoch [2539\\200]: train D loss: 0.4520, train F loss: -0.041794, acc 0.999100\n",
      "D_acc: 0.79 7.72s, epoch [2540\\200]: train D loss: 0.4516, train F loss: -0.042077, acc 0.999500\n",
      "D_acc: 0.79 7.69s, epoch [2541\\200]: train D loss: 0.4610, train F loss: -0.041978, acc 0.999200\n",
      "D_acc: 0.78 7.71s, epoch [2542\\200]: train D loss: 0.4603, train F loss: -0.041346, acc 0.999300\n",
      "D_acc: 0.79 7.75s, epoch [2543\\200]: train D loss: 0.4501, train F loss: -0.041821, acc 0.999400\n",
      "D_acc: 0.79 7.76s, epoch [2544\\200]: train D loss: 0.4458, train F loss: -0.040229, acc 0.999300\n",
      "D_acc: 0.79 7.73s, epoch [2545\\200]: train D loss: 0.4463, train F loss: -0.040942, acc 0.999400\n",
      "D_acc: 0.80 7.71s, epoch [2546\\200]: train D loss: 0.4399, train F loss: -0.041184, acc 0.999600\n",
      "D_acc: 0.79 7.75s, epoch [2547\\200]: train D loss: 0.4460, train F loss: -0.041426, acc 0.999500\n",
      "D_acc: 0.79 7.67s, epoch [2548\\200]: train D loss: 0.4475, train F loss: -0.041379, acc 0.999400\n",
      "D_acc: 0.79 7.72s, epoch [2549\\200]: train D loss: 0.4518, train F loss: -0.040641, acc 0.999100\n",
      "D_acc: 0.79 7.72s, epoch [2550\\200]: train D loss: 0.4518, train F loss: -0.039608, acc 0.998600\n",
      "D_acc: 0.79 7.72s, epoch [2551\\200]: train D loss: 0.4508, train F loss: -0.034382, acc 0.998200\n",
      "D_acc: 0.78 7.72s, epoch [2552\\200]: train D loss: 0.4953, train F loss: -0.011136, acc 0.992400\n",
      "D_acc: 0.78 7.74s, epoch [2553\\200]: train D loss: 0.4759, train F loss: -0.033363, acc 0.996300\n",
      "D_acc: 0.79 7.71s, epoch [2554\\200]: train D loss: 0.4505, train F loss: -0.039129, acc 0.999000\n",
      "D_acc: 0.79 7.73s, epoch [2555\\200]: train D loss: 0.4421, train F loss: -0.040327, acc 0.999300\n",
      "D_acc: 0.79 7.69s, epoch [2556\\200]: train D loss: 0.4438, train F loss: -0.040536, acc 0.999400\n",
      "D_acc: 0.79 7.73s, epoch [2557\\200]: train D loss: 0.4392, train F loss: -0.040326, acc 0.999500\n",
      "D_acc: 0.79 7.73s, epoch [2558\\200]: train D loss: 0.4440, train F loss: -0.041320, acc 0.999500\n",
      "D_acc: 0.79 7.71s, epoch [2559\\200]: train D loss: 0.4439, train F loss: -0.040690, acc 0.999300\n",
      "D_acc: 0.79 7.72s, epoch [2560\\200]: train D loss: 0.4519, train F loss: -0.041352, acc 0.999300\n",
      "D_acc: 0.79 7.71s, epoch [2561\\200]: train D loss: 0.4503, train F loss: -0.040713, acc 0.999000\n",
      "D_acc: 0.79 7.74s, epoch [2562\\200]: train D loss: 0.4466, train F loss: -0.040297, acc 0.998800\n",
      "D_acc: 0.80 7.72s, epoch [2563\\200]: train D loss: 0.4407, train F loss: -0.040875, acc 0.999500\n",
      "D_acc: 0.79 7.67s, epoch [2564\\200]: train D loss: 0.4457, train F loss: -0.040173, acc 0.999500\n",
      "D_acc: 0.79 7.73s, epoch [2565\\200]: train D loss: 0.4489, train F loss: -0.040852, acc 0.998900\n",
      "D_acc: 0.79 7.74s, epoch [2566\\200]: train D loss: 0.4477, train F loss: -0.041151, acc 0.999300\n",
      "D_acc: 0.79 7.69s, epoch [2567\\200]: train D loss: 0.4494, train F loss: -0.041113, acc 0.999100\n",
      "D_acc: 0.79 7.74s, epoch [2568\\200]: train D loss: 0.4536, train F loss: -0.040447, acc 0.999200\n",
      "D_acc: 0.78 7.73s, epoch [2569\\200]: train D loss: 0.4564, train F loss: -0.042194, acc 0.999200\n",
      "D_acc: 0.78 7.73s, epoch [2570\\200]: train D loss: 0.4603, train F loss: -0.042544, acc 0.999400\n",
      "D_acc: 0.78 7.74s, epoch [2571\\200]: train D loss: 0.4575, train F loss: -0.042425, acc 0.999300\n",
      "D_acc: 0.78 7.72s, epoch [2572\\200]: train D loss: 0.4625, train F loss: -0.040218, acc 0.998700\n",
      "D_acc: 0.79 7.71s, epoch [2573\\200]: train D loss: 0.4541, train F loss: -0.042682, acc 0.999400\n",
      "D_acc: 0.79 7.72s, epoch [2574\\200]: train D loss: 0.4540, train F loss: -0.042525, acc 0.999500\n",
      "D_acc: 0.78 7.73s, epoch [2575\\200]: train D loss: 0.4616, train F loss: -0.042632, acc 0.999400\n",
      "D_acc: 0.78 7.71s, epoch [2576\\200]: train D loss: 0.4666, train F loss: -0.043370, acc 0.999200\n",
      "D_acc: 0.78 7.70s, epoch [2577\\200]: train D loss: 0.4681, train F loss: -0.042465, acc 0.999000\n",
      "D_acc: 0.78 7.71s, epoch [2578\\200]: train D loss: 0.4675, train F loss: -0.043629, acc 0.999400\n",
      "D_acc: 0.78 7.73s, epoch [2579\\200]: train D loss: 0.4654, train F loss: -0.042162, acc 0.998900\n",
      "D_acc: 0.78 7.75s, epoch [2580\\200]: train D loss: 0.4660, train F loss: -0.041579, acc 0.999100\n",
      "D_acc: 0.78 7.72s, epoch [2581\\200]: train D loss: 0.4640, train F loss: -0.040800, acc 0.998600\n",
      "D_acc: 0.78 7.70s, epoch [2582\\200]: train D loss: 0.4666, train F loss: -0.038891, acc 0.998200\n",
      "D_acc: 0.78 7.71s, epoch [2583\\200]: train D loss: 0.4628, train F loss: -0.040014, acc 0.997900\n",
      "D_acc: 0.78 7.71s, epoch [2584\\200]: train D loss: 0.4623, train F loss: -0.039626, acc 0.998300\n",
      "D_acc: 0.79 7.73s, epoch [2585\\200]: train D loss: 0.4564, train F loss: -0.041440, acc 0.998900\n",
      "D_acc: 0.79 7.71s, epoch [2586\\200]: train D loss: 0.4555, train F loss: -0.042405, acc 0.999300\n",
      "D_acc: 0.79 7.71s, epoch [2587\\200]: train D loss: 0.4532, train F loss: -0.042011, acc 0.999300\n",
      "D_acc: 0.79 7.73s, epoch [2588\\200]: train D loss: 0.4544, train F loss: -0.039564, acc 0.998900\n",
      "D_acc: 0.78 7.77s, epoch [2589\\200]: train D loss: 0.4636, train F loss: -0.040825, acc 0.998500\n",
      "D_acc: 0.78 7.73s, epoch [2590\\200]: train D loss: 0.4585, train F loss: -0.042469, acc 0.999000\n",
      "D_acc: 0.79 7.73s, epoch [2591\\200]: train D loss: 0.4514, train F loss: -0.041913, acc 0.999200\n",
      "D_acc: 0.78 7.73s, epoch [2592\\200]: train D loss: 0.4613, train F loss: -0.042470, acc 0.999200\n",
      "D_acc: 0.78 7.69s, epoch [2593\\200]: train D loss: 0.4667, train F loss: -0.043328, acc 0.999300\n",
      "D_acc: 0.78 7.72s, epoch [2594\\200]: train D loss: 0.4599, train F loss: -0.042688, acc 0.999100\n",
      "D_acc: 0.78 7.73s, epoch [2595\\200]: train D loss: 0.4584, train F loss: -0.041684, acc 0.999100\n",
      "D_acc: 0.78 7.71s, epoch [2596\\200]: train D loss: 0.4664, train F loss: -0.042492, acc 0.999200\n",
      "D_acc: 0.78 7.73s, epoch [2597\\200]: train D loss: 0.4644, train F loss: -0.040889, acc 0.998800\n",
      "D_acc: 0.79 7.75s, epoch [2598\\200]: train D loss: 0.4562, train F loss: -0.042132, acc 0.998800\n",
      "D_acc: 0.79 7.72s, epoch [2599\\200]: train D loss: 0.4533, train F loss: -0.039196, acc 0.998300\n",
      "D_acc: 0.78 7.73s, epoch [2600\\200]: train D loss: 0.4583, train F loss: -0.040513, acc 0.998900\n",
      "add : 5000 new source size: 10000\n",
      "D_acc: 0.76 7.70s, epoch [2601\\200]: train D loss: 0.5144, train F loss: 0.294197, acc 0.929600\n",
      "D_acc: 0.76 7.71s, epoch [2602\\200]: train D loss: 0.4956, train F loss: 0.092678, acc 0.961300\n",
      "D_acc: 0.77 7.67s, epoch [2603\\200]: train D loss: 0.4738, train F loss: 0.024635, acc 0.979600\n",
      "D_acc: 0.77 7.71s, epoch [2604\\200]: train D loss: 0.4598, train F loss: -0.001174, acc 0.986800\n",
      "D_acc: 0.78 7.69s, epoch [2605\\200]: train D loss: 0.4514, train F loss: -0.009919, acc 0.990700\n",
      "D_acc: 0.78 7.72s, epoch [2606\\200]: train D loss: 0.4458, train F loss: -0.013052, acc 0.991700\n",
      "D_acc: 0.79 7.69s, epoch [2607\\200]: train D loss: 0.4431, train F loss: -0.017896, acc 0.992700\n",
      "D_acc: 0.79 7.69s, epoch [2608\\200]: train D loss: 0.4450, train F loss: -0.020216, acc 0.993300\n",
      "D_acc: 0.78 7.71s, epoch [2609\\200]: train D loss: 0.4515, train F loss: -0.021489, acc 0.993300\n",
      "D_acc: 0.78 7.67s, epoch [2610\\200]: train D loss: 0.4505, train F loss: -0.022848, acc 0.993700\n",
      "D_acc: 0.78 7.65s, epoch [2611\\200]: train D loss: 0.4560, train F loss: -0.024448, acc 0.994400\n",
      "D_acc: 0.78 7.72s, epoch [2612\\200]: train D loss: 0.4520, train F loss: -0.025539, acc 0.994500\n",
      "D_acc: 0.79 7.70s, epoch [2613\\200]: train D loss: 0.4487, train F loss: -0.024320, acc 0.994400\n",
      "D_acc: 0.78 7.70s, epoch [2614\\200]: train D loss: 0.4542, train F loss: -0.024868, acc 0.994500\n",
      "D_acc: 0.78 7.70s, epoch [2615\\200]: train D loss: 0.4559, train F loss: -0.025938, acc 0.994300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.78 7.68s, epoch [2616\\200]: train D loss: 0.4611, train F loss: -0.026678, acc 0.994700\n",
      "D_acc: 0.78 7.69s, epoch [2617\\200]: train D loss: 0.4602, train F loss: -0.026679, acc 0.994600\n",
      "D_acc: 0.78 7.71s, epoch [2618\\200]: train D loss: 0.4608, train F loss: -0.026192, acc 0.994600\n",
      "D_acc: 0.78 7.66s, epoch [2619\\200]: train D loss: 0.4566, train F loss: -0.027391, acc 0.994500\n",
      "D_acc: 0.78 7.68s, epoch [2620\\200]: train D loss: 0.4580, train F loss: -0.027443, acc 0.994700\n",
      "D_acc: 0.78 7.68s, epoch [2621\\200]: train D loss: 0.4618, train F loss: -0.027323, acc 0.994300\n",
      "D_acc: 0.78 7.70s, epoch [2622\\200]: train D loss: 0.4592, train F loss: -0.025533, acc 0.994400\n",
      "D_acc: 0.78 7.67s, epoch [2623\\200]: train D loss: 0.4622, train F loss: -0.025971, acc 0.994600\n",
      "D_acc: 0.78 7.74s, epoch [2624\\200]: train D loss: 0.4572, train F loss: -0.028173, acc 0.994800\n",
      "D_acc: 0.78 7.69s, epoch [2625\\200]: train D loss: 0.4602, train F loss: -0.028965, acc 0.994900\n",
      "D_acc: 0.78 7.71s, epoch [2626\\200]: train D loss: 0.4593, train F loss: -0.029230, acc 0.995300\n",
      "D_acc: 0.78 7.70s, epoch [2627\\200]: train D loss: 0.4648, train F loss: -0.029014, acc 0.995100\n",
      "D_acc: 0.78 7.69s, epoch [2628\\200]: train D loss: 0.4723, train F loss: -0.029403, acc 0.995100\n",
      "D_acc: 0.77 7.69s, epoch [2629\\200]: train D loss: 0.4772, train F loss: -0.029245, acc 0.994800\n",
      "D_acc: 0.77 7.70s, epoch [2630\\200]: train D loss: 0.4806, train F loss: -0.026382, acc 0.994200\n",
      "D_acc: 0.77 7.74s, epoch [2631\\200]: train D loss: 0.4727, train F loss: -0.027944, acc 0.994400\n",
      "D_acc: 0.77 7.69s, epoch [2632\\200]: train D loss: 0.4727, train F loss: -0.026185, acc 0.994300\n",
      "D_acc: 0.78 7.75s, epoch [2633\\200]: train D loss: 0.4702, train F loss: -0.024008, acc 0.992500\n",
      "D_acc: 0.77 7.68s, epoch [2634\\200]: train D loss: 0.4717, train F loss: -0.014564, acc 0.992700\n",
      "D_acc: 0.78 7.70s, epoch [2635\\200]: train D loss: 0.4653, train F loss: -0.016213, acc 0.991200\n",
      "D_acc: 0.78 7.68s, epoch [2636\\200]: train D loss: 0.4633, train F loss: -0.014805, acc 0.992100\n",
      "D_acc: 0.78 7.72s, epoch [2637\\200]: train D loss: 0.4578, train F loss: -0.015416, acc 0.992000\n",
      "D_acc: 0.78 7.70s, epoch [2638\\200]: train D loss: 0.4554, train F loss: -0.023796, acc 0.992800\n",
      "D_acc: 0.78 7.69s, epoch [2639\\200]: train D loss: 0.4575, train F loss: -0.026699, acc 0.994900\n",
      "D_acc: 0.78 7.71s, epoch [2640\\200]: train D loss: 0.4540, train F loss: -0.027657, acc 0.994600\n",
      "D_acc: 0.78 7.68s, epoch [2641\\200]: train D loss: 0.4550, train F loss: -0.028383, acc 0.994600\n",
      "D_acc: 0.78 7.85s, epoch [2642\\200]: train D loss: 0.4528, train F loss: -0.029475, acc 0.995200\n",
      "D_acc: 0.78 7.74s, epoch [2643\\200]: train D loss: 0.4570, train F loss: -0.029058, acc 0.995300\n",
      "D_acc: 0.78 7.72s, epoch [2644\\200]: train D loss: 0.4558, train F loss: -0.029719, acc 0.994800\n",
      "D_acc: 0.78 7.67s, epoch [2645\\200]: train D loss: 0.4593, train F loss: -0.030023, acc 0.995000\n",
      "D_acc: 0.78 7.71s, epoch [2646\\200]: train D loss: 0.4624, train F loss: -0.030419, acc 0.994700\n",
      "D_acc: 0.78 7.71s, epoch [2647\\200]: train D loss: 0.4642, train F loss: -0.030556, acc 0.995100\n",
      "D_acc: 0.78 7.70s, epoch [2648\\200]: train D loss: 0.4676, train F loss: -0.030836, acc 0.995000\n",
      "D_acc: 0.78 7.71s, epoch [2649\\200]: train D loss: 0.4674, train F loss: -0.029314, acc 0.994900\n",
      "D_acc: 0.77 7.71s, epoch [2650\\200]: train D loss: 0.4712, train F loss: -0.019958, acc 0.993200\n",
      "D_acc: 0.78 7.71s, epoch [2651\\200]: train D loss: 0.4754, train F loss: 0.001933, acc 0.991400\n",
      "D_acc: 0.77 7.72s, epoch [2652\\200]: train D loss: 0.4905, train F loss: -0.007041, acc 0.989000\n",
      "D_acc: 0.77 7.70s, epoch [2653\\200]: train D loss: 0.4814, train F loss: -0.020661, acc 0.992200\n",
      "D_acc: 0.78 7.71s, epoch [2654\\200]: train D loss: 0.4645, train F loss: -0.029657, acc 0.995000\n",
      "D_acc: 0.78 7.71s, epoch [2655\\200]: train D loss: 0.4538, train F loss: -0.031398, acc 0.996000\n",
      "D_acc: 0.78 7.72s, epoch [2656\\200]: train D loss: 0.4566, train F loss: -0.029581, acc 0.995800\n",
      "D_acc: 0.78 7.70s, epoch [2657\\200]: train D loss: 0.4550, train F loss: -0.031107, acc 0.995800\n",
      "D_acc: 0.78 7.68s, epoch [2658\\200]: train D loss: 0.4566, train F loss: -0.031552, acc 0.995800\n",
      "D_acc: 0.78 7.70s, epoch [2659\\200]: train D loss: 0.4622, train F loss: -0.031933, acc 0.995800\n",
      "D_acc: 0.78 7.72s, epoch [2660\\200]: train D loss: 0.4651, train F loss: -0.032233, acc 0.995800\n",
      "D_acc: 0.78 7.71s, epoch [2661\\200]: train D loss: 0.4606, train F loss: -0.032099, acc 0.995800\n",
      "D_acc: 0.78 7.70s, epoch [2662\\200]: train D loss: 0.4618, train F loss: -0.031993, acc 0.995900\n",
      "D_acc: 0.78 7.72s, epoch [2663\\200]: train D loss: 0.4610, train F loss: -0.032142, acc 0.995900\n",
      "D_acc: 0.78 7.70s, epoch [2664\\200]: train D loss: 0.4656, train F loss: -0.032561, acc 0.996000\n",
      "D_acc: 0.77 7.71s, epoch [2665\\200]: train D loss: 0.4664, train F loss: -0.032760, acc 0.996000\n",
      "D_acc: 0.77 7.70s, epoch [2666\\200]: train D loss: 0.4678, train F loss: -0.032432, acc 0.995800\n",
      "D_acc: 0.78 7.72s, epoch [2667\\200]: train D loss: 0.4662, train F loss: -0.032080, acc 0.995900\n",
      "D_acc: 0.77 7.68s, epoch [2668\\200]: train D loss: 0.4686, train F loss: -0.032870, acc 0.995900\n",
      "D_acc: 0.77 7.71s, epoch [2669\\200]: train D loss: 0.4689, train F loss: -0.032961, acc 0.996000\n",
      "D_acc: 0.77 7.75s, epoch [2670\\200]: train D loss: 0.4705, train F loss: -0.032981, acc 0.996000\n",
      "D_acc: 0.77 7.68s, epoch [2671\\200]: train D loss: 0.4786, train F loss: -0.033599, acc 0.996000\n",
      "D_acc: 0.77 7.72s, epoch [2672\\200]: train D loss: 0.4752, train F loss: -0.033712, acc 0.996000\n",
      "D_acc: 0.77 7.70s, epoch [2673\\200]: train D loss: 0.4710, train F loss: -0.033478, acc 0.996000\n",
      "D_acc: 0.77 7.70s, epoch [2674\\200]: train D loss: 0.4740, train F loss: -0.033031, acc 0.995800\n",
      "D_acc: 0.77 7.70s, epoch [2675\\200]: train D loss: 0.4740, train F loss: -0.033222, acc 0.995700\n",
      "D_acc: 0.77 7.69s, epoch [2676\\200]: train D loss: 0.4731, train F loss: -0.031210, acc 0.995700\n",
      "D_acc: 0.77 7.70s, epoch [2677\\200]: train D loss: 0.4792, train F loss: -0.031387, acc 0.995000\n",
      "D_acc: 0.77 7.67s, epoch [2678\\200]: train D loss: 0.4794, train F loss: -0.028738, acc 0.994500\n",
      "D_acc: 0.77 7.75s, epoch [2679\\200]: train D loss: 0.4840, train F loss: -0.024439, acc 0.993400\n",
      "D_acc: 0.77 7.69s, epoch [2680\\200]: train D loss: 0.4897, train F loss: -0.028495, acc 0.993700\n",
      "D_acc: 0.77 7.71s, epoch [2681\\200]: train D loss: 0.4854, train F loss: -0.029283, acc 0.994300\n",
      "D_acc: 0.77 7.71s, epoch [2682\\200]: train D loss: 0.4761, train F loss: -0.029561, acc 0.995000\n",
      "D_acc: 0.77 7.71s, epoch [2683\\200]: train D loss: 0.4786, train F loss: -0.030362, acc 0.994900\n",
      "D_acc: 0.77 7.67s, epoch [2684\\200]: train D loss: 0.4820, train F loss: -0.028498, acc 0.994800\n",
      "D_acc: 0.77 7.68s, epoch [2685\\200]: train D loss: 0.4701, train F loss: -0.031557, acc 0.995600\n",
      "D_acc: 0.78 7.70s, epoch [2686\\200]: train D loss: 0.4650, train F loss: -0.033220, acc 0.996200\n",
      "D_acc: 0.78 7.71s, epoch [2687\\200]: train D loss: 0.4637, train F loss: -0.033278, acc 0.996300\n",
      "D_acc: 0.78 7.74s, epoch [2688\\200]: train D loss: 0.4647, train F loss: -0.033017, acc 0.996200\n",
      "D_acc: 0.78 7.70s, epoch [2689\\200]: train D loss: 0.4661, train F loss: -0.033374, acc 0.996200\n",
      "D_acc: 0.78 7.73s, epoch [2690\\200]: train D loss: 0.4708, train F loss: -0.032685, acc 0.996200\n",
      "D_acc: 0.78 7.66s, epoch [2691\\200]: train D loss: 0.4675, train F loss: -0.033645, acc 0.996100\n",
      "D_acc: 0.78 7.71s, epoch [2692\\200]: train D loss: 0.4658, train F loss: -0.032907, acc 0.996200\n",
      "D_acc: 0.78 7.69s, epoch [2693\\200]: train D loss: 0.4708, train F loss: -0.033306, acc 0.996000\n",
      "D_acc: 0.77 7.71s, epoch [2694\\200]: train D loss: 0.4721, train F loss: -0.032806, acc 0.995800\n",
      "D_acc: 0.78 7.67s, epoch [2695\\200]: train D loss: 0.4704, train F loss: -0.032975, acc 0.996000\n",
      "D_acc: 0.77 7.73s, epoch [2696\\200]: train D loss: 0.4735, train F loss: -0.033191, acc 0.995900\n",
      "D_acc: 0.77 7.72s, epoch [2697\\200]: train D loss: 0.4839, train F loss: -0.034338, acc 0.996000\n",
      "D_acc: 0.77 7.66s, epoch [2698\\200]: train D loss: 0.4799, train F loss: -0.032465, acc 0.995600\n",
      "D_acc: 0.78 7.71s, epoch [2699\\200]: train D loss: 0.4742, train F loss: -0.031103, acc 0.995500\n",
      "D_acc: 0.77 7.70s, epoch [2700\\200]: train D loss: 0.4751, train F loss: -0.029186, acc 0.994800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.77 7.72s, epoch [2701\\200]: train D loss: 0.4756, train F loss: -0.033257, acc 0.995300\n",
      "D_acc: 0.77 7.70s, epoch [2702\\200]: train D loss: 0.4725, train F loss: -0.033938, acc 0.996100\n",
      "D_acc: 0.77 7.70s, epoch [2703\\200]: train D loss: 0.4711, train F loss: -0.034843, acc 0.996300\n",
      "D_acc: 0.77 7.69s, epoch [2704\\200]: train D loss: 0.4726, train F loss: -0.035066, acc 0.996400\n",
      "D_acc: 0.77 7.68s, epoch [2705\\200]: train D loss: 0.4750, train F loss: -0.035249, acc 0.996300\n",
      "D_acc: 0.77 7.73s, epoch [2706\\200]: train D loss: 0.4844, train F loss: -0.036529, acc 0.996600\n",
      "D_acc: 0.77 7.69s, epoch [2707\\200]: train D loss: 0.4782, train F loss: -0.035947, acc 0.996500\n",
      "D_acc: 0.77 7.71s, epoch [2708\\200]: train D loss: 0.4738, train F loss: -0.035288, acc 0.996500\n",
      "D_acc: 0.77 7.70s, epoch [2709\\200]: train D loss: 0.4727, train F loss: -0.035488, acc 0.996500\n",
      "D_acc: 0.77 7.72s, epoch [2710\\200]: train D loss: 0.4710, train F loss: -0.035450, acc 0.996500\n",
      "D_acc: 0.77 7.70s, epoch [2711\\200]: train D loss: 0.4783, train F loss: -0.035638, acc 0.996300\n",
      "D_acc: 0.77 7.70s, epoch [2712\\200]: train D loss: 0.4792, train F loss: -0.036111, acc 0.996500\n",
      "D_acc: 0.77 7.69s, epoch [2713\\200]: train D loss: 0.4770, train F loss: -0.036071, acc 0.996500\n",
      "D_acc: 0.77 7.71s, epoch [2714\\200]: train D loss: 0.4793, train F loss: -0.035809, acc 0.996400\n",
      "D_acc: 0.77 7.70s, epoch [2715\\200]: train D loss: 0.4846, train F loss: -0.034785, acc 0.995700\n",
      "D_acc: 0.77 7.71s, epoch [2716\\200]: train D loss: 0.4890, train F loss: -0.032326, acc 0.995600\n",
      "D_acc: 0.77 7.72s, epoch [2717\\200]: train D loss: 0.4799, train F loss: -0.032719, acc 0.995500\n",
      "D_acc: 0.77 7.70s, epoch [2718\\200]: train D loss: 0.4801, train F loss: -0.036954, acc 0.996600\n",
      "D_acc: 0.77 7.69s, epoch [2719\\200]: train D loss: 0.4763, train F loss: -0.036087, acc 0.996200\n",
      "D_acc: 0.77 7.69s, epoch [2720\\200]: train D loss: 0.4766, train F loss: -0.031251, acc 0.995700\n",
      "D_acc: 0.77 7.72s, epoch [2721\\200]: train D loss: 0.4819, train F loss: -0.026408, acc 0.994300\n",
      "D_acc: 0.77 7.66s, epoch [2722\\200]: train D loss: 0.4784, train F loss: -0.035139, acc 0.996300\n",
      "D_acc: 0.78 7.66s, epoch [2723\\200]: train D loss: 0.4698, train F loss: -0.036352, acc 0.996900\n",
      "D_acc: 0.77 7.69s, epoch [2724\\200]: train D loss: 0.4690, train F loss: -0.036809, acc 0.997000\n",
      "D_acc: 0.77 7.75s, epoch [2725\\200]: train D loss: 0.4716, train F loss: -0.037019, acc 0.997100\n",
      "D_acc: 0.77 7.67s, epoch [2726\\200]: train D loss: 0.4732, train F loss: -0.037060, acc 0.997100\n",
      "D_acc: 0.77 7.69s, epoch [2727\\200]: train D loss: 0.4726, train F loss: -0.037239, acc 0.997100\n",
      "D_acc: 0.77 7.69s, epoch [2728\\200]: train D loss: 0.4768, train F loss: -0.037680, acc 0.997100\n",
      "D_acc: 0.77 7.73s, epoch [2729\\200]: train D loss: 0.4806, train F loss: -0.037686, acc 0.997100\n",
      "D_acc: 0.77 7.66s, epoch [2730\\200]: train D loss: 0.4843, train F loss: -0.037504, acc 0.996700\n",
      "D_acc: 0.77 7.67s, epoch [2731\\200]: train D loss: 0.4799, train F loss: -0.037801, acc 0.996800\n",
      "D_acc: 0.77 7.69s, epoch [2732\\200]: train D loss: 0.4789, train F loss: -0.037123, acc 0.997100\n",
      "D_acc: 0.77 7.72s, epoch [2733\\200]: train D loss: 0.4800, train F loss: -0.037434, acc 0.996800\n",
      "D_acc: 0.77 7.73s, epoch [2734\\200]: train D loss: 0.4787, train F loss: -0.035762, acc 0.996500\n",
      "D_acc: 0.77 7.71s, epoch [2735\\200]: train D loss: 0.4823, train F loss: -0.036566, acc 0.996600\n",
      "D_acc: 0.77 7.70s, epoch [2736\\200]: train D loss: 0.4800, train F loss: -0.037290, acc 0.996600\n",
      "D_acc: 0.77 7.69s, epoch [2737\\200]: train D loss: 0.4809, train F loss: -0.037104, acc 0.996700\n",
      "D_acc: 0.77 7.69s, epoch [2738\\200]: train D loss: 0.4775, train F loss: -0.035877, acc 0.996800\n",
      "D_acc: 0.77 7.71s, epoch [2739\\200]: train D loss: 0.4760, train F loss: -0.034550, acc 0.996600\n",
      "D_acc: 0.77 7.72s, epoch [2740\\200]: train D loss: 0.4772, train F loss: -0.036695, acc 0.996600\n",
      "D_acc: 0.77 7.67s, epoch [2741\\200]: train D loss: 0.4784, train F loss: -0.027734, acc 0.996200\n",
      "D_acc: 0.77 7.72s, epoch [2742\\200]: train D loss: 0.4956, train F loss: -0.021804, acc 0.992300\n",
      "D_acc: 0.77 7.76s, epoch [2743\\200]: train D loss: 0.4855, train F loss: -0.034671, acc 0.996100\n",
      "D_acc: 0.77 7.68s, epoch [2744\\200]: train D loss: 0.4764, train F loss: -0.034593, acc 0.996800\n",
      "D_acc: 0.77 7.72s, epoch [2745\\200]: train D loss: 0.4706, train F loss: -0.037069, acc 0.997200\n",
      "D_acc: 0.77 7.69s, epoch [2746\\200]: train D loss: 0.4709, train F loss: -0.036957, acc 0.997000\n",
      "D_acc: 0.77 7.71s, epoch [2747\\200]: train D loss: 0.4697, train F loss: -0.036991, acc 0.997000\n",
      "D_acc: 0.78 7.72s, epoch [2748\\200]: train D loss: 0.4674, train F loss: -0.037492, acc 0.997300\n",
      "D_acc: 0.77 7.70s, epoch [2749\\200]: train D loss: 0.4696, train F loss: -0.037440, acc 0.997300\n",
      "D_acc: 0.77 7.71s, epoch [2750\\200]: train D loss: 0.4729, train F loss: -0.037844, acc 0.997300\n",
      "D_acc: 0.77 7.67s, epoch [2751\\200]: train D loss: 0.4743, train F loss: -0.038035, acc 0.997300\n",
      "D_acc: 0.77 7.73s, epoch [2752\\200]: train D loss: 0.4737, train F loss: -0.038251, acc 0.997300\n",
      "D_acc: 0.77 7.70s, epoch [2753\\200]: train D loss: 0.4746, train F loss: -0.038014, acc 0.997200\n",
      "D_acc: 0.77 7.71s, epoch [2754\\200]: train D loss: 0.4762, train F loss: -0.038591, acc 0.997400\n",
      "D_acc: 0.77 7.70s, epoch [2755\\200]: train D loss: 0.4711, train F loss: -0.038042, acc 0.997300\n",
      "D_acc: 0.77 7.72s, epoch [2756\\200]: train D loss: 0.4725, train F loss: -0.037635, acc 0.997300\n",
      "D_acc: 0.77 7.70s, epoch [2757\\200]: train D loss: 0.4730, train F loss: -0.038291, acc 0.997400\n",
      "D_acc: 0.77 7.69s, epoch [2758\\200]: train D loss: 0.4804, train F loss: -0.038753, acc 0.997400\n",
      "D_acc: 0.77 7.72s, epoch [2759\\200]: train D loss: 0.4835, train F loss: -0.038411, acc 0.997200\n",
      "D_acc: 0.77 7.70s, epoch [2760\\200]: train D loss: 0.4854, train F loss: -0.038952, acc 0.997300\n",
      "D_acc: 0.77 7.74s, epoch [2761\\200]: train D loss: 0.4860, train F loss: -0.039098, acc 0.997200\n",
      "D_acc: 0.77 7.69s, epoch [2762\\200]: train D loss: 0.4834, train F loss: -0.039083, acc 0.997300\n",
      "D_acc: 0.77 7.71s, epoch [2763\\200]: train D loss: 0.4799, train F loss: -0.038893, acc 0.997400\n",
      "D_acc: 0.77 7.72s, epoch [2764\\200]: train D loss: 0.4837, train F loss: -0.038888, acc 0.997300\n",
      "D_acc: 0.77 7.70s, epoch [2765\\200]: train D loss: 0.4802, train F loss: -0.038490, acc 0.997300\n",
      "D_acc: 0.77 7.71s, epoch [2766\\200]: train D loss: 0.4841, train F loss: -0.037581, acc 0.997200\n",
      "D_acc: 0.77 7.70s, epoch [2767\\200]: train D loss: 0.4837, train F loss: -0.037116, acc 0.997000\n",
      "D_acc: 0.77 7.68s, epoch [2768\\200]: train D loss: 0.4839, train F loss: -0.032775, acc 0.996100\n",
      "D_acc: 0.77 7.69s, epoch [2769\\200]: train D loss: 0.4848, train F loss: -0.033102, acc 0.995800\n",
      "D_acc: 0.77 7.76s, epoch [2770\\200]: train D loss: 0.4824, train F loss: -0.034590, acc 0.996600\n",
      "D_acc: 0.77 7.67s, epoch [2771\\200]: train D loss: 0.4857, train F loss: -0.038386, acc 0.997000\n",
      "D_acc: 0.77 7.71s, epoch [2772\\200]: train D loss: 0.4793, train F loss: -0.038633, acc 0.997100\n",
      "D_acc: 0.77 7.72s, epoch [2773\\200]: train D loss: 0.4804, train F loss: -0.039044, acc 0.997400\n",
      "D_acc: 0.77 7.70s, epoch [2774\\200]: train D loss: 0.4808, train F loss: -0.039158, acc 0.997400\n",
      "D_acc: 0.77 7.71s, epoch [2775\\200]: train D loss: 0.4800, train F loss: -0.038545, acc 0.997200\n",
      "D_acc: 0.77 7.72s, epoch [2776\\200]: train D loss: 0.4825, train F loss: -0.039342, acc 0.997400\n",
      "D_acc: 0.77 7.71s, epoch [2777\\200]: train D loss: 0.4871, train F loss: -0.039448, acc 0.997400\n",
      "D_acc: 0.77 7.71s, epoch [2778\\200]: train D loss: 0.4865, train F loss: -0.039834, acc 0.997400\n",
      "D_acc: 0.77 7.74s, epoch [2779\\200]: train D loss: 0.4891, train F loss: -0.039705, acc 0.997300\n",
      "D_acc: 0.77 7.72s, epoch [2780\\200]: train D loss: 0.4903, train F loss: -0.039368, acc 0.997200\n",
      "D_acc: 0.77 7.71s, epoch [2781\\200]: train D loss: 0.4881, train F loss: -0.039687, acc 0.997200\n",
      "D_acc: 0.77 7.71s, epoch [2782\\200]: train D loss: 0.4846, train F loss: -0.039400, acc 0.997300\n",
      "D_acc: 0.77 7.68s, epoch [2783\\200]: train D loss: 0.4863, train F loss: -0.039907, acc 0.997400\n",
      "D_acc: 0.77 7.71s, epoch [2784\\200]: train D loss: 0.4868, train F loss: -0.037903, acc 0.996700\n",
      "D_acc: 0.77 7.72s, epoch [2785\\200]: train D loss: 0.4918, train F loss: -0.037811, acc 0.996800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.77 7.70s, epoch [2786\\200]: train D loss: 0.4898, train F loss: -0.039361, acc 0.997200\n",
      "D_acc: 0.77 7.68s, epoch [2787\\200]: train D loss: 0.4874, train F loss: -0.039547, acc 0.997200\n",
      "D_acc: 0.77 7.68s, epoch [2788\\200]: train D loss: 0.4856, train F loss: -0.039134, acc 0.997100\n",
      "D_acc: 0.77 7.75s, epoch [2789\\200]: train D loss: 0.4872, train F loss: -0.039361, acc 0.997300\n",
      "D_acc: 0.77 7.69s, epoch [2790\\200]: train D loss: 0.4873, train F loss: -0.037432, acc 0.996800\n",
      "D_acc: 0.77 7.71s, epoch [2791\\200]: train D loss: 0.4891, train F loss: -0.039035, acc 0.997100\n",
      "D_acc: 0.77 7.71s, epoch [2792\\200]: train D loss: 0.4873, train F loss: -0.037674, acc 0.996700\n",
      "D_acc: 0.77 7.70s, epoch [2793\\200]: train D loss: 0.4820, train F loss: -0.037188, acc 0.996700\n",
      "D_acc: 0.77 7.72s, epoch [2794\\200]: train D loss: 0.4890, train F loss: -0.034181, acc 0.996000\n",
      "D_acc: 0.76 7.70s, epoch [2795\\200]: train D loss: 0.4929, train F loss: -0.036106, acc 0.996400\n",
      "D_acc: 0.76 7.72s, epoch [2796\\200]: train D loss: 0.4915, train F loss: -0.038679, acc 0.996900\n",
      "D_acc: 0.77 7.71s, epoch [2797\\200]: train D loss: 0.4865, train F loss: -0.038158, acc 0.997000\n",
      "D_acc: 0.77 7.72s, epoch [2798\\200]: train D loss: 0.4804, train F loss: -0.039585, acc 0.997600\n",
      "D_acc: 0.77 7.72s, epoch [2799\\200]: train D loss: 0.4788, train F loss: -0.039235, acc 0.997400\n",
      "D_acc: 0.77 7.71s, epoch [2800\\200]: train D loss: 0.4787, train F loss: -0.039071, acc 0.997300\n",
      "add : 5000 new source size: 10000\n",
      "D_acc: 0.76 7.69s, epoch [2801\\200]: train D loss: 0.5315, train F loss: 0.378120, acc 0.923400\n",
      "D_acc: 0.76 7.67s, epoch [2802\\200]: train D loss: 0.5225, train F loss: 0.107000, acc 0.960400\n",
      "D_acc: 0.76 7.71s, epoch [2803\\200]: train D loss: 0.5124, train F loss: 0.028836, acc 0.981600\n",
      "D_acc: 0.76 7.71s, epoch [2804\\200]: train D loss: 0.5073, train F loss: 0.002488, acc 0.988100\n",
      "D_acc: 0.76 7.76s, epoch [2805\\200]: train D loss: 0.5057, train F loss: -0.002449, acc 0.989200\n",
      "D_acc: 0.76 7.69s, epoch [2806\\200]: train D loss: 0.5022, train F loss: -0.012566, acc 0.991600\n",
      "D_acc: 0.75 7.72s, epoch [2807\\200]: train D loss: 0.5054, train F loss: -0.017176, acc 0.992800\n",
      "D_acc: 0.76 7.68s, epoch [2808\\200]: train D loss: 0.5050, train F loss: -0.020301, acc 0.993300\n",
      "D_acc: 0.76 7.70s, epoch [2809\\200]: train D loss: 0.5063, train F loss: -0.019755, acc 0.993600\n",
      "D_acc: 0.76 7.70s, epoch [2810\\200]: train D loss: 0.5068, train F loss: -0.013488, acc 0.993900\n",
      "D_acc: 0.75 7.70s, epoch [2811\\200]: train D loss: 0.5127, train F loss: -0.024441, acc 0.994200\n",
      "D_acc: 0.76 7.71s, epoch [2812\\200]: train D loss: 0.5076, train F loss: -0.024676, acc 0.994200\n",
      "D_acc: 0.76 7.69s, epoch [2813\\200]: train D loss: 0.5046, train F loss: -0.014904, acc 0.994200\n",
      "D_acc: 0.76 7.76s, epoch [2814\\200]: train D loss: 0.5032, train F loss: -0.020112, acc 0.994100\n",
      "D_acc: 0.76 7.67s, epoch [2815\\200]: train D loss: 0.5013, train F loss: -0.018156, acc 0.993500\n",
      "D_acc: 0.76 7.71s, epoch [2816\\200]: train D loss: 0.4990, train F loss: -0.027558, acc 0.994100\n",
      "D_acc: 0.76 7.72s, epoch [2817\\200]: train D loss: 0.4995, train F loss: -0.034356, acc 0.995600\n",
      "D_acc: 0.76 7.70s, epoch [2818\\200]: train D loss: 0.5002, train F loss: -0.034213, acc 0.995900\n",
      "D_acc: 0.76 7.71s, epoch [2819\\200]: train D loss: 0.5000, train F loss: -0.035147, acc 0.995900\n",
      "D_acc: 0.76 7.70s, epoch [2820\\200]: train D loss: 0.4996, train F loss: -0.036603, acc 0.996400\n",
      "D_acc: 0.76 7.72s, epoch [2821\\200]: train D loss: 0.4990, train F loss: -0.037517, acc 0.996400\n",
      "D_acc: 0.76 7.72s, epoch [2822\\200]: train D loss: 0.5010, train F loss: -0.038073, acc 0.996500\n",
      "D_acc: 0.76 7.71s, epoch [2823\\200]: train D loss: 0.5023, train F loss: -0.036884, acc 0.996600\n",
      "D_acc: 0.76 7.76s, epoch [2824\\200]: train D loss: 0.5020, train F loss: -0.038950, acc 0.996800\n",
      "D_acc: 0.76 7.71s, epoch [2825\\200]: train D loss: 0.5029, train F loss: -0.038713, acc 0.996500\n",
      "D_acc: 0.76 7.69s, epoch [2826\\200]: train D loss: 0.5085, train F loss: -0.039759, acc 0.997000\n",
      "D_acc: 0.76 7.69s, epoch [2827\\200]: train D loss: 0.5104, train F loss: -0.040179, acc 0.996800\n",
      "D_acc: 0.76 7.70s, epoch [2828\\200]: train D loss: 0.5059, train F loss: -0.040359, acc 0.996900\n",
      "D_acc: 0.76 7.72s, epoch [2829\\200]: train D loss: 0.5065, train F loss: -0.039833, acc 0.997100\n",
      "D_acc: 0.76 7.70s, epoch [2830\\200]: train D loss: 0.5054, train F loss: -0.040532, acc 0.997100\n",
      "D_acc: 0.76 7.70s, epoch [2831\\200]: train D loss: 0.5105, train F loss: -0.039086, acc 0.997100\n",
      "D_acc: 0.76 7.70s, epoch [2832\\200]: train D loss: 0.5111, train F loss: -0.041334, acc 0.997000\n",
      "D_acc: 0.76 7.75s, epoch [2833\\200]: train D loss: 0.5060, train F loss: -0.041167, acc 0.997400\n",
      "D_acc: 0.76 7.71s, epoch [2834\\200]: train D loss: 0.5058, train F loss: -0.041012, acc 0.997300\n",
      "D_acc: 0.76 7.70s, epoch [2835\\200]: train D loss: 0.5053, train F loss: -0.040468, acc 0.997400\n",
      "D_acc: 0.76 7.72s, epoch [2836\\200]: train D loss: 0.5065, train F loss: -0.041233, acc 0.997500\n",
      "D_acc: 0.76 7.66s, epoch [2837\\200]: train D loss: 0.5068, train F loss: -0.040870, acc 0.997400\n",
      "D_acc: 0.76 7.72s, epoch [2838\\200]: train D loss: 0.5052, train F loss: -0.040224, acc 0.997100\n",
      "D_acc: 0.76 7.70s, epoch [2839\\200]: train D loss: 0.5055, train F loss: -0.040951, acc 0.997400\n",
      "D_acc: 0.76 7.70s, epoch [2840\\200]: train D loss: 0.5047, train F loss: -0.041078, acc 0.997400\n",
      "D_acc: 0.76 7.71s, epoch [2841\\200]: train D loss: 0.5048, train F loss: -0.041525, acc 0.997500\n",
      "D_acc: 0.76 7.74s, epoch [2842\\200]: train D loss: 0.5068, train F loss: -0.041712, acc 0.997500\n",
      "D_acc: 0.76 7.72s, epoch [2843\\200]: train D loss: 0.5064, train F loss: -0.041932, acc 0.997600\n",
      "D_acc: 0.76 7.69s, epoch [2844\\200]: train D loss: 0.5073, train F loss: -0.041974, acc 0.997700\n",
      "D_acc: 0.76 7.70s, epoch [2845\\200]: train D loss: 0.5073, train F loss: -0.042013, acc 0.997600\n",
      "D_acc: 0.76 7.71s, epoch [2846\\200]: train D loss: 0.5072, train F loss: -0.042126, acc 0.997700\n",
      "D_acc: 0.76 7.70s, epoch [2847\\200]: train D loss: 0.5064, train F loss: -0.041943, acc 0.997600\n",
      "D_acc: 0.76 7.71s, epoch [2848\\200]: train D loss: 0.5073, train F loss: -0.042269, acc 0.997700\n",
      "D_acc: 0.76 7.71s, epoch [2849\\200]: train D loss: 0.5080, train F loss: -0.042282, acc 0.997600\n",
      "D_acc: 0.76 7.68s, epoch [2850\\200]: train D loss: 0.5076, train F loss: -0.042395, acc 0.997700\n",
      "D_acc: 0.76 7.69s, epoch [2851\\200]: train D loss: 0.5082, train F loss: -0.042556, acc 0.997800\n",
      "D_acc: 0.76 7.75s, epoch [2852\\200]: train D loss: 0.5078, train F loss: -0.042596, acc 0.997800\n",
      "D_acc: 0.76 7.70s, epoch [2853\\200]: train D loss: 0.5093, train F loss: -0.042729, acc 0.997800\n",
      "D_acc: 0.76 7.70s, epoch [2854\\200]: train D loss: 0.5088, train F loss: -0.042691, acc 0.997800\n",
      "D_acc: 0.76 7.72s, epoch [2855\\200]: train D loss: 0.5114, train F loss: -0.042857, acc 0.997800\n",
      "D_acc: 0.76 7.70s, epoch [2856\\200]: train D loss: 0.5125, train F loss: -0.043100, acc 0.997800\n",
      "D_acc: 0.76 7.67s, epoch [2857\\200]: train D loss: 0.5116, train F loss: -0.042993, acc 0.997800\n",
      "D_acc: 0.76 7.70s, epoch [2858\\200]: train D loss: 0.5085, train F loss: -0.042673, acc 0.997700\n",
      "D_acc: 0.76 7.74s, epoch [2859\\200]: train D loss: 0.5076, train F loss: -0.042438, acc 0.997700\n",
      "D_acc: 0.76 7.66s, epoch [2860\\200]: train D loss: 0.5105, train F loss: -0.042537, acc 0.997700\n",
      "D_acc: 0.76 7.72s, epoch [2861\\200]: train D loss: 0.5124, train F loss: -0.041861, acc 0.997500\n",
      "D_acc: 0.76 7.71s, epoch [2862\\200]: train D loss: 0.5121, train F loss: -0.029907, acc 0.994800\n",
      "D_acc: 0.76 7.73s, epoch [2863\\200]: train D loss: 0.5282, train F loss: 0.021734, acc 0.980500\n",
      "D_acc: 0.76 7.71s, epoch [2864\\200]: train D loss: 0.5188, train F loss: 0.001685, acc 0.985400\n",
      "D_acc: 0.76 7.71s, epoch [2865\\200]: train D loss: 0.5117, train F loss: -0.024469, acc 0.994300\n",
      "D_acc: 0.76 7.72s, epoch [2866\\200]: train D loss: 0.5057, train F loss: -0.036978, acc 0.995900\n",
      "D_acc: 0.76 7.68s, epoch [2867\\200]: train D loss: 0.5038, train F loss: -0.037807, acc 0.997000\n",
      "D_acc: 0.76 7.71s, epoch [2868\\200]: train D loss: 0.5070, train F loss: -0.031288, acc 0.996000\n",
      "D_acc: 0.76 7.70s, epoch [2869\\200]: train D loss: 0.5053, train F loss: -0.039376, acc 0.996800\n",
      "D_acc: 0.76 7.73s, epoch [2870\\200]: train D loss: 0.5016, train F loss: -0.039925, acc 0.997600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.76 7.72s, epoch [2871\\200]: train D loss: 0.5004, train F loss: -0.041985, acc 0.997700\n",
      "D_acc: 0.76 7.71s, epoch [2872\\200]: train D loss: 0.4988, train F loss: -0.041800, acc 0.998000\n",
      "D_acc: 0.76 7.69s, epoch [2873\\200]: train D loss: 0.4983, train F loss: -0.042565, acc 0.998000\n",
      "D_acc: 0.76 7.72s, epoch [2874\\200]: train D loss: 0.4998, train F loss: -0.042692, acc 0.998000\n",
      "D_acc: 0.76 7.71s, epoch [2875\\200]: train D loss: 0.5001, train F loss: -0.042846, acc 0.998000\n",
      "D_acc: 0.76 7.67s, epoch [2876\\200]: train D loss: 0.4998, train F loss: -0.042766, acc 0.998000\n",
      "D_acc: 0.76 7.69s, epoch [2877\\200]: train D loss: 0.5007, train F loss: -0.042816, acc 0.997900\n",
      "D_acc: 0.76 7.72s, epoch [2878\\200]: train D loss: 0.5020, train F loss: -0.043043, acc 0.998000\n",
      "D_acc: 0.76 7.74s, epoch [2879\\200]: train D loss: 0.5027, train F loss: -0.043135, acc 0.997900\n",
      "D_acc: 0.76 7.72s, epoch [2880\\200]: train D loss: 0.5023, train F loss: -0.043112, acc 0.997900\n",
      "D_acc: 0.76 7.72s, epoch [2881\\200]: train D loss: 0.5025, train F loss: -0.043093, acc 0.997900\n",
      "D_acc: 0.76 7.68s, epoch [2882\\200]: train D loss: 0.5010, train F loss: -0.043077, acc 0.998000\n",
      "D_acc: 0.76 7.69s, epoch [2883\\200]: train D loss: 0.5019, train F loss: -0.043165, acc 0.998000\n",
      "D_acc: 0.76 7.69s, epoch [2884\\200]: train D loss: 0.5018, train F loss: -0.043138, acc 0.998000\n",
      "D_acc: 0.76 7.72s, epoch [2885\\200]: train D loss: 0.5027, train F loss: -0.043233, acc 0.998000\n",
      "D_acc: 0.76 7.66s, epoch [2886\\200]: train D loss: 0.5030, train F loss: -0.043268, acc 0.998000\n",
      "D_acc: 0.76 7.68s, epoch [2887\\200]: train D loss: 0.5033, train F loss: -0.043156, acc 0.998000\n",
      "D_acc: 0.76 7.71s, epoch [2888\\200]: train D loss: 0.5016, train F loss: -0.042927, acc 0.998000\n",
      "D_acc: 0.76 7.76s, epoch [2889\\200]: train D loss: 0.5019, train F loss: -0.043022, acc 0.998000\n",
      "D_acc: 0.76 7.71s, epoch [2890\\200]: train D loss: 0.5063, train F loss: -0.043619, acc 0.998000\n",
      "D_acc: 0.76 7.71s, epoch [2891\\200]: train D loss: 0.5057, train F loss: -0.043470, acc 0.997900\n",
      "D_acc: 0.76 7.71s, epoch [2892\\200]: train D loss: 0.5056, train F loss: -0.043391, acc 0.997900\n",
      "D_acc: 0.76 7.68s, epoch [2893\\200]: train D loss: 0.5043, train F loss: -0.043082, acc 0.997900\n",
      "D_acc: 0.76 7.71s, epoch [2894\\200]: train D loss: 0.5040, train F loss: -0.043427, acc 0.998000\n",
      "D_acc: 0.76 7.70s, epoch [2895\\200]: train D loss: 0.5035, train F loss: -0.043440, acc 0.998000\n",
      "D_acc: 0.76 7.71s, epoch [2896\\200]: train D loss: 0.5025, train F loss: -0.043070, acc 0.997900\n",
      "D_acc: 0.76 7.71s, epoch [2897\\200]: train D loss: 0.5030, train F loss: -0.040538, acc 0.997400\n",
      "D_acc: 0.76 7.74s, epoch [2898\\200]: train D loss: 0.5062, train F loss: -0.035483, acc 0.995400\n",
      "D_acc: 0.76 7.68s, epoch [2899\\200]: train D loss: 0.5091, train F loss: 0.002735, acc 0.988900\n",
      "D_acc: 0.76 7.71s, epoch [2900\\200]: train D loss: 0.5006, train F loss: -0.017937, acc 0.990700\n",
      "D_acc: 0.76 7.71s, epoch [2901\\200]: train D loss: 0.4940, train F loss: -0.036210, acc 0.996100\n",
      "D_acc: 0.77 7.68s, epoch [2902\\200]: train D loss: 0.4866, train F loss: -0.040960, acc 0.997700\n",
      "D_acc: 0.77 7.72s, epoch [2903\\200]: train D loss: 0.4839, train F loss: -0.041841, acc 0.998000\n",
      "D_acc: 0.77 7.71s, epoch [2904\\200]: train D loss: 0.4839, train F loss: -0.042037, acc 0.998100\n",
      "D_acc: 0.77 7.71s, epoch [2905\\200]: train D loss: 0.4865, train F loss: -0.042407, acc 0.998100\n",
      "D_acc: 0.77 7.73s, epoch [2906\\200]: train D loss: 0.4860, train F loss: -0.042383, acc 0.998100\n",
      "D_acc: 0.77 7.73s, epoch [2907\\200]: train D loss: 0.4869, train F loss: -0.042392, acc 0.998100\n",
      "D_acc: 0.77 7.68s, epoch [2908\\200]: train D loss: 0.4885, train F loss: -0.042606, acc 0.998100\n",
      "D_acc: 0.77 7.72s, epoch [2909\\200]: train D loss: 0.4907, train F loss: -0.042843, acc 0.998100\n",
      "D_acc: 0.76 7.71s, epoch [2910\\200]: train D loss: 0.4915, train F loss: -0.042857, acc 0.998100\n",
      "D_acc: 0.76 7.74s, epoch [2911\\200]: train D loss: 0.4967, train F loss: -0.043302, acc 0.998100\n",
      "D_acc: 0.76 7.71s, epoch [2912\\200]: train D loss: 0.4957, train F loss: -0.043386, acc 0.998100\n",
      "D_acc: 0.76 7.70s, epoch [2913\\200]: train D loss: 0.4983, train F loss: -0.043406, acc 0.998000\n",
      "D_acc: 0.76 7.70s, epoch [2914\\200]: train D loss: 0.4996, train F loss: -0.043568, acc 0.998100\n",
      "D_acc: 0.76 7.71s, epoch [2915\\200]: train D loss: 0.4998, train F loss: -0.043709, acc 0.998000\n",
      "D_acc: 0.76 7.77s, epoch [2916\\200]: train D loss: 0.4969, train F loss: -0.042691, acc 0.997900\n",
      "D_acc: 0.76 7.72s, epoch [2917\\200]: train D loss: 0.4944, train F loss: -0.042717, acc 0.998000\n",
      "D_acc: 0.77 7.70s, epoch [2918\\200]: train D loss: 0.4939, train F loss: -0.040792, acc 0.997700\n",
      "D_acc: 0.76 7.72s, epoch [2919\\200]: train D loss: 0.4980, train F loss: -0.038208, acc 0.997300\n",
      "D_acc: 0.76 7.70s, epoch [2920\\200]: train D loss: 0.4976, train F loss: -0.042590, acc 0.997900\n",
      "D_acc: 0.76 7.72s, epoch [2921\\200]: train D loss: 0.4955, train F loss: -0.042963, acc 0.998000\n",
      "D_acc: 0.76 7.71s, epoch [2922\\200]: train D loss: 0.4961, train F loss: -0.043101, acc 0.998100\n",
      "D_acc: 0.76 7.67s, epoch [2923\\200]: train D loss: 0.4973, train F loss: -0.043393, acc 0.998100\n",
      "D_acc: 0.76 7.72s, epoch [2924\\200]: train D loss: 0.4971, train F loss: -0.043421, acc 0.998100\n",
      "D_acc: 0.76 7.70s, epoch [2925\\200]: train D loss: 0.4986, train F loss: -0.043559, acc 0.998100\n",
      "D_acc: 0.76 7.74s, epoch [2926\\200]: train D loss: 0.5004, train F loss: -0.043461, acc 0.998000\n",
      "D_acc: 0.76 7.72s, epoch [2927\\200]: train D loss: 0.4998, train F loss: -0.039765, acc 0.996800\n",
      "D_acc: 0.76 7.70s, epoch [2928\\200]: train D loss: 0.4994, train F loss: -0.036472, acc 0.995000\n",
      "D_acc: 0.76 7.69s, epoch [2929\\200]: train D loss: 0.5031, train F loss: -0.026734, acc 0.993200\n",
      "D_acc: 0.76 7.70s, epoch [2930\\200]: train D loss: 0.4988, train F loss: -0.027081, acc 0.993100\n",
      "D_acc: 0.76 7.73s, epoch [2931\\200]: train D loss: 0.4953, train F loss: -0.035832, acc 0.995800\n",
      "D_acc: 0.76 7.67s, epoch [2932\\200]: train D loss: 0.4897, train F loss: -0.038803, acc 0.997300\n",
      "D_acc: 0.77 7.71s, epoch [2933\\200]: train D loss: 0.4864, train F loss: -0.042766, acc 0.998100\n",
      "D_acc: 0.77 7.71s, epoch [2934\\200]: train D loss: 0.4862, train F loss: -0.042851, acc 0.998200\n",
      "D_acc: 0.77 7.75s, epoch [2935\\200]: train D loss: 0.4845, train F loss: -0.042892, acc 0.998300\n",
      "D_acc: 0.77 7.73s, epoch [2936\\200]: train D loss: 0.4848, train F loss: -0.042861, acc 0.998300\n",
      "D_acc: 0.77 7.74s, epoch [2937\\200]: train D loss: 0.4870, train F loss: -0.043066, acc 0.998300\n",
      "D_acc: 0.77 7.72s, epoch [2938\\200]: train D loss: 0.4871, train F loss: -0.043130, acc 0.998300\n",
      "D_acc: 0.77 7.72s, epoch [2939\\200]: train D loss: 0.4889, train F loss: -0.043266, acc 0.998300\n",
      "D_acc: 0.77 7.72s, epoch [2940\\200]: train D loss: 0.4912, train F loss: -0.043478, acc 0.998300\n",
      "D_acc: 0.77 7.71s, epoch [2941\\200]: train D loss: 0.4908, train F loss: -0.043504, acc 0.998300\n",
      "D_acc: 0.77 7.72s, epoch [2942\\200]: train D loss: 0.4916, train F loss: -0.043580, acc 0.998400\n",
      "D_acc: 0.77 7.67s, epoch [2943\\200]: train D loss: 0.4924, train F loss: -0.043656, acc 0.998400\n",
      "D_acc: 0.76 7.73s, epoch [2944\\200]: train D loss: 0.4944, train F loss: -0.043713, acc 0.998300\n",
      "D_acc: 0.76 7.71s, epoch [2945\\200]: train D loss: 0.4967, train F loss: -0.044006, acc 0.998400\n",
      "D_acc: 0.77 7.70s, epoch [2946\\200]: train D loss: 0.4946, train F loss: -0.043972, acc 0.998400\n",
      "D_acc: 0.76 7.72s, epoch [2947\\200]: train D loss: 0.4956, train F loss: -0.043959, acc 0.998400\n",
      "D_acc: 0.76 7.72s, epoch [2948\\200]: train D loss: 0.4939, train F loss: -0.043817, acc 0.998400\n",
      "D_acc: 0.77 7.69s, epoch [2949\\200]: train D loss: 0.4943, train F loss: -0.043857, acc 0.998400\n",
      "D_acc: 0.76 7.73s, epoch [2950\\200]: train D loss: 0.5060, train F loss: -0.044725, acc 0.998400\n",
      "D_acc: 0.76 7.71s, epoch [2951\\200]: train D loss: 0.5012, train F loss: -0.044696, acc 0.998400\n",
      "D_acc: 0.76 7.69s, epoch [2952\\200]: train D loss: 0.4963, train F loss: -0.044239, acc 0.998400\n",
      "D_acc: 0.76 7.70s, epoch [2953\\200]: train D loss: 0.4940, train F loss: -0.043905, acc 0.998400\n",
      "D_acc: 0.76 7.75s, epoch [2954\\200]: train D loss: 0.4953, train F loss: -0.044107, acc 0.998400\n",
      "D_acc: 0.76 7.69s, epoch [2955\\200]: train D loss: 0.4959, train F loss: -0.044132, acc 0.998400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_acc: 0.76 7.70s, epoch [2956\\200]: train D loss: 0.4997, train F loss: -0.044465, acc 0.998400\n",
      "D_acc: 0.76 7.73s, epoch [2957\\200]: train D loss: 0.4984, train F loss: -0.044475, acc 0.998400\n",
      "D_acc: 0.76 7.71s, epoch [2958\\200]: train D loss: 0.4988, train F loss: -0.044395, acc 0.998400\n",
      "D_acc: 0.76 7.71s, epoch [2959\\200]: train D loss: 0.4997, train F loss: -0.044515, acc 0.998400\n",
      "D_acc: 0.76 7.72s, epoch [2960\\200]: train D loss: 0.5004, train F loss: -0.044327, acc 0.998400\n",
      "D_acc: 0.76 7.70s, epoch [2961\\200]: train D loss: 0.5000, train F loss: -0.043995, acc 0.998300\n",
      "D_acc: 0.76 7.73s, epoch [2962\\200]: train D loss: 0.5012, train F loss: -0.043062, acc 0.998000\n",
      "D_acc: 0.76 7.76s, epoch [2963\\200]: train D loss: 0.5000, train F loss: -0.044560, acc 0.998400\n",
      "D_acc: 0.76 7.70s, epoch [2964\\200]: train D loss: 0.5012, train F loss: -0.044644, acc 0.998400\n",
      "D_acc: 0.76 7.71s, epoch [2965\\200]: train D loss: 0.4985, train F loss: -0.044453, acc 0.998400\n",
      "D_acc: 0.76 7.66s, epoch [2966\\200]: train D loss: 0.4987, train F loss: -0.044244, acc 0.998300\n",
      "D_acc: 0.76 7.68s, epoch [2967\\200]: train D loss: 0.5005, train F loss: -0.044010, acc 0.998200\n",
      "D_acc: 0.76 7.71s, epoch [2968\\200]: train D loss: 0.5002, train F loss: -0.044188, acc 0.998200\n",
      "D_acc: 0.76 7.73s, epoch [2969\\200]: train D loss: 0.5044, train F loss: -0.038580, acc 0.997000\n",
      "D_acc: 0.76 7.67s, epoch [2970\\200]: train D loss: 0.5130, train F loss: -0.025222, acc 0.993600\n",
      "D_acc: 0.76 7.71s, epoch [2971\\200]: train D loss: 0.5044, train F loss: -0.028772, acc 0.994100\n",
      "D_acc: 0.76 7.76s, epoch [2972\\200]: train D loss: 0.4950, train F loss: -0.038976, acc 0.996900\n",
      "D_acc: 0.76 7.67s, epoch [2973\\200]: train D loss: 0.4914, train F loss: -0.038917, acc 0.997500\n",
      "D_acc: 0.77 7.73s, epoch [2974\\200]: train D loss: 0.4862, train F loss: -0.042396, acc 0.998000\n",
      "D_acc: 0.77 7.70s, epoch [2975\\200]: train D loss: 0.4850, train F loss: -0.042985, acc 0.998300\n",
      "D_acc: 0.77 7.71s, epoch [2976\\200]: train D loss: 0.4858, train F loss: -0.043010, acc 0.998300\n",
      "D_acc: 0.77 7.70s, epoch [2977\\200]: train D loss: 0.4882, train F loss: -0.043325, acc 0.998300\n",
      "D_acc: 0.77 7.70s, epoch [2978\\200]: train D loss: 0.4908, train F loss: -0.043545, acc 0.998400\n",
      "D_acc: 0.77 7.74s, epoch [2979\\200]: train D loss: 0.4906, train F loss: -0.043712, acc 0.998500\n",
      "D_acc: 0.77 7.74s, epoch [2980\\200]: train D loss: 0.4916, train F loss: -0.043751, acc 0.998500\n",
      "D_acc: 0.76 7.73s, epoch [2981\\200]: train D loss: 0.4953, train F loss: -0.043951, acc 0.998400\n",
      "D_acc: 0.77 7.72s, epoch [2982\\200]: train D loss: 0.4946, train F loss: -0.043922, acc 0.998300\n",
      "D_acc: 0.76 7.72s, epoch [2983\\200]: train D loss: 0.4935, train F loss: -0.043738, acc 0.998200\n",
      "D_acc: 0.76 7.71s, epoch [2984\\200]: train D loss: 0.4970, train F loss: -0.044187, acc 0.998200\n",
      "D_acc: 0.76 7.71s, epoch [2985\\200]: train D loss: 0.4972, train F loss: -0.044699, acc 0.998300\n",
      "D_acc: 0.76 7.70s, epoch [2986\\200]: train D loss: 0.4954, train F loss: -0.044491, acc 0.998400\n",
      "D_acc: 0.76 7.73s, epoch [2987\\200]: train D loss: 0.4965, train F loss: -0.044293, acc 0.998400\n",
      "D_acc: 0.76 7.70s, epoch [2988\\200]: train D loss: 0.4981, train F loss: -0.044622, acc 0.998500\n",
      "D_acc: 0.76 7.73s, epoch [2989\\200]: train D loss: 0.4963, train F loss: -0.044594, acc 0.998400\n",
      "D_acc: 0.76 7.67s, epoch [2990\\200]: train D loss: 0.4974, train F loss: -0.044783, acc 0.998500\n",
      "D_acc: 0.76 7.73s, epoch [2991\\200]: train D loss: 0.4988, train F loss: -0.045089, acc 0.998500\n",
      "D_acc: 0.76 7.72s, epoch [2992\\200]: train D loss: 0.4979, train F loss: -0.045002, acc 0.998500\n",
      "D_acc: 0.76 7.66s, epoch [2993\\200]: train D loss: 0.4997, train F loss: -0.045139, acc 0.998400\n",
      "D_acc: 0.76 7.68s, epoch [2994\\200]: train D loss: 0.4992, train F loss: -0.044991, acc 0.998400\n",
      "D_acc: 0.76 7.71s, epoch [2995\\200]: train D loss: 0.4987, train F loss: -0.045155, acc 0.998500\n",
      "D_acc: 0.76 7.73s, epoch [2996\\200]: train D loss: 0.4996, train F loss: -0.045322, acc 0.998500\n",
      "D_acc: 0.76 7.65s, epoch [2997\\200]: train D loss: 0.4999, train F loss: -0.045282, acc 0.998400\n",
      "D_acc: 0.76 7.69s, epoch [2998\\200]: train D loss: 0.4988, train F loss: -0.044938, acc 0.998400\n",
      "D_acc: 0.76 7.70s, epoch [2999\\200]: train D loss: 0.4970, train F loss: -0.044586, acc 0.998000\n",
      "D_acc: 0.76 7.75s, epoch [3000\\200]: train D loss: 0.4993, train F loss: -0.042647, acc 0.997600\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAYjElEQVR4nO3dfbRddX3n8fenRAGFAEK0kABBRTvAqmHIIDMODi1a48MITrENawaoExthYatTZ1Ww9WGmYsVRmaIjFgV5Uh4ELVShyqhLlyOCF0SeqQEiuSaFCIgRAZv4nT/279bDzbn3Jvfe3JNw36+1zrr7fPf+7fPbNzfnc/Zv77N3qgpJkn5j0B2QJG0dDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCBiTJJ5O8ezPbvCvJp9v0wiSVZM409WefJD9Pst10rG8zXjdJPpPkkSQ3zORrS6MZCJp2SVYmeTzJuiQ/TfKdJCcm+Ze/t6o6sar+qi1/RJLhidZbVR+oqjdPYx9f0bPu+6tqp6raMB3r3wz/HnglsKCqDk3yviQXTecLJDkoyVeS/CRJ3y8eJVma5M4kjyW5J8nhPfOOTHJXkl8k+UaSfXvmJcnpSR5qjw8lyXT2XzPHQNCW8h+ramdgX+CDwDuBcya7sunaE9gK7QusrKrHpmNlY/ye/hm4DFg2RptXAqcDbwJ2Bl4O3Nvm7QF8AXg38BxgCLi0p/ly4GjgJcBvA68D3jINm6JBqCofPqb1AawEXjGqdijwK+Cg9vw84P3As4HH27yft8dewPuAy4GLgJ8Bb261i1r7hUDRvSGtBtYA7+h5vfOA9/c8PwIYbtMXttd7vL3en/esb05bZi/gKuBhYAXwxz3reh/dG+wFwDrgdmDxOL+PvwFWte24ETi81ZcBTwAbWj+uB35J9wb+c+AHbbld6MJ0DfDj9nvbrs37I+D/AWe0vr5/nH68sPsvv1H9O8CyMdosB77T83zk3+u3etou75m/DPjuoP8GfUzu4R6CZkRV3QAMA4ePqj8GvBpYXd2QzU5VtbrNPoouFHYFPjvGqn8H2B/4PeCU3mGgcfpyHHA/3V7MTlX1oT6LXdz6uxdwDPCBJEf2zH89cEnr21XAx8d5ye8Bi+g+YX8O+HySHarqHOBE4LrWj5cCHwAubc9f0tqfD6yne0M/uG1r79DZS+k+0T8XOG2i7e/VjpksBuYlWZFkOMnHk+zYFjkQ+MHI8u3f655W32h+mz4QbZMMBM2k1XRvipvquqr6u6r6VVU9PsYy/6OqHquqW4HPAMdOtZNJ9qYb239nVT1RVTcDnwaO61ns21V1dXXHHC6kGzLpq6ouqqqHqmp9VX0E2B548Sb25Xl0gfn2tp0P0u0NLO1ZbHVVfaytf6zf01ieBzyDLvQOpwuug4G/bPN3Ah4d1eZRuqGlfvMfBXbyOMK2yUDQTJpPN6yxqVZt5jI/ovtEP1V7AQ9X1bpR657f8/yfeqZ/Aeww1nGOJO9oB2wfTfJTuiGgPTaxL/vSvWGvaQfofwr8Ld3ewIhN+T2NZSRAPlZVa6rqJ8BHgde0+s+BuaPazKUbKus3fy7w86ryqpnbIANBMyLJv6F7Q/12n9ljvXlsypvK3j3T+9DthQA8BjyrZ95vbsa6VwPPSbJzT20fuvH7zdLO1nkn8AfAblW1K92n6LE+QY/u1yrgSWCPqtq1PeZW1YHjtNlkVfUI3dDYWOu4nZ69nyTPBl7Q6hvNb9O3o22SgaAtKsncJK+jG2+/qA3tjPYAsHuSXSbxEu9O8qwkB9KdJTNyBszNwGuSPCfJbwJv7/Oaz++3wqpaRXew9K+T7JDkt+kOlo51HGM8O9ON/68F5iR5Dxt/4h7dr4Ujp+hW1Rrgq8BH2u/yN5K8IMl/2NQOtFNDdwCe2Z7vkGT7nkU+A/xJkucm2Y3ud/WlNu+LwEFJfr+t4z3ALVV1V5t/AfBnSeYn2Qt4B90BfW2DDARtKX+fZB3dJ9y/oBuGeFO/Bduby8XAvW1YZHOGfb5JdxbQ14APV9VXW/1CugOcK+neUC8d1e6vgb9sr/ff+6z3WLozj1bTvSm+t6qu3Yx+jfgKcA3wj3TDTk8w/hDP59vPh5Lc1KaPp3szvwN4hO5A+56b0Yd96YaGRj65Pw7c3TP/r+gOfP8jcCfwfdrB6apaC/x+e/4I3QHs3uMXfwv8PXArcBvw5VbTNigO9UmSwD0ESVJjIEiSAANBktQYCJIkALbZC4btsccetXDhwkF3Q5K2KTfeeONPqmpev3nbbCAsXLiQoaGhQXdDkrYpSX401jyHjCRJgIEgSWoMBEkSYCBIkpoJAyHJuUkeTHJbT+3SJDe3x8okN7f6wnYv3ZF5n+xpc0iSW9tNOM4cuV56ku3b+lYkuT7JwunfTEnSRDZlD+E8YElvoar+sKoWVdUi4Aq6e66OuGdkXlWd2FM/i+52fPu3x8g6lwGPVNUL6W78cfqktkSSNCUTBkJVfYsxbmrSPuX/Ad2VKseUZE9gblVd126ccQHdjbmhu03i+W36cuBI77YkSTNvqscQDgceqKof9tT2S/L9JN9sNweB7sYowz3LDPPru0/Np10OuKrW0908ZPd+L5ZkeZKhJENr166dYtclSb2mGgjH8tS9gzXAPlV1MPBnwOeSzKX/3aFGrrs93rynFqvOrqrFVbV43ry+X7STJE3SpL+p3O4f+5+AQ0ZqVfUk3e3+qKobk9wDvIhuj2BBT/MF/PpWh8N0t0Ecbuvchc277662AQtP+fIWf42VH3ztFn8N6elsKnsIrwDuqqp/GQpKMi/Jdm36+XQHj+9ttwFcl+SwdnzgeODK1uwq4IQ2fQzwdW/QLUkzb1NOO70YuA54cZLhJMvarKVsfDD55cAtSX5Ad4D4xKoa+bR/EvBputsd3kN3W0GAc+jup7uCbpjplClsjyRpkiYcMqqqY8eo/1Gf2hV0p6H2W34IOKhP/QngjRP1Q5K0ZflNZUkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAZsQCEnOTfJgktt6au9L8uMkN7fHa3rmnZpkRZK7k7yqp35IklvbvDOTpNW3T3Jpq1+fZOH0bqIkaVNsyh7CecCSPvUzqmpRe1wNkOQAYClwYGvziSTbteXPApYD+7fHyDqXAY9U1QuBM4DTJ7ktkqQpmDAQqupbwMObuL6jgEuq6smqug9YARyaZE9gblVdV1UFXAAc3dPm/DZ9OXDkyN6DJGnmTOUYwluT3NKGlHZrtfnAqp5lhlttfpseXX9Km6paDzwK7N7vBZMsTzKUZGjt2rVT6LokabTJBsJZwAuARcAa4COt3u+TfY1TH6/NxsWqs6tqcVUtnjdv3ub1WJI0rkkFQlU9UFUbqupXwKeAQ9usYWDvnkUXAKtbfUGf+lPaJJkD7MKmD1FJkqbJpAKhHRMY8QZg5Aykq4Cl7cyh/egOHt9QVWuAdUkOa8cHjgeu7GlzQps+Bvh6O84gSZpBcyZaIMnFwBHAHkmGgfcCRyRZRDe0sxJ4C0BV3Z7kMuAOYD1wclVtaKs6ie6MpR2Ba9oD4BzgwiQr6PYMlk7HhkmSNs+EgVBVx/YpnzPO8qcBp/WpDwEH9ak/Abxxon5IkrYsv6ksSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1EwYCEnOTfJgktt6av8ryV1JbknyxSS7tvrCJI8nubk9PtnT5pAktyZZkeTMJGn17ZNc2urXJ1k4/ZspSZrInE1Y5jzg48AFPbVrgVOran2S04FTgXe2efdU1aI+6zkLWA58F7gaWAJcAywDHqmqFyZZCpwO/OEktkXSKAtP+fIWXf/KD752i65fM2vCPYSq+hbw8KjaV6tqfXv6XWDBeOtIsicwt6quq6qiC5ej2+yjgPPb9OXAkSN7D5KkmTMdxxD+K90n/RH7Jfl+km8mObzV5gPDPcsMt9rIvFUALWQeBXafhn5JkjbDpgwZjSnJXwDrgc+20hpgn6p6KMkhwN8lORDo94m/RlYzzrzRr7ecbtiJffbZZypdlySNMuk9hCQnAK8D/nMbBqKqnqyqh9r0jcA9wIvo9gh6h5UWAKvb9DCwd1vnHGAXRg1Rjaiqs6tqcVUtnjdv3mS7LknqY1KBkGQJ3UHk11fVL3rq85Js16afD+wP3FtVa4B1SQ5rxweOB65sza4CTmjTxwBfHwkYSdLMmXDIKMnFwBHAHkmGgffSnVW0PXBtO/773ao6EXg58D+TrAc2ACdW1cin/ZPozljake6Yw8hxh3OAC5OsoNszWDotWyZJ2iwTBkJVHdunfM4Yy14BXDHGvCHgoD71J4A3TtQPSdKW5TeVJUmAgSBJagwESRJgIEiSGgNBkgRM8ZvKkibmBea0rXAPQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkZlZeumJLX0oAvJyApG2PewiSJMBAkCQ1BoIkCTAQJEnNhIGQ5NwkDya5raf2nCTXJvlh+7lbz7xTk6xIcneSV/XUD0lya5t3ZpK0+vZJLm3165MsnN5NlCRtik3ZQzgPWDKqdgrwtaraH/hae06SA4ClwIGtzSeSbNfanAUsB/Zvj5F1LgMeqaoXAmcAp092YyRJkzdhIFTVt4CHR5WPAs5v0+cDR/fUL6mqJ6vqPmAFcGiSPYG5VXVdVRVwwag2I+u6HDhyZO9BkjRzJnsM4XlVtQag/Xxuq88HVvUsN9xq89v06PpT2lTVeuBRYPd+L5pkeZKhJENr166dZNclSf1M90Hlfp/sa5z6eG02LladXVWLq2rxvHnzJtlFSVI/kw2EB9owEO3ng60+DOzds9wCYHWrL+hTf0qbJHOAXdh4iEqStIVN9tIVVwEnAB9sP6/sqX8uyUeBvegOHt9QVRuSrEtyGHA9cDzwsVHrug44Bvh6O87wtORlMyRtrSYMhCQXA0cAeyQZBt5LFwSXJVkG3A+8EaCqbk9yGXAHsB44uao2tFWdRHfG0o7ANe0BcA5wYZIVdHsGS6dlyyRJm2XCQKiqY8eYdeQYy58GnNanPgQc1Kf+BC1QJEmD4zeVJUmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJKayV7+WtqmbOnLjnvJcT0duIcgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEnNpL+YluTFwKU9pecD7wF2Bf4YWNvq76qqq1ubU4FlwAbgT6vqK61+CHAesCNwNfC2qqrJ9k3S7Lalv4gIT88vI056D6Gq7q6qRVW1CDgE+AXwxTb7jJF5PWFwALAUOBBYAnwiyXZt+bOA5cD+7bFksv2SJE3OdA0ZHQncU1U/GmeZo4BLqurJqroPWAEcmmRPYG5VXdf2Ci4Ajp6mfkmSNtF0BcJS4OKe529NckuSc5Ps1mrzgVU9ywy32vw2Pbq+kSTLkwwlGVq7dm2/RSRJkzTlQEjyTOD1wOdb6SzgBcAiYA3wkZFF+zSvceobF6vOrqrFVbV43rx5U+q3JOmppuNqp68GbqqqBwBGfgIk+RTwpfZ0GNi7p90CYHWrL+hTl6RtzrZ8QHs6AuFYeoaLkuxZVWva0zcAt7Xpq4DPJfkosBfdweMbqmpDknVJDgOuB44HPjYN/dIo2/IfqqQtb0qBkORZwCuBt/SUP5RkEd2wz8qReVV1e5LLgDuA9cDJVbWhtTmJX592ek17SJJm0JQCoap+Aew+qnbcOMufBpzWpz4EHDSVvkiSpsY7pknaIhyi3PZ46QpJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkZkqBkGRlkluT3JxkqNWek+TaJD9sP3frWf7UJCuS3J3kVT31Q9p6ViQ5M0mm0i9J0uabjj2E36mqRVW1uD0/BfhaVe0PfK09J8kBwFLgQGAJ8Ikk27U2ZwHLgf3bY8k09EuStBm2xJDRUcD5bfp84Oie+iVV9WRV3QesAA5Nsicwt6quq6oCLuhpI0maIVMNhAK+muTGJMtb7XlVtQag/Xxuq88HVvW0HW61+W16dH0jSZYnGUoytHbt2il2XZLUa84U27+sqlYneS5wbZK7xlm233GBGqe+cbHqbOBsgMWLF/ddRpI0OVPaQ6iq1e3ng8AXgUOBB9owEO3ng23xYWDvnuYLgNWtvqBPXZI0gyYdCEmenWTnkWng94DbgKuAE9piJwBXtumrgKVJtk+yH93B4xvasNK6JIe1s4uO72kjSZohUxkyeh7wxXaG6Bzgc1X1D0m+B1yWZBlwP/BGgKq6PcllwB3AeuDkqtrQ1nUScB6wI3BNe0iSZtCkA6Gq7gVe0qf+EHDkGG1OA07rUx8CDppsXyRJU+c3lSVJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBEwhEJLsneQbSe5McnuSt7X6+5L8OMnN7fGanjanJlmR5O4kr+qpH5Lk1jbvzCSZ2mZJkjbXnCm0XQ+8o6puSrIzcGOSa9u8M6rqw70LJzkAWAocCOwF/N8kL6qqDcBZwHLgu8DVwBLgmin0TZK0mSa9h1BVa6rqpja9DrgTmD9Ok6OAS6rqyaq6D1gBHJpkT2BuVV1XVQVcABw92X5JkiZnWo4hJFkIHAxc30pvTXJLknOT7NZq84FVPc2GW21+mx5d7/c6y5MMJRlau3btdHRdktRMORCS7ARcAby9qn5GN/zzAmARsAb4yMiifZrXOPWNi1VnV9Xiqlo8b968qXZdktRjSoGQ5Bl0YfDZqvoCQFU9UFUbqupXwKeAQ9viw8DePc0XAKtbfUGfuiRpBk3lLKMA5wB3VtVHe+p79iz2BuC2Nn0VsDTJ9kn2A/YHbqiqNcC6JIe1dR4PXDnZfkmSJmcqZxm9DDgOuDXJza32LuDYJIvohn1WAm8BqKrbk1wG3EF3htLJ7QwjgJOA84Ad6c4u8gwjSZphkw6Eqvo2/cf/rx6nzWnAaX3qQ8BBk+2LJGnq/KayJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJKArSgQkixJcneSFUlOGXR/JGm22SoCIcl2wP8BXg0cAByb5IDB9kqSZpetIhCAQ4EVVXVvVf0SuAQ4asB9kqRZJVU16D6Q5BhgSVW9uT0/DnhpVb111HLLgeXt6YuBu2ewm3sAP5nB19tauN2zi9v99LdvVc3rN2POTPdkDOlT2yipqups4Owt352NJRmqqsWDeO1BcrtnF7d7dttahoyGgb17ni8AVg+oL5I0K20tgfA9YP8k+yV5JrAUuGrAfZKkWWWrGDKqqvVJ3gp8BdgOOLeqbh9wt0YbyFDVVsDtnl3c7llsqzioLEkavK1lyEiSNGAGgiQJMBAmNBsvqZFk7yTfSHJnktuTvG3QfZpJSbZL8v0kXxp0X2ZSkl2TXJ7krvZv/28H3aeZkOS/tb/z25JcnGSHQfdpUAyEccziS2qsB95RVf8KOAw4eZZs94i3AXcOuhMD8DfAP1TVbwEvYRb8DpLMB/4UWFxVB9Gd1LJ0sL0aHANhfLPykhpVtaaqbmrT6+jeGOYPtlczI8kC4LXApwfdl5mUZC7wcuAcgKr6ZVX9dLC9mjFzgB2TzAGexSz+DpSBML75wKqe58PMkjfGEUkWAgcD1w+2JzPmfwN/Dvxq0B2ZYc8H1gKfacNln07y7EF3akurqh8DHwbuB9YAj1bVVwfbq8ExEMa3SZfUeLpKshNwBfD2qvrZoPuzpSV5HfBgVd046L4MwBzgXwNnVdXBwGPA0/6YWZLd6Pb69wP2Ap6d5L8MtleDYyCMb9ZeUiPJM+jC4LNV9YVB92eGvAx4fZKVdMODv5vkosF2acYMA8NVNbIneDldQDzdvQK4r6rWVtU/A18A/t2A+zQwBsL4ZuUlNZKEbiz5zqr66KD7M1Oq6tSqWlBVC+n+rb9eVbPi02JV/ROwKsmLW+lI4I4Bdmmm3A8cluRZ7e/+SGbBwfSxbBWXrthabSOX1NgSXgYcB9ya5OZWe1dVXT3APmnL+xPgs+3Dz73Amwbcny2uqq5PcjlwE93Zdd9nFl/GwktXSJIAh4wkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNf8fY9vvXckJcMMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAaFklEQVR4nO3df7RdZX3n8fenSZVfRkEChSQaqlELGR1LimkdO7axQ1odw1rKrDBVMg6djAy12tqxYMfqdJoWHauVTmGGASSABVKKJdbSykJbl1MKvfijMSA1FSTXRHNVxEgVG/zOH/uJPdyce5PcH+ck3PdrrbPOPt9nP/s85ybrfM5+9j5np6qQJOkHhj0ASdKhwUCQJAEGgiSpMRAkSYCBIElqDARJEmAgaEiS/O8kbzvIPm9NckVbXpqkksyfofE8I8m3ksybie0dxPMmyfuTPJTkrkE+tzSegaAZl+SBJN9OsjvJN5L8dZLXJ/n+/7eqen1V/Y+2/kuTjO5vu1X121X1CzM4xpf1bPvBqjqmqh6bie0fhH8F/AywuKrOSPKOJNfN5BMkWZfk7iTfTDKa5F29QZrkuCQfTPJIki8m+ffj+q9K8rkk/5jkY0me2dOWJO9M8rV2e1eSzOT4NTgGgmbLv62qpwDPBC4Gfg24cqobm6k9gUPQM4EHquqRmdjYBH+no4A3AccDLwJWAb/a0/4HwHeBE4GfBy5Lclrb3vHAzcDbgOOAEeDGnr7rgbOAFwDPB14B/OeZeC0agqry5m1Gb8ADwMvG1c4Avgcsb4+vBn4LOBr4dmv7VrudDLwDuAm4Dvgm8Autdl3rvxQoujekHcBO4M09z3c18Fs9j18KjLbla9vzfbs931t6tje/rXMysBn4OrAN+E8923oHsAm4BtgNbAVWTPL3eB+wvb2Ou4GXtPp5wHeAx9o47qR7Y/6n9vgzbb2n0oXpTuBL7e82r7X9B+D/Ae9tY/2tA/j3+RXgQ2356Pacz+lpvxa4uC2vB/66p23vv9fz2uO/Btb3tJ8H/M2w/w96m9rNPQQNRFXdBYwCLxlXfwT4WWBHdVM2x1TVjta8hi4UngZ8YIJN/xSwDPg3wIW900CTjOW1wIN0ezHHVNW7+qx2fRvvycCrgd9Osqqn/ZXADW1sm4H/NclT/i3wL+k+Yf8h8EdJjqiqK4HXA3e0cbwI+G3gxvb4Ba3/RmAP8Gzghe219k6dvQj4AnACsGF/rx/4SboQA3gO8FhV/X1P+2eA09ryae0x8P1/r3+YqH1cXx1mDAQN0g66N8UDdUdV/UlVfa+qvj3BOv+9qh6pqi3A+4FzpjvIJEvo5vZ/raq+U1WfBq4AXtuz2ieq6s+qO+ZwLd2USV9VdV1Vfa2q9lTV7wJPBp57gGM5kS4w39Re5y66vYG1PavtqKrfb9uf6O+0d3uvA1YA726lY4CHx632MPCUKbY/DBzjcYTD0xN1XlaHpkV00xoHavtBrvNF4F8c1Ij6Oxn4elXtHrftFT2Pv9yz/I/AEUnmV9We8RtL8ma6T/Qn001LLaCbzz8QzwR+ENjZ8x77Azz+dR/I34kkZ9Edz3lZVX21lb/VxtNrAd1U2FTaFwDfqip/NfMw5B6CBiLJj9EFwif6NE/05nEgbypLepafQbcXAvAI3cHUvX7oILa9AzguyVN6as+gm78/KEleQndA/d8Bx1bV0+g+RU/0CXr8uLYDjwLHV9XT2m1BVZ02SZ9+41gN/F+6abItPU1/D8xPsqyn9gL+eUppKz17P0mOBp41Ufu4vjrMGAiaVUkWJHkF3Xz7dePejPb6CvD0JE+dwlO8LclR7ayY1/HPZ8B8Gvi5dkrlD9GdZTP+OX+43warajvdwdLfSXJEkufTHSyd6DjGZJ5CN/8/RvfG+xvs+4l7/LiW7j1Ft6p2Ah8Bfrf9LX8gybOS/OsDHUCSn25jf1U7lvN97ZjAzcBvJjk6yYvpjt1c21b5ILA8yauSHAH8BvB3VfW51n4N8CtJFiU5GXgz3QF9HYYMBM2WDyXZTfcJ99eB99C9Ye+jvblcD3yhfW/h5IN4nr+iOwvoduDdVfWRVr+W7gDnA3RvqDeO6/c7wH9rz/er7OscujOPdtC9Kb69qm47iHHt9RfArXSfxL9Id1bRZFM8f9Tuv5bkk235XOBJwD3AQ3QH2k86iDG8je5MpT9rX777VpJbe9r/C3AksIvu3+H8qtoKUFVjwKvoDlY/RHcAu/f4xf8BPgRsAT4LfLjVdBiKU32SJHAPQZLUGAiSJMBAkCQ1BoIkCTiMv5h2/PHH19KlS4c9DEk6rNx9991fraqF/doO20BYunQpIyMjwx6GJB1WknxxojanjCRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEnAYfxNZR1ell744Vl/jgcufvmsP4f0RLbfPYQkVyXZleSz4+pvSHJfkq1J3tVTvyjJttZ2Zk/99CRbWtslaVcMT/LkJDe2+p1Jls7cy5MkHagDmTK6GljdW0jyU3TXXX1+u9j3u1v9VLrL653W+lyaZF7rdhmwHljWbnu3eR7wUFU9G3gv8M5pvB5J0hTtNxCq6uPA18eVzwcurqpH2zq7Wn0NcENVPVpV99Nd6/aMJCcBC6rqjuqu2XkNcFZPn41t+SZg1d69B0nS4Ez1oPJzgJe0KZ6/SvJjrb6Ix19AfLTVFrXl8fXH9amqPcDDwNP7PWmS9UlGkoyMjY1NceiSpH6mGgjzgWOBlcB/BTa1T/X9PtnXJHX20/b4YtXlVbWiqlYsXNj357wlSVM01UAYBW6uzl3A94DjW31Jz3qLgR2tvrhPnd4+SeYDT2XfKSpJ0iybaiD8CfDTAEmeAzwJ+CqwGVjbzhw6he7g8V1VtRPYnWRl25M4F7ilbWszsK4tvxr4aDvOIEkaoP1+DyHJ9cBLgeOTjAJvB64Crmqnon4XWNfexLcm2QTcA+wBLqiqx9qmzqc7Y+lI4NZ2A7gSuDbJNro9g7Uz89IkSQdjv4FQVedM0PSaCdbfAGzoUx8Blvepfwc4e3/jkCTNLn+6QpIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJKa/QZCkquS7GpXRxvf9qtJKsnxPbWLkmxLcl+SM3vqpyfZ0touaZfSpF1u88ZWvzPJ0pl5aZKkg3EgewhXA6vHF5MsAX4GeLCndirdJTBPa30uTTKvNV8GrKe7zvKynm2eBzxUVc8G3gu8cyovRJI0PfsNhKr6ON21jsd7L/AWoHpqa4AbqurRqrof2AackeQkYEFV3dGuvXwNcFZPn41t+SZg1d69B0nS4EzpGEKSVwJfqqrPjGtaBGzveTzaaova8vj64/pU1R7gYeDpEzzv+iQjSUbGxsamMnRJ0gQOOhCSHAX8OvAb/Zr71GqS+mR99i1WXV5VK6pqxcKFCw9kuJKkAzSVPYRnAacAn0nyALAY+GSSH6L75L+kZ93FwI5WX9ynTm+fJPOBp9J/ikqSNIsOOhCqaktVnVBVS6tqKd0b+o9W1ZeBzcDadubQKXQHj++qqp3A7iQr2/GBc4Fb2iY3A+va8quBj7bjDJKkATqQ006vB+4AnptkNMl5E61bVVuBTcA9wJ8DF1TVY635fOAKugPN/wDc2upXAk9Psg34FeDCKb4WSdI0zN/fClV1zn7al457vAHY0Ge9EWB5n/p3gLP3Nw5J0uzym8qSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUrPfn7+WdPhaeuGHZ3X7D1z88lndvgbLPQRJEjBH9xBm+1MT+MlJ0uHnQC6heVWSXUk+21P7n0k+l+TvknwwydN62i5Ksi3JfUnO7KmfnmRLa7ukXVuZdv3lG1v9ziRLZ/YlSpIOxIFMGV0NrB5Xuw1YXlXPB/4euAggyanAWuC01ufSJPNan8uA9cCydtu7zfOAh6rq2cB7gXdO9cVIkqZuv4FQVR8Hvj6u9pGq2tMe/g2wuC2vAW6oqker6n5gG3BGkpOABVV1R1UVcA1wVk+fjW35JmDV3r0HSdLgzMRB5f8I3NqWFwHbe9pGW21RWx5ff1yfFjIPA0/v90RJ1icZSTIyNjY2A0OXJO01rUBI8uvAHuADe0t9VqtJ6pP12bdYdXlVraiqFQsXLjzY4UqSJjHlQEiyDngF8PNtGgi6T/5LelZbDOxo9cV96o/rk2Q+8FTGTVFJkmbflAIhyWrg14BXVtU/9jRtBta2M4dOoTt4fFdV7QR2J1nZjg+cC9zS02ddW3418NGegJEkDch+v4eQ5HrgpcDxSUaBt9OdVfRk4LZ2/Pdvqur1VbU1ySbgHrqppAuq6rG2qfPpzlg6ku6Yw97jDlcC1ybZRrdnsHZmXpok6WDsNxCq6pw+5SsnWX8DsKFPfQRY3qf+HeDs/Y1DkjS7/OkKSRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkScAcvUCONEhexlKHC/cQJEmAgSBJagwESRJgIEiSGg8qD9hsH2AEDzJKmhr3ECRJgIEgSWoMBEkScACBkOSqJLuSfLandlyS25J8vt0f29N2UZJtSe5LcmZP/fQkW1rbJe1SmrTLbd7Y6ncmWTqzL1GSdCAOZA/hamD1uNqFwO1VtQy4vT0myal0l8A8rfW5NMm81ucyYD3ddZaX9WzzPOChqno28F7gnVN9MZKkqdtvIFTVx+muddxrDbCxLW8Ezuqp31BVj1bV/cA24IwkJwELquqOqirgmnF99m7rJmDV3r0HSdLgTPUYwolVtROg3Z/Q6ouA7T3rjbbaorY8vv64PlW1B3gYeHq/J02yPslIkpGxsbEpDl2S1M9MH1Tu98m+JqlP1mffYtXlVbWiqlYsXLhwikOUJPUz1UD4SpsGot3vavVRYEnPeouBHa2+uE/9cX2SzAeeyr5TVJKkWTbVQNgMrGvL64Bbeupr25lDp9AdPL6rTSvtTrKyHR84d1yfvdt6NfDRdpxBkjRA+/3piiTXAy8Fjk8yCrwduBjYlOQ84EHgbICq2ppkE3APsAe4oKoea5s6n+6MpSOBW9sN4Erg2iTb6PYM1s7IK5MkHZT9BkJVnTNB06oJ1t8AbOhTHwGW96l/hxYokqTh8ZvKkiTAQJAkNQaCJAkwECRJjRfI0Zww2xcm8qJEeiJwD0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQB0wyEJL+cZGuSzya5PskRSY5LcluSz7f7Y3vWvyjJtiT3JTmzp356ki2t7ZJ2mU1J0gBNORCSLAJ+CVhRVcuBeXSXv7wQuL2qlgG3t8ckObW1nwasBi5NMq9t7jJgPd01mJe1dknSAE13ymg+cGSS+cBRwA5gDbCxtW8EzmrLa4AbqurRqrof2AackeQkYEFV3VFVBVzT00eSNCBTDoSq+hLwbuBBYCfwcFV9BDixqna2dXYCJ7Qui4DtPZsYbbVFbXl8fR9J1icZSTIyNjY21aFLkvqYzpTRsXSf+k8BTgaOTvKaybr0qdUk9X2LVZdX1YqqWrFw4cKDHbIkaRLTuUDOy4D7q2oMIMnNwE8AX0lyUlXtbNNBu9r6o8CSnv6L6aaYRtvy+LokHXZm+2JMMHsXZJpOIDwIrExyFPBtYBUwAjwCrAMubve3tPU3A3+Y5D10exTLgLuq6rEku5OsBO4EzgV+fxrjkjTHHc5vysM05UCoqjuT3AR8EtgDfAq4HDgG2JTkPLrQOLutvzXJJuCetv4FVfVY29z5wNXAkcCt7SZJGqBpXVO5qt4OvH1c+VG6vYV+628ANvSpjwDLpzMWSdL0+E1lSRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqRmWj9up8OLPwksaTLuIUiSAANBktQYCJIkwECQJDXTCoQkT0tyU5LPJbk3yY8nOS7JbUk+3+6P7Vn/oiTbktyX5Mye+ulJtrS2S5JkOuOSJB286e4hvA/486p6HvAC4F7gQuD2qloG3N4ek+RUYC1wGrAauDTJvLady4D1wLJ2Wz3NcUmSDtKUAyHJAuAngSsBquq7VfUNYA2wsa22ETirLa8BbqiqR6vqfmAbcEaSk4AFVXVHVRVwTU8fSdKATGcP4YeBMeD9ST6V5IokRwMnVtVOgHZ/Qlt/EbC9p/9oqy1qy+Pr+0iyPslIkpGxsbFpDF2SNN50vpg2H/hR4A1VdWeS99GmhybQ77hATVLft1h1OXA5wIoVK/quI+nQ4BchDz/T2UMYBUar6s72+Ca6gPhKmwai3e/qWX9JT//FwI5WX9ynLkkaoCkHQlV9Gdie5LmttAq4B9gMrGu1dcAtbXkzsDbJk5OcQnfw+K42rbQ7ycp2dtG5PX0kSQMy3d8yegPwgSRPAr4AvI4uZDYlOQ94EDgboKq2JtlEFxp7gAuq6rG2nfOBq4EjgVvbTZI0QNMKhKr6NLCiT9OqCdbfAGzoUx8Blk9nLJKk6fGbypIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUTDsQksxL8qkkf9oeH5fktiSfb/fH9qx7UZJtSe5LcmZP/fQkW1rbJe1SmpKkAZqJPYQ3Avf2PL4QuL2qlgG3t8ckORVYC5wGrAYuTTKv9bkMWE93neVlrV2SNEDTCoQki4GXA1f0lNcAG9vyRuCsnvoNVfVoVd0PbAPOSHISsKCq7qiqAq7p6SNJGpDp7iH8HvAW4Hs9tROraidAuz+h1RcB23vWG221RW15fH0fSdYnGUkyMjY2Ns2hS5J6TTkQkrwC2FVVdx9olz61mqS+b7Hq8qpaUVUrFi5ceIBPK0k6EPOn0ffFwCuT/BxwBLAgyXXAV5KcVFU723TQrrb+KLCkp/9iYEerL+5TlyQN0JT3EKrqoqpaXFVL6Q4Wf7SqXgNsBta11dYBt7TlzcDaJE9OcgrdweO72rTS7iQr29lF5/b0kSQNyHT2ECZyMbApyXnAg8DZAFW1Nckm4B5gD3BBVT3W+pwPXA0cCdzabpKkAZqRQKiqvwT+si1/DVg1wXobgA196iPA8pkYiyRpavymsiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1Uw6EJEuSfCzJvUm2Jnljqx+X5LYkn2/3x/b0uSjJtiT3JTmzp356ki2t7ZJ2KU1J0gBNZw9hD/DmqvoRYCVwQZJTgQuB26tqGXB7e0xrWwucBqwGLk0yr23rMmA93XWWl7V2SdIATTkQqmpnVX2yLe8G7gUWAWuAjW21jcBZbXkNcENVPVpV9wPbgDOSnAQsqKo7qqqAa3r6SJIGZEaOISRZCrwQuBM4sap2QhcawAlttUXA9p5uo622qC2Pr/d7nvVJRpKMjI2NzcTQJUnNtAMhyTHAHwNvqqpvTrZqn1pNUt+3WHV5Va2oqhULFy48+MFKkiY0rUBI8oN0YfCBqrq5lb/SpoFo97tafRRY0tN9MbCj1Rf3qUuSBmg6ZxkFuBK4t6re09O0GVjXltcBt/TU1yZ5cpJT6A4e39WmlXYnWdm2eW5PH0nSgMyfRt8XA68FtiT5dKu9FbgY2JTkPOBB4GyAqtqaZBNwD90ZShdU1WOt3/nA1cCRwK3tJkkaoCkHQlV9gv7z/wCrJuizAdjQpz4CLJ/qWCRJ0+c3lSVJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkScAhFAhJVie5L8m2JBcOezySNNccEoGQZB7wB8DPAqcC5yQ5dbijkqS55ZAIBOAMYFtVfaGqvgvcAKwZ8pgkaU5JVQ17DCR5NbC6qn6hPX4t8KKq+sVx660H1reHzwXuG+Awjwe+OsDnO1T4uucWX/cT3zOramG/hvmDHskE0qe2T1JV1eXA5bM/nH0lGamqFcN47mHydc8tvu657VCZMhoFlvQ8XgzsGNJYJGlOOlQC4W+BZUlOSfIkYC2wechjkqQ55ZCYMqqqPUl+EfgLYB5wVVVtHfKwxhvKVNUhwNc9t/i657BD4qCyJGn4DpUpI0nSkBkIkiTAQNivufiTGkmWJPlYknuTbE3yxmGPaZCSzEvyqSR/OuyxDFKSpyW5Kcnn2r/9jw97TIOQ5Jfb//PPJrk+yRHDHtOwGAiTmMM/qbEHeHNV/QiwErhgjrzuvd4I3DvsQQzB+4A/r6rnAS9gDvwNkiwCfglYUVXL6U5qWTvcUQ2PgTC5OfmTGlW1s6o+2ZZ3070xLBruqAYjyWLg5cAVwx7LICVZAPwkcCVAVX23qr4x3FENzHzgyCTzgaOYw9+BMhAmtwjY3vN4lDnyxrhXkqXAC4E7hzuSgfk94C3A94Y9kAH7YWAMeH+bLrsiydHDHtRsq6ovAe8GHgR2Ag9X1UeGO6rhMRAmd0A/qfFEleQY4I+BN1XVN4c9ntmW5BXArqq6e9hjGYL5wI8Cl1XVC4FHgCf8MbMkx9Lt9Z8CnAwcneQ1wx3V8BgIk5uzP6mR5AfpwuADVXXzsMczIC8GXpnkAbrpwZ9Oct1whzQwo8BoVe3dE7yJLiCe6F4G3F9VY1X1T8DNwE8MeUxDYyBMbk7+pEaS0M0l31tV7xn2eAalqi6qqsVVtZTu3/qjVTUnPi1W1ZeB7Ume20qrgHuGOKRBeRBYmeSo9v9+FXPgYPpEDomfrjhUHSY/qTEbXgy8FtiS5NOt9taq+rMhjkmz7w3AB9qHny8ArxvyeGZdVd2Z5Cbgk3Rn132KOfwzFv50hSQJcMpIktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUvP/AeuqbpXPC2l+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for sl_epoch in range(1, sl_epochs+1):\n",
    "    # 构造新的source data\n",
    "    test_loader = get_data_loader(test_dataset, mode = 'eval')\n",
    "    pred_labels, pred_probs,_ = testing(feature_extractor, label_predictor, test_loader)\n",
    "    idx = (pred_probs.max(axis=1) > threshold) & (pred_labels != 3)\n",
    "    np.random.shuffle(idx)\n",
    "    add_data, add_labels = test_loader.dataset.data[idx][:mix_num], pred_labels[idx][:mix_num]\n",
    "    source_dataset = AddDataset(np.concatenate([source_x_paths, add_data]), np.concatenate([source_y, add_labels]), source_transform, target_transform)\n",
    "    print(f'add : {len(add_data)} new source size: {len(source_dataset)}')\n",
    "    source_loader = get_data_loader(source_dataset, batch_size=256, mode = 'train')\n",
    "    target_loader = get_data_loader(target_dataset, batch_size=256, mode = 'train')\n",
    "    test_loader = get_data_loader(test_dataset, batch_size=512, mode = 'eval')\n",
    "    ## 画出分布\n",
    "    labels_set, counts = np.unique(pred_labels,return_counts=True)\n",
    "    fig = plot_distribution(pred_labels)\n",
    "    fig.savefig(os.path.join(path, f'Ditribution after {total_epochs}_no3.png'), dpi = 300)\n",
    "    fig.show()\n",
    "    ## test\n",
    "    df = pd.DataFrame({'id': np.arange(0,len(pred_labels)), 'label': pred_labels})\n",
    "    df.to_csv(f'DaNN_submission_no3_{total_epochs}.csv',index=False)\n",
    "    # train\n",
    "    feature_extractor.train()\n",
    "    label_predictor.train()\n",
    "    domain_classifier.train()\n",
    "    for epoch in range(total_epochs+1,total_epochs+ epochs+1):\n",
    "        st = time.time()\n",
    "        train_D_loss, train_F_loss, train_acc = train_epoch(source_loader, target_loader, lamb=0.1)\n",
    "        ed = time.time()\n",
    "        if epoch % 100 == 0:\n",
    "            save_model(feature_extractor,  f'extractor_model_no3_{epoch}.bin')\n",
    "            save_model(label_predictor, f'predictor_model_no3_{epoch}.bin')\n",
    "            save_model(domain_classifier, f'domain_classifier_no3_{epoch}.bin')\n",
    "\n",
    "            \n",
    "        print('{:4.2f}s, epoch [{:0>3d}\\{}]: train D loss: {:6.4f}, train F loss: {:6.6f}, acc {:6.6f}'.format(ed-st, epoch, epochs,  train_D_loss, train_F_loss, train_acc))\n",
    "    total_epochs += epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "h-cLCwhS5Xuf",
    "outputId": "9243dadf-3b92-4dd6-b95d-8bd26295b79a"
   },
   "outputs": [],
   "source": [
    "# 构造新的source data\n",
    "test_loader = get_data_loader(test_dataset, mode = 'eval')\n",
    "pred_labels, pred_probs,_ = testing(feature_extractor, label_predictor, test_loader)\n",
    "idx = (pred_probs.max(axis=1) > threshold)\n",
    "np.random.shuffle(idx)\n",
    "add_data, add_labels = test_loader.dataset.data[idx][:mix_num], pred_labels[idx][:mix_num]\n",
    "source_dataset = AddDataset(np.concatenate([source_x_paths, add_data]), np.concatenate([source_y, add_labels]), source_transform, target_transform)\n",
    "print(f'add : {len(add_data)} new source size: {len(source_dataset)}')\n",
    "source_loader = get_data_loader(source_dataset, mode = 'train')\n",
    "target_loader = get_data_loader(target_dataset, mode = 'train')\n",
    "test_loader = get_data_loader(test_dataset, mode = 'eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "yzpHbjz65Xun",
    "outputId": "c6318f7c-adb0-4e66-fe22-8166b3953256"
   },
   "outputs": [],
   "source": [
    "## 画出分布\n",
    "labels_set, counts = np.unique(pred_labels,return_counts=True)\n",
    "fig = plot_distribution(pred_labels)\n",
    "fig.savefig(os.path.join(path, f'Ditribution after {total_epochs}.png'), dpi = 300)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 801
    },
    "colab_type": "code",
    "id": "cnZsdKZK5Xus",
    "outputId": "289797a3-0784-4c0a-d666-19bfea64894a"
   },
   "outputs": [],
   "source": [
    "# train\n",
    "feature_extractor.train()\n",
    "label_predictor.train()\n",
    "domain_classifier.train()\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(total_epochs+1,total_epochs+ epochs+1):\n",
    "    st = time.time()\n",
    "    train_D_loss, train_F_loss, train_acc = train_epoch(source_loader, target_loader, lamb=0.1)\n",
    "    ed = time.time()\n",
    "    if epoch % 50 == 0:\n",
    "        save_model(feature_extractor,  f'extractor_model_{epoch}.bin')\n",
    "        save_model(label_predictor, f'predictor_model_{epoch}.bin')\n",
    "        save_model(domain_classifier, f'domain_classifier_{epoch}.bin')\n",
    "\n",
    "    print('source size {} {:4.2f}s, epoch [{:0>3d}\\{}]: train D loss: {:6.4f}, train F loss: {:6.6f}, acc {:6.6f}'.format(len(source_loader.dataset), ed-st, epoch, epochs,  train_D_loss, train_F_loss, train_acc))\n",
    "total_epoch += epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gnz5qZ4o5Xuy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#predict\n",
    "pred_labels, pred_probs, latents = testing(feature_extractor, label_predictor, test_loader)\n",
    "# save\n",
    "df = pd.DataFrame({'id': np.arange(0,len(pred_labels)), 'label': pred_labels})\n",
    "df.to_csv(f'DaNN_submission_{ex_name}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTYqUpv3m5bY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw12.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
