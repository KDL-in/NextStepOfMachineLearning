{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "551.2px",
        "left": "672px",
        "top": "66.8px",
        "width": "307.2px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "hw8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgtIlUlN4o4g",
        "colab_type": "code",
        "outputId": "cce7d3d5-9394-40b8-8c76-81c9dc0df90f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "from google.colab import drive as gdrive\n",
        "gdrive.mount('/content/drive')\n",
        "!pip install pytorch-model-summary"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Collecting pytorch-model-summary\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/de/f3548f3081045cfc4020fc297cc9db74839a6849da8a41b89c48a3307da7/pytorch_model_summary-0.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-model-summary) (4.41.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-model-summary) (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-model-summary) (1.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-model-summary) (0.16.0)\n",
            "Installing collected packages: pytorch-model-summary\n",
            "Successfully installed pytorch-model-summary-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLirLyZp58zR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -Rf /content/drive/My\\ Drive/hw8/cmn-eng ./"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjCVQLe24iTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed(9)\n",
        "\n",
        "device = torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBsdsJBJ4iTN",
        "colab_type": "text"
      },
      "source": [
        "# Seq2Seq\n",
        "\n",
        "机器翻译，英文到中文的seq2seq实验。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbGEsBfg4iTO",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEJigvLp4iTP",
        "colab_type": "text"
      },
      "source": [
        "## Data Structure\n",
        "\n",
        "提供训练集、测试集、验证集，都是一句英文一句英文。其中中文利用jieba进行分词，英文使用subword-nmt将word转化为subword。如\"loved\",\"loving\",\"loves\"这三个单词，其本身的语义都是”爱”的意思。BPE通过训练，能够把上面的3个单词拆分成”lov”,”ed”,”ing”,”es”几部分，这样可以把词的本身的意思和时态分开，有效的减少了词表的数量。词与词之间用空白隔开，中英文之间用tab隔开。\n",
        "\n",
        "````python\n",
        "what were you doing in the at@@ tic ? \t你 在 閣樓 上 做 了 什麼 ？ \n",
        "````\n",
        "\n",
        "字典部分，已经处理好中英的字典，放在json文件中，word2int，int2word都有。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul2UNwZF4iTQ",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess - SeqDataset\n",
        "\n",
        "需要做的事主要是：\n",
        "\n",
        "- 特殊字元： < PAD >, < BOS >, < EOS >, < UNK >转化，分别用于填充，标记开始，标记结束，标记未知\n",
        "- 长度规整，输入输出，需要规整到相同长度\n",
        "- word to index，中英文分别处理。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kP0IaIE4iTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SeqDataset(data.Dataset):\n",
        "    def __init__(self, path, name, sen_len):\n",
        "        self.path = path # data path\n",
        "        self.sen_len = sen_len\n",
        "        self.name = name\n",
        "        # load dict\n",
        "        self.word2idx_cn, self.idx2word_cn = self.load_dict('cn')\n",
        "        self.word2idx_en, self.idx2word_en = self.load_dict('en')\n",
        "        # sentence to idx\n",
        "        self.data, self.labels = self.load_data()\n",
        "        self.cn_vocab_size = len(self.word2idx_cn)\n",
        "        self.en_vocab_size = len(self.word2idx_en)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def load_dict(self, lang):\n",
        "        with open(os.path.join(self.path, f'int2word_{lang}.json'), 'r', encoding='utf-8') as f:\n",
        "            idx2word = json.load(f)\n",
        "        with open(os.path.join(self.path, f'word2int_{lang}.json'), 'r', encoding='utf-8') as f:\n",
        "            word2idx = json.load(f)\n",
        "        return word2idx,idx2word\n",
        "    \n",
        "    def load_data(self):    \n",
        "        # building method\n",
        "        def format_len(temp, sen_len, pad):\n",
        "            if len(temp) > sen_len:\n",
        "                end = temp[-1]\n",
        "                temp = temp[:sen_len]\n",
        "                temp[-1] = end\n",
        "            else:\n",
        "                temp = np.pad(temp, (0, sen_len - len(temp)), constant_values = pad)\n",
        "            return np.array(temp)\n",
        "\n",
        "        def sentence_to_idxs(sens, word2idx, sen_len):\n",
        "            data = []\n",
        "            BOS, EOS, UNK, PAD = word2idx['<BOS>'],word2idx['<EOS>'],word2idx['<UNK>'],word2idx['<PAD>']\n",
        "            for sen in sens:\n",
        "                temp = [BOS]\n",
        "                for word in list(filter(None, sen.split(' '))):\n",
        "                    temp.append(word2idx.get(word, UNK))\n",
        "                temp.append(EOS)\n",
        "                temp  = format_len(temp, sen_len, PAD)\n",
        "                data.append(temp[np.newaxis, :])\n",
        "            data = np.concatenate(data)\n",
        "            return data\n",
        "\n",
        "        # read data\n",
        "        with open(os.path.join(self.path, f'{self.name}.txt'), 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "            en,cn = [],[]\n",
        "        lines = list(filter(None, lines))\n",
        "        # split cn en\n",
        "        for line in lines:\n",
        "            temp = re.split('[\\t\\n]', line.strip())\n",
        "            assert len(temp) == 2 and temp[0] is not None and temp[1] is not None\n",
        "            en.append(temp[0])\n",
        "            cn.append(temp[1])\n",
        "        # word to idx\n",
        "        data = sentence_to_idxs(en, self.word2idx_en, self.sen_len)\n",
        "        labels = sentence_to_idxs(cn, self.word2idx_cn, self.sen_len)\n",
        "\n",
        "        return data, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9phcPvo4iTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = './cmn-eng/'\n",
        "name = 'testing'\n",
        "lang = 'cn'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWVPscHu4iTX",
        "colab_type": "code",
        "outputId": "04646c44-187f-4514-a9b7-bfd912964931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# TEST\n",
        "train_set = SeqDataset(path, name, 10)\n",
        "print(train_set.word2idx_cn['快樂'], train_set.idx2word_cn['847'], train_set.word2idx_en['happy'])\n",
        "train_set.labels.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "847 快樂 219\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2636, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yuWxOq_4iTc",
        "colab_type": "text"
      },
      "source": [
        "# Achitecture \n",
        "\n",
        "模型的主体，包含\n",
        "\n",
        "- Encoder\n",
        "- Decoder\n",
        "- Seq2Seq\n",
        "- Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im3lmInp4iTc",
        "colab_type": "text"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UallM0Vr4iTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, en_vocab_size, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(en_vocab_size, emb_dim)\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_):\n",
        "        # input = [batch size, sequence len, vocab size]\n",
        "        embedding = self.embedding(input_)\n",
        "        # embedding = [none, seq_len, emb_dim]\n",
        "        outputs, hidden = self.rnn(self.dropout(embedding))\n",
        "        # outputs = [batch size, sequence len, hid dim * directions]\n",
        "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
        "        # outputs 是最上層RNN的輸出\n",
        "\n",
        "        return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBkkM09A4iTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_model_summary import summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvuUCReG4iTk",
        "colab_type": "code",
        "outputId": "03924d13-2212-4517-8437-d62a4713f3c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "print(summary(Encoder(100,256, 256, 2, 0.5).to(device), torch.zeros((2, 10), dtype = torch.long).to(device), show_hierarchical=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------\n",
            "      Layer (type)                  Output Shape         Param #     Tr. Param #\n",
            "=================================================================================\n",
            "       Embedding-1                  [2, 10, 256]          25,600          25,600\n",
            "         Dropout-2                  [2, 10, 256]               0               0\n",
            "             GRU-3     [2, 10, 512], [4, 2, 256]       1,972,224       1,972,224\n",
            "=================================================================================\n",
            "Total params: 1,997,824\n",
            "Trainable params: 1,997,824\n",
            "Non-trainable params: 0\n",
            "---------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "========================================== Hierarchical Summary ==========================================\n",
            "\n",
            "Encoder(\n",
            "  (embedding): Embedding(100, 256), 25,600 params\n",
            "  (rnn): GRU(256, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True), 1,972,224 params\n",
            "  (dropout): Dropout(p=0.5, inplace=False), 0 params\n",
            "), 1,997,824 params\n",
            "\n",
            "\n",
            "==========================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZLcSbMG4iTp",
        "colab_type": "text"
      },
      "source": [
        "## Attention\n",
        "\n",
        "Attention的实现，主要是通过decoder当前时间步的信息 => seq len个权重。具体的实现方法有很多。我看了一些资料，小结一下。\n",
        "\n",
        "Attenion输入，\n",
        "\n",
        "- encoder_outputs = [none, seq_len, hidden_dim], 注意hidden_dim和encoder中GRU的方向有关，可能得x2\n",
        "- hidden = [num_layers <* num_directions>, batch, hidden_dim] 当前时间步的hidden output，默认情况下，decoder中GRU为单向, 如果使用encoder_hidden做decoder第一个时间步的输入，那么需要把双向的结果接起来，最后维度x2\n",
        "- input = [batch, 1]，输入，因为decoder是单步执行，所以只传一个时间步上的值，经过embedding会变成[batch, 1, emb_dim]\n",
        "\n",
        "notes：attention求法\n",
        "\n",
        "- $\\boldsymbol{h}_{t}^{\\top} \\boldsymbol{W} \\overline{\\boldsymbol{h}}_{s} \\quad$ [Luong's multiplicative style]， 其中h分别为encoder_outputs和hidden\n",
        "- $\\boldsymbol{v}_{a}^{\\top} \\tanh \\left(\\boldsymbol{W}_{1} \\boldsymbol{h}_{t}+\\boldsymbol{W}_{2} \\overline{\\boldsymbol{h}}_{s}\\right)$，其中v，w都为参数矩阵，也就是linear\n",
        "- 只使用input和hidden进行concatenate，然后利用linear转为seq len个单元\n",
        "\n",
        "最后使用softmax求除权重。\n",
        "\n",
        "注意上述过程中，会出现维度不匹配问题，多半是层次数引起的，其实在该维度上，上述方法都可以直接广播复制，只关注最后的维度即可。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8k0FClR4iTp",
        "colab_type": "text"
      },
      "source": [
        "notes：\n",
        "\n",
        "无需担心3d矩阵乘法问题，实际上以下代码只关注dim 2的值，对这个dim上的值做线性变化。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_454jXZt4iTq",
        "colab_type": "code",
        "outputId": "be34c107-f432-487d-e5f8-c846b5438a44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a = torch.zeros(128, 10, 256)\n",
        "linear = nn.Linear(256,10)\n",
        "b = linear(a)\n",
        "print(b.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 10, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEBUQDBk4iTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [2, 10, 512], [4, 2, 256]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToA_I9vM4iTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_layer, hidden_dim):\n",
        "        # 这里的hidden_dim 为decoder的，是encoder的两倍\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.W1 = nn.Linear(num_layer * hidden_dim, hidden_dim)\n",
        "        self.W2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.V = nn.Linear(hidden_dim, 1)\n",
        "    \n",
        "    def forward(self, input_, hidden, encoder_outputs):\n",
        "        '''\n",
        "        input_:  decoder的输入，经过embedding [batch, 1, emb_dim]\n",
        "        hidden: decoder的隐藏层，[num_layers x 1 , batch, hidden_dim], 其中的参数为decoder中的参数\n",
        "        encoder_outputs: encoder输出，[batch, seq_len, hidden_dim]\n",
        "        '''\n",
        "        hidden = torch.cat([hidden[i, :, :] for i in range(hidden.size(0))], dim = 1).unsqueeze(1)\n",
        "        # [batch, 1, total dim], 拼接所有层的最后一个dim\n",
        "        score = torch.tanh(self.W1(hidden) + self.W2(encoder_outputs))\n",
        "        # [batch, seq_len, hidden_dim], 两者通过linear转化最后一个维度，最后相加（中间维度广播道seq len）\n",
        "        score = self.V(score)\n",
        "        # [batch, seq_len,1]\n",
        "        ahlpas = torch.softmax(score, dim = 1)\n",
        "        # [batch, seq_len,1]\n",
        "        context = torch.sum(encoder_outputs * ahlpas, dim = 1)\n",
        "        # [batch, hidden_dim]\n",
        "        return context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV6QfaX74iT0",
        "colab_type": "code",
        "outputId": "f6e37be6-0304-454b-b188-881af5cfc963",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# TEST\n",
        "num_layer = 2\n",
        "hidden_dim = 256 * num_layer\n",
        "input_ = torch.zeros((128, 1), dtype = torch.int64)\n",
        "hidden = torch.zeros((num_layer, 128, hidden_dim))\n",
        "encoder_outputs = torch.zeros((128, 10, hidden_dim))\n",
        "att = Attention(num_layer, hidden_dim)\n",
        "print(att(input_, hidden, encoder_outputs).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxPu7Da04iT3",
        "colab_type": "text"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "Decoder任务比较简单，就是跑数据，不用管teacher force以及beam search，需要注意的是，decoder的输入是当个time step上的数据。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMkOL3yz4iT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, cn_vocab_size,  emb_dim,  hidden_dim, num_layer,dropout, isatt):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(cn_vocab_size, emb_dim)\n",
        "        t_dim =  emb_dim\n",
        "        self.isatt = False\n",
        "        if isatt == True:\n",
        "            self.isatt = isatt\n",
        "            self.att = Attention(num_layer, hidden_dim)\n",
        "            t_dim += hidden_dim\n",
        "        \n",
        "        self.rnn = nn.GRU(t_dim, hidden_dim, num_layer, dropout = dropout, batch_first = True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim * 4),\n",
        "            nn.Linear(hidden_dim * 4, cn_vocab_size)\n",
        "        )\n",
        "        self.cn_vocab_size = cn_vocab_size\n",
        "    def forward(self, input_, hidden, encoder_outputs):\n",
        "        '''\n",
        "        input_ = [batch, 1]\n",
        "        hidden = [num_layer, batch, hidden_dim], 其中hidden dim受GRU层数的影响，方向定为单向\n",
        "        encoder_outputs = [batch, seq_len, hidden_dim]\n",
        "        '''\n",
        "        emb = self.emb(input_)\n",
        "        emb = self.dropout(emb)\n",
        "        in_cat = emb\n",
        "        # [batch, 1, emb_dim]\n",
        "        # attention\n",
        "        if self.isatt == True:\n",
        "            context = self.att(input_, hidden, encoder_outputs)\n",
        "            # context = [batch, hidden_dim]\n",
        "            context = context.unsqueeze(1)\n",
        "            # [batch, 1, hidden_dim]\n",
        "            in_cat = torch.cat([emb, context], dim = 2)\n",
        "        # [batch, i, hidden_dim + emb_dim]\n",
        "        out, hidden = self.rnn(in_cat)\n",
        "\n",
        "        # out = [batch, 1, hidden_dim]\n",
        "        out = out.squeeze(1)\n",
        "        #[batch, hidden_dim]\n",
        "        out = self.fc(out)\n",
        "        # [batch, cn_vocab_size]\n",
        "        return out, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ewj6Tc94iT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST\n",
        "cn_vocab_size, en_vocab_size = 3000, 2500\n",
        "emb_dim, num_layer =  128, 3\n",
        "hidden_dim, dropout = 256, 0.5\n",
        "decoder = Decoder(cn_vocab_size, emb_dim, hidden_dim, num_layer, dropout, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN3TLenA4iT-",
        "colab_type": "code",
        "outputId": "03f057f1-369d-47b4-9da0-a02ca598bb95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        }
      },
      "source": [
        "print(summary(decoder, torch.zeros((2, 1), dtype = torch.long), torch.zeros(3, 2, 256), torch.zeros(2, 10, 256), show_hierarchical=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "      Layer (type)                 Output Shape         Param #     Tr. Param #\n",
            "================================================================================\n",
            "       Embedding-1                  [2, 1, 128]         384,000         384,000\n",
            "         Dropout-2                  [2, 1, 128]               0               0\n",
            "       Attention-3                     [2, 256]         262,913         262,913\n",
            "             GRU-4     [2, 1, 256], [3, 2, 256]       1,282,560       1,282,560\n",
            "          Linear-5                     [2, 512]         131,584         131,584\n",
            "          Linear-6                    [2, 1024]         525,312         525,312\n",
            "          Linear-7                    [2, 3000]       3,075,000       3,075,000\n",
            "================================================================================\n",
            "Total params: 5,661,369\n",
            "Trainable params: 5,661,369\n",
            "Non-trainable params: 0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "================================ Hierarchical Summary ================================\n",
            "\n",
            "Decoder(\n",
            "  (emb): Embedding(3000, 128), 384,000 params\n",
            "  (att): Attention(\n",
            "    (W1): Linear(in_features=768, out_features=256, bias=True), 196,864 params\n",
            "    (W2): Linear(in_features=256, out_features=256, bias=True), 65,792 params\n",
            "    (V): Linear(in_features=256, out_features=1, bias=True), 257 params\n",
            "  ), 262,913 params\n",
            "  (rnn): GRU(384, 256, num_layers=3, batch_first=True, dropout=0.5), 1,282,560 params\n",
            "  (dropout): Dropout(p=0.5, inplace=False), 0 params\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=512, bias=True), 131,584 params\n",
            "    (1): Linear(in_features=512, out_features=1024, bias=True), 525,312 params\n",
            "    (2): Linear(in_features=1024, out_features=3000, bias=True), 3,075,000 params\n",
            "  ), 3,731,896 params\n",
            "), 5,661,369 params\n",
            "\n",
            "\n",
            "======================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc49zRRY4iUB",
        "colab_type": "code",
        "outputId": "e074aa95-701d-4b85-aeaa-7880784efd82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "decoder"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (emb): Embedding(3000, 128)\n",
              "  (att): Attention(\n",
              "    (W1): Linear(in_features=768, out_features=256, bias=True)\n",
              "    (W2): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (V): Linear(in_features=256, out_features=1, bias=True)\n",
              "  )\n",
              "  (rnn): GRU(384, 256, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
              "    (1): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    (2): Linear(in_features=1024, out_features=3000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM-Dwpyn4iUE",
        "colab_type": "text"
      },
      "source": [
        "## Seq2Seq\n",
        "\n",
        "这个负责构建整个模型架构。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhgmYiSd4iUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    \n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "    \n",
        "    def forward(self, input_, target, teacher_force_rate):\n",
        "        '''\n",
        "        input_ = [batch, seq_len], 输入句子样本, en\n",
        "        target = [batch, seq_len], 输出翻译样本，cn\n",
        "        '''\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_)\n",
        "        # encoder_outputs = [batch size, sequence len, hid dim * directions]\n",
        "        # encoder_hidden =  [num_layers * directions, batch size  , hid dim]\n",
        "        shape = encoder_hidden.size()\n",
        "        hidden = encoder_hidden.view(int(shape[0]/2), 2,  shape[1], shape[2])\n",
        "        hidden = torch.cat([hidden[:, i, :, :] for i in range(2)], dim = 2)\n",
        "        # [num_layter, batch_size, hid_dim_dec]\n",
        "        x_dec = target[:, 0:1]\n",
        "        # 预测概率和标签\n",
        "        outputs = torch.zeros(input_.shape[0], input_.shape[1], self.decoder.cn_vocab_size).to(device)\n",
        "        preds = []\n",
        "        for step in range(1, target.size(1)):\n",
        "            # [batch, 1]\n",
        "            out, hidden = self.decoder(x_dec, hidden, encoder_outputs)\n",
        "            # out = [batch, cn_vocab_size]\n",
        "            # hidden = [num_layter, batch_size, hid_dim_dec]\n",
        "            pred = out.argmax(1, keepdim = True)\n",
        "            outputs[:,step, :] = out\n",
        "            teacher_force = random.random() <= teacher_force_rate\n",
        "            x_dec = target[:, step:step+1] if teacher_force else pred\n",
        "            preds.append(pred)\n",
        "        preds = torch.cat(preds, dim = 1).to(device)\n",
        "        # preds = [batch,]\n",
        "        # outputs = [batch, seq_len, cn_vocab_size]\n",
        "        return outputs, preds\n",
        "\n",
        "    def inference(self, input_, target, beam = 3):\n",
        "        '''\n",
        "        input_ = [batch, seq_len], 输入句子样本, en\n",
        "        target = [batch, seq_len], 输出翻译样本，cn\n",
        "        '''\n",
        "        def top_k(k, probs, max_likelihood):\n",
        "            max_likelihood = probs * max_likelihood\n",
        "            _, idxs = max_likelihood.reshape(-1).topk(k)\n",
        "            return idxs/k, idxs%k\n",
        "\n",
        "        #  teacher_force_rate = 0\n",
        "        # ecoder\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_)\n",
        "        # encoder_outputs = [batch size, sequence len, hid dim * directions]\n",
        "        # encoder_hidden =  [num_layers * directions, batch size  , hid dim]\n",
        "        shape = encoder_hidden.size()\n",
        "        hidden = encoder_hidden.view(int(shape[0]/2), 2,  shape[1], shape[2])\n",
        "        hidden = torch.cat([hidden[:, i, :, :] for i in range(2)], dim = 2)\n",
        "        \n",
        "        batch = input_.size(0)\n",
        "        vocab = self.decoder.cn_vocab_size\n",
        "        seq_len = input_.size(1)\n",
        "        # 复制输入为beam宽度\n",
        "        x_decs= target[:, 0].repeat(beam,1,1).to(device)\n",
        "        hiddens = hidden.repeat(beam, 1, 1, 1).to(device)\n",
        "        # 输出初始化为beam宽度\n",
        "        outputs_b = torch.zeros(beam, batch, seq_len, vocab)\n",
        "        preds_b = torch.zeros(beam, batch, seq_len, dtype = torch.long)\n",
        "        parents_b = torch.zeros(beam, batch, seq_len, dtype = torch.int32)\n",
        "        # 该路径上的累计概率\n",
        "        max_likelihood = torch.ones(beam,1)\n",
        "\n",
        "        for step in range(0, target.size(1)):\n",
        "            # step中，宽度为beam的最优结果\n",
        "            outputs = torch.zeros(beam, batch, vocab)\n",
        "            preds = torch.zeros(beam, beam, dtype = torch.long)\n",
        "            probs = torch.zeros(beam, beam)\n",
        "            # 遍历所有beam，预测所有样本\n",
        "            for i, (x_dec, hidden) in enumerate(zip(x_decs, hiddens)):\n",
        "                x_dec,hidden = x_dec.to(device, torch.long), hidden.to(device)\n",
        "                out, hidden = self.decoder(x_dec, hidden, encoder_outputs)\n",
        "                out = out.softmax(dim=1)\n",
        "                prob, pred = out.topk(beam, dim = 1)\n",
        "                outputs[i] = out\n",
        "                hiddens[i] = hidden\n",
        "                preds[i] = pred\n",
        "                probs[i] = prob\n",
        "            # print(f\"x_dec\\n{x_dec}\")\n",
        "            # print(f\"模型输出\\n{prob}\\n{pred}\")\n",
        "            # 再beam x beam个输出中，找到当前输出中的top k个样本的idxs\n",
        "            if step == 0:\n",
        "                #  step = 0时，beam宽度为1，只有一个样本\n",
        "                beam_idxs, top_idxs =  top_k(beam, probs[0,:].reshape(beam,1), max_likelihood)\n",
        "            else: \n",
        "                beam_idxs, top_idxs = top_k(beam, probs, max_likelihood)\n",
        "\n",
        "            # teacher_force = random.random() <= teacher_force_rate\n",
        "            x_decs = preds[beam_idxs, top_idxs].reshape(beam, 1, 1)\n",
        "            outputs_b[:, :, step, :] = outputs[beam_idxs]\n",
        "            preds_b[:, :, step] = x_dec.unsqueeze(1)\n",
        "            parents_b[:, :, step] = beam_idxs.unsqueeze(1)\n",
        "            max_likelihood *= probs[beam_idxs, top_idxs].unsqueeze(1)\n",
        "           # print(f\"模型总输出：\\n{probs}\")\n",
        "           # print(f\"模型总labels: \\n{preds}\")\n",
        "           # print(f\"累计概率为\\n{probs * max_likelihood}\")\n",
        "           # print(f\"模型总选择\\n{x_decs.squeeze()}\\n{beam_idxs}\")\n",
        "           # print(\"-------------------------\")\n",
        "        # 回溯最大路径\n",
        "        outputs = torch.zeros(batch, seq_len, vocab).to(device)\n",
        "        preds = torch.zeros(batch, seq_len, dtype = torch.long).to(device)\n",
        "        step = seq_len-1\n",
        "        idx = max_likelihood.argmax()\n",
        "        idx = parents_b[ idx, 0 ,step]\n",
        "        preds[:,step] = 2\n",
        "        while step > 0:\n",
        "            idx = parents_b[ idx, 0 ,step]\n",
        "            preds[:, step-1], outputs[:, step, : ] = preds_b[idx, 0, step],outputs_b[idx, :, step, :]\n",
        "            step -= 1\n",
        "            \n",
        "        return outputs,preds\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiIdFIrm4iUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST\n",
        "cn_vocab_size, en_vocab_size = train_set.cn_vocab_size,train_set.en_vocab_size\n",
        "emb_dim, num_layer =  128, 3\n",
        "hidden_dim, dropout = 256, 0.5\n",
        "batch_size = 64\n",
        "decoder = Decoder(cn_vocab_size, emb_dim, hidden_dim, num_layer, dropout, True)\n",
        "encoder = Encoder(en_vocab_size, emb_dim, int(hidden_dim/2), num_layer, dropout)\n",
        "seq2seq = Seq2Seq(encoder, decoder)\n",
        "dataloader = data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "for input_, target in dataloader:\n",
        "    break\n",
        "input_ = torch.tensor(input_, dtype = torch.long)\n",
        "target = torch.tensor(target, dtype = torch.long)\n",
        "pred_probs,pred_labels =seq2seq(input_, target,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvKStK_u4iUL",
        "colab_type": "text"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyMW8oHV4iUM",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8U7rL8n4iUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(model, store_model_path, step):\n",
        "    torch.save(model.state_dict(), f'{store_model_path}/model_{step}.ckpt')\n",
        "    return\n",
        "\n",
        "def load_model(model, load_model_path):\n",
        "    print(f'Load model from {load_model_path}')\n",
        "    model.load_state_dict(torch.load(f'{load_model_path}.ckpt'))\n",
        "    return model\n",
        "\n",
        "def build_model(config, en_vocab_size, cn_vocab_size):\n",
        "    # 建構模型\n",
        "    encoder = Encoder(en_vocab_size, config.emb_dim, config.hid_dim, config.n_layers, config.dropout).to(device)\n",
        "    decoder = Decoder(cn_vocab_size, config.emb_dim, config.hid_dim * 2, config.n_layers, config.dropout, config.attention).to(device)\n",
        "    model = Seq2Seq(encoder, decoder)\n",
        "    print(model)\n",
        "    # 建構 optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "    print(optimizer)\n",
        "    if config.load_model:\n",
        "        model = load_model(model, config.load_model_path)\n",
        "    model = model.to(device)\n",
        "\n",
        "    return model, optimizer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbqE2AHP4iUP",
        "colab_type": "text"
      },
      "source": [
        "## Other"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QkrHlU04iUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokens2sentence(outputs, int2word):\n",
        "    sentences = []\n",
        "    for tokens in outputs:\n",
        "        sentence = []\n",
        "        for token in tokens:\n",
        "            word = int2word[str(int(token))]\n",
        "            if word == '<EOS>':\n",
        "                break\n",
        "            sentence.append(word)\n",
        "        sentences.append(sentence)\n",
        "  \n",
        "    return sentences\n",
        "\n",
        "def infinite_iter(data_loader):\n",
        "    it = iter(data_loader)\n",
        "    while True:\n",
        "        try:\n",
        "            ret = next(it)\n",
        "            yield ret\n",
        "        except StopIteration:\n",
        "            it = iter(data_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72LjLlol4iUS",
        "colab_type": "text"
      },
      "source": [
        "## BLEU score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7VIDiT34iUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "def computebleu(sentences, targets):\n",
        "    score = 0 \n",
        "    assert (len(sentences) == len(targets))\n",
        "    #  cut_token 将中文分词切分成字\n",
        "    def cut_token(sentence):\n",
        "        tmp = []\n",
        "        for token in sentence:\n",
        "            if token == '<UNK>' or token.isdigit() or len(bytes(token[0], encoding='utf-8')) == 1:\n",
        "                tmp.append(token)\n",
        "            else:\n",
        "                tmp += [word for word in token]\n",
        "        return tmp \n",
        "\n",
        "    for sentence, target in zip(sentences, targets):\n",
        "        sentence = cut_token(sentence)\n",
        "        target = cut_token(target)\n",
        "        # notes: bleus score，其中weight指定的是n-grams的权重，reference需要是一个列表，我还不知道为什么\n",
        "        score += sentence_bleu([target], sentence, weights=(1, 0, 0, 0))                                                                                          \n",
        "    return score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nutyedui4iUV",
        "colab_type": "text"
      },
      "source": [
        "## PLOT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YXW9e3W4iUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot(train_losses, val_losses, bleu_scores):\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses)\n",
        "    plt.xlabel('次數')\n",
        "    plt.ylabel('loss')\n",
        "    plt.title('train loss')\n",
        "    plt.show()\n",
        "    plt.figure()\n",
        "    plt.plot(val_losses)\n",
        "    plt.xlabel('次數')\n",
        "    plt.ylabel('loss')\n",
        "    plt.title('validation loss')\n",
        "    plt.show()\n",
        "    plt.figure()\n",
        "    plt.plot(bleu_scores)\n",
        "    plt.xlabel('次數')\n",
        "    plt.ylabel('BLEU score')\n",
        "    plt.title('BLEU score')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWTZ8q3W4iUY",
        "colab_type": "text"
      },
      "source": [
        "## Schedule Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9BU9fb54iUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def schedule_sampling(k=900, mode = 'rev_sigmoid'):\n",
        "    if mode == 'none':\n",
        "        while True:\n",
        "            yield 0\n",
        "    elif mode == 'always':\n",
        "        while True:\n",
        "            yield 1\n",
        "    elif mode == 'rev_sigmoid':\n",
        "        i = 0\n",
        "        while True:\n",
        "            ret = k/(k + np.exp(i/k))\n",
        "            i+=1\n",
        "            yield ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgqEpmJD4iUc",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STQeSxWU4iUd",
        "colab_type": "text"
      },
      "source": [
        "## train epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apgGaoqr4iUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, train_iter, loss_function, total_steps, summary_steps, train_dataset):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    losses = []\n",
        "    loss_sum = 0.0\n",
        "    for step in range(summary_steps):\n",
        "        sources, targets = next(train_iter)\n",
        "        sources, targets = sources.to(device, torch.long), targets.to(device, torch.long)\n",
        "        outputs, preds = model(sources, targets, next(config.schedule_sampling))\n",
        "        # notes: seq2seq loss计算，问题一，忽略<BOS>\n",
        "        # notes: seq2seq loss计算，问题二，由于cross_entropy直接收二维数据，这里直接reshape到二维\\\n",
        "        outputs = outputs[:, 1:].reshape(-1, outputs.size(2))\n",
        "        targets = targets[:, 1:].reshape(-1)\n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # notes: seq2seq 梯度限制，nlp lstm会遇到的问题之一，函数部分区域非常陡峭，梯度会突然很大，导致无法训练\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "        optimizer.step()\n",
        "        # notes: loss技巧，下面使用了exp(loss)，方便观察变化\n",
        "        # 每五次step打印一次，打印loss平均值，不再以\"epoch\"为单位\n",
        "        loss_sum += loss.item()\n",
        "        if (step + 1) % 5 == 0:\n",
        "            loss_sum = loss_sum / 5\n",
        "            print (\"\\r\", \"train [{}] loss: {:.3f}, Perplexity: {:.3f}      \".format(total_steps + step + 1, loss_sum, np.exp(loss_sum)), end=\" \")\n",
        "            losses.append(loss_sum)\n",
        "            loss_sum = 0.0\n",
        "\n",
        "    return model, optimizer, losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyMcjkR64iUg",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRgfhsuH4iUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD2qBcFB4iUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, dataloader, loss_function):\n",
        "    model.eval()\n",
        "    loss_sum, bleu_score= 0.0, 0.0\n",
        "    n = 0\n",
        "    result = []\n",
        "    time_start = time.time()\n",
        "    for sources, targets in dataloader:\n",
        "        sources, targets = sources.to(device, torch.long), targets.to(device, torch.long)\n",
        "        batch_size = sources.size(0)\n",
        "        outputs, preds = model.inference(sources, targets, beam = config.beam)\n",
        "        \n",
        "        outputs = outputs[:, 1:].reshape(-1, outputs.size(2))\n",
        "        targets = targets[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss_sum += loss.item()\n",
        "\n",
        "        # 將預測結果轉為文字\n",
        "        targets = targets.view(sources.size(0), -1) # 维度恢复\n",
        "        preds = tokens2sentence(preds, dataloader.dataset.idx2word_cn)\n",
        "        sources = tokens2sentence(sources, dataloader.dataset.idx2word_en)\n",
        "        targets = tokens2sentence(targets, dataloader.dataset.idx2word_cn)\n",
        "        for source, pred, target in zip(sources, preds, targets):\n",
        "            result.append((source, pred, target))\n",
        "        # 計算 Bleu Score\n",
        "        bleu_score += computebleu(preds, targets)\n",
        "        n += batch_size\n",
        "    time_end = time.time()\n",
        "    # print(time_end - time_start)\n",
        "    return loss_sum / len(dataloader), bleu_score / n, result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VUfIfrF4iUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_dataset = SeqDataset(config.data_path, 'training', config.max_output_len)\n",
        "# model, optimizer = build_model(config, train_dataset.en_vocab_size, train_dataset.cn_vocab_size)\n",
        "# loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
        "# val_dataset = SeqDataset(config.data_path, 'validation', config.max_output_len)\n",
        "# val_loader = data.DataLoader(val_dataset, batch_size=1)\n",
        "# test(model, val_loader, loss_function)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCMAS-Vr4iUs",
        "colab_type": "text"
      },
      "source": [
        "## Train Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3GgCyBt4iUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# notes: 大量参数的参数技巧，简化api，config存放可变参数，尽量不用全局变量，但config不接触底层\n",
        "def train_process(config):\n",
        "    # 準備訓練資料\n",
        "    train_dataset = SeqDataset(config.data_path, 'training', config.max_output_len)\n",
        "    train_loader = data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "    train_iter = infinite_iter(train_loader)\n",
        "    # 準備檢驗資料\n",
        "    # valid过程无法批量操作\n",
        "    val_dataset = SeqDataset(config.data_path, 'validation', config.max_output_len)\n",
        "    val_loader = data.DataLoader(val_dataset, batch_size=1)\n",
        "    # 建構模型\n",
        "    model, optimizer = build_model(config, train_dataset.en_vocab_size, train_dataset.cn_vocab_size)\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    train_losses, val_losses, bleu_scores = [], [], []\n",
        "    total_steps = 0\n",
        "    # notes：训练的另一种写法，无epoch如何控制训练过程\n",
        "    while (total_steps < config.num_steps):\n",
        "        # 訓練模型\n",
        "        model, optimizer, loss = train(model, optimizer, train_iter, loss_function, total_steps, config.summary_steps, train_dataset)\n",
        "        train_losses += loss\n",
        "        # 檢驗模型\n",
        "    \n",
        "        val_loss, bleu_score, result = test(model, val_loader, loss_function)\n",
        "        val_losses.append(val_loss)\n",
        "        bleu_scores.append(bleu_score)\n",
        "        \n",
        "        total_steps += config.summary_steps\n",
        "        print (\"\\r\", \"val [{}] loss: {:.3f}, Perplexity: {:.3f}, bleu score: {:.3f}       \".format(total_steps, val_loss, np.exp(val_loss), bleu_score))\n",
        "\n",
        "        # 儲存模型和結果\n",
        "        # notes：机器翻译任务early stop，因为bleus不像acc，它不是精准的指标，所以此处每隔一定的step保存一次模型，靠人为选择出合适的模型\n",
        "        if total_steps % config.store_steps == 0 or total_steps >= config.num_steps:\n",
        "            save_model(model, config.store_model_path, total_steps)\n",
        "            with open(f'{config.store_model_path}/output_{total_steps}.txt', 'w') as f:\n",
        "                for en, p, cn in result:\n",
        "                    print (' '.join(en), ' '.join(cn),  ' '.join(p), file=f)\n",
        "    \n",
        "    return train_losses, val_losses, bleu_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-dXgNFW4iUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #     train_dataset = SeqDataset(config.data_path, 'training', config.max_output_len)\n",
        "#     train_loader = data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "#     train_iter = infinite_iter(train_loader)\n",
        "#     # 準備檢驗資料\n",
        "#     # valid过程无法批量操作\n",
        "#     val_dataset = SeqDataset(config.data_path, 'validation', config.max_output_len)\n",
        "#     val_loader = data.DataLoader(val_dataset, batch_size=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xths_ZJl4iU5",
        "colab_type": "text"
      },
      "source": [
        "## Test Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9coOvV74iU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_process(config):\n",
        "    # 準備測試資料\n",
        "    test_dataset = SeqDataset(config.data_path, 'testing', config.max_output_len)\n",
        "    # 无法批量操作，所以只能当个执行，效率也很慢就是\n",
        "    test_loader = data.DataLoader(test_dataset, batch_size=1)\n",
        "    # 建構模型\n",
        "    model, optimizer = build_model(config, test_dataset.en_vocab_size, test_dataset.cn_vocab_size)\n",
        "    print (\"Finish build model\")\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    model.eval()\n",
        "    # 測試模型\n",
        "    test_loss, bleu_score, result = test(model, test_loader, loss_function)\n",
        "    # 儲存結果\n",
        "    with open(f'./test_output.txt', 'w') as f:\n",
        "        for line in result:\n",
        "            print (line, file=f)\n",
        "\n",
        "    return test_loss, bleu_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QSzhlZA4iU8",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khIG9DZM4iU8",
        "colab_type": "text"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E0w4ymx4iU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class configurations(object):\n",
        "    def __init__(self):\n",
        "        self.batch_size = 60\n",
        "        self.emb_dim = 128\n",
        "        self.hid_dim = 128\n",
        "        self.n_layers = 3\n",
        "        self.dropout = 0.5\n",
        "        self.learning_rate = 0.00005\n",
        "        self.max_output_len = 50              # 最後輸出句子的最大長度\n",
        "        self.num_steps = 3000                # 總訓練次數\n",
        "        self.store_steps = 100                # 訓練多少次後須儲存模型\n",
        "        self.summary_steps = 100              # 訓練多少次後須檢驗是否有overfitting\n",
        "        self.load_model = False               # 是否需載入模型\n",
        "        self.load_model_path = None           # 載入模型的位置 e.g. \"./ckpt/model_{step}\" \n",
        "        self.attention = True                # 是否使用 Attention Mechanism\n",
        "        self.schedule_sampling = schedule_sampling(mode = 'always')\n",
        "        self.base_path = './'\n",
        "        self.store_model_path = os.path.join(self.base_path, 'ckpt')     # 儲存模型的位置\n",
        "        self.data_path =  os.path.join(self.base_path, 'cmn-eng')         # 資料存放的位置"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drlcWL0g4iU_",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Zv-b5_eZ4iVA",
        "colab_type": "code",
        "outputId": "c02aa8a6-36fe-4fc1-d01e-18b5b6397041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    config = configurations()\n",
        "    print ('config:\\n', vars(config))\n",
        "    set_seed(9)\n",
        "    # train_losses, val_dataset_losses, bleu_scores = train_process(config)\n",
        "    # plot(train_lossesn, val_dataset_losses, bleu_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "config:\n",
            " {'batch_size': 60, 'emb_dim': 128, 'hid_dim': 128, 'n_layers': 3, 'dropout': 0.5, 'learning_rate': 5e-05, 'max_output_len': 50, 'num_steps': 3000, 'store_steps': 100, 'summary_steps': 100, 'load_model': False, 'load_model_path': None, 'attention': True, 'schedule_sampling': <generator object schedule_sampling at 0x7f4ce8711f68>, 'base_path': './', 'store_model_path': './ckpt', 'data_path': './cmn-eng'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0h_11x74iVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # summary\n",
        "# train_dataset = SeqDataset(config.data_path, 'training', config.max_output_len)\n",
        "# train_loader = data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "# train_iter = infinite_iter(train_loader)\n",
        "# # 準備檢驗資料\n",
        "# # valid过程无法批量操作\n",
        "# val_dataset = SeqDataset(config.data_path, 'validation', config.max_output_len)\n",
        "# val_loader = data.DataLoader(val_dataset, batch_size=1)\n",
        "# # 建構模型\n",
        "# model, optimizer = build_model(config, train_dataset.en_vocab_size, train_dataset.cn_vocab_size)\n",
        "# input_,target = next(train_iter)\n",
        "# print(summary(model, input_.to(device, torch.long), target.to(device, torch.long), 1,show_hierarchical=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PleF88z4iVF",
        "colab_type": "text"
      },
      "source": [
        "# 实验\n",
        "\n",
        "其中，test和val的时候，teacher force都强制关掉。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NCdRbl44iVG",
        "colab_type": "text"
      },
      "source": [
        "## 无Teacher Force\n",
        "\n",
        "结论：\n",
        "\n",
        "没有跑完实验，表现很差，loss和bleu都不好。\n",
        "\n",
        "其实会这样的原因很好理解，观察这里bleu = 0.109的text可以发现\n",
        "\n",
        "> \\<BOS> almost everyone in our village is related to one another . 我們\\ <UNK> 所有 的 \\<UNK> 幾乎 彼此 都 是 親戚 。 我 是 \\<UNK> 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
        "\n",
        "可以看到，预测的文本中，有大量类似重复错误，这样的话，如果训练阶段再每个time step上使用这些预测作为输入，那么会引起强烈的exposure bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6OdrBor4iVG",
        "colab_type": "code",
        "outputId": "5a4e1d2e-bbfe-4043-f5e6-acacf12cd33b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 828
        }
      },
      "source": [
        "class configurations(object):\n",
        "    def __init__(self):\n",
        "        self.batch_size = 60\n",
        "        self.emb_dim = 256\n",
        "        self.hid_dim = 512\n",
        "        self.n_layers = 3\n",
        "        self.dropout = 0.5\n",
        "\n",
        "        self.max_output_len = 50              # 最後輸出句子的最大長度\n",
        "        self.num_steps = 12000                # 總訓練次數\n",
        "        self.store_steps = 300                # 訓練多少次後須儲存模型\n",
        "        self.summary_steps = 300              # 訓練多少次後須檢驗是否有overfitting\n",
        "              \n",
        "        self.attention = False                # 是否使用 Attention Mechanism\n",
        "        self.base_path = './drive/My Drive/hw8'\n",
        "        self.schedule_sampling = schedule_sampling(mode = 'none')\n",
        "        self.store_model_path = os.path.join(self.base_path, 'ckpt')     # 儲存模型的位置\n",
        "        self.data_path =  os.path.join(self.base_path, 'cmn-eng')         # 資料存放的位置\n",
        "\n",
        "        self.learning_rate = 0.001\n",
        "        self.load_model = False               # 是否需載入模型\n",
        "        self.load_model_path = os.path.join(self.base_path+'/ckpt', 'model_3600') # 載入模型的位置 e.g. \"./ckpt/model_{step}\" \n",
        "        \n",
        "if __name__ == '__main__':\n",
        "    config = configurations()\n",
        "    print ('config:\\n', vars(config))\n",
        "    set_seed(9)\n",
        "    train_losses, val_dataset_losses, bleu_scores = train_process(config)\n",
        "    plot(train_lossesn, val_dataset_losses, bleu_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "config:\n",
            " {'batch_size': 60, 'emb_dim': 256, 'hid_dim': 512, 'n_layers': 3, 'dropout': 0.5, 'max_output_len': 50, 'num_steps': 12000, 'store_steps': 300, 'summary_steps': 300, 'attention': False, 'base_path': './drive/My Drive/hw8', 'schedule_sampling': <generator object schedule_sampling at 0x7f4ce87bcf68>, 'store_model_path': './drive/My Drive/hw8/ckpt', 'data_path': './drive/My Drive/hw8/cmn-eng', 'learning_rate': 0.001, 'load_model': False, 'load_model_path': './drive/My Drive/hw8/ckpt/model_3600'}\n",
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(3922, 256)\n",
            "    (rnn): GRU(256, 512, num_layers=3, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (emb): Embedding(3805, 256)\n",
            "    (rnn): GRU(256, 1024, num_layers=3, batch_first=True, dropout=0.5)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "    (fc): Sequential(\n",
            "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "      (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
            "      (2): Linear(in_features=4096, out_features=3805, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-27c03c150ea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'config:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_lossesn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-2167785ca8f5>\u001b[0m in \u001b[0;36mtrain_process\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_steps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# 訓練模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mtrain_losses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# 檢驗模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-3fae520eb67e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_iter, loss_function, total_steps, summary_steps, train_dataset)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule_sampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# notes: seq2seq loss计算，问题一，忽略<BOS>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# notes: seq2seq loss计算，问题二，由于cross_entropy直接收二维数据，这里直接reshape到二维\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-320ecf37de47>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_, target, teacher_force_rate)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# [batch, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;31m# out = [batch, cn_vocab_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# hidden = [num_layter, batch_size, hid_dim_dec]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-e4d28c3cd5a2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0min_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# [batch, i, hidden_dim + emb_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# out = [batch, 1, hidden_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 727\u001b[0;31m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cj3w5o_HYZs",
        "colab_type": "text"
      },
      "source": [
        "## Teacher Force = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J_6BZ1xHcuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class configurations(object):\n",
        "    def __init__(self):\n",
        "        self.batch_size = 60\n",
        "        self.emb_dim = 256\n",
        "        self.hid_dim = 512\n",
        "        self.n_layers = 3\n",
        "        self.dropout = 0.5\n",
        "\n",
        "        self.max_output_len = 50              # 最後輸出句子的最大長度\n",
        "        self.num_steps = 12000                # 總訓練次數\n",
        "        self.store_steps = 300                # 訓練多少次後須儲存模型\n",
        "        self.summary_steps = 300              # 訓練多少次後須檢驗是否有overfitting\n",
        "              \n",
        "        self.attention = False                # 是否使用 Attention Mechanism\n",
        "        self.base_path = './drive/My Drive/hw8'\n",
        "        self.schedule_sampling = schedule_sampling(mode = 'always')\n",
        "        self.store_model_path = os.path.join(self.base_path, 'ckpt')     # 儲存模型的位置\n",
        "        self.data_path =  os.path.join(self.base_path, 'cmn-eng')         # 資料存放的位置\n",
        "\n",
        "        self.learning_rate = 0.001\n",
        "        self.load_model = True               # 是否需載入模型\n",
        "        self.load_model_path = os.path.join(self.base_path+'/ckpt', 'model_3000') # 載入模型的位置 e.g. \"./ckpt/model_{step}\" \n",
        "        \n",
        "if __name__ == '__main__':\n",
        "    config = configurations()\n",
        "    print ('config:\\n', vars(config))\n",
        "    set_seed(9)\n",
        "    train_losses, val_dataset_losses, bleu_scores = train_process(config)\n",
        "    plot(train_lossesn, val_dataset_losses, bleu_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHRo1mT5Paea",
        "colab_type": "text"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szqmM2VNJh1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class configurations(object):\n",
        "    def __init__(self):\n",
        "        self.batch_size = 60\n",
        "        self.emb_dim = 256\n",
        "        self.hid_dim = 512\n",
        "        self.n_layers = 3\n",
        "        self.dropout = 0.5\n",
        "\n",
        "        self.max_output_len = 50              # 最後輸出句子的最大長度\n",
        "        self.num_steps = 3000                # 總訓練次數\n",
        "        self.store_steps = 300                # 訓練多少次後須儲存模型\n",
        "        self.summary_steps = 300              # 訓練多少次後須檢驗是否有overfitting\n",
        "        \n",
        "        self.load_model_path = None           # 載入模型的位置 e.g. \"./ckpt/model_{step}\" \n",
        "        self.attention = True                # 是否使用 Attention Mechanism\n",
        "        self.base_path = './drive/My Drive/hw8'\n",
        "        self.schedule_mode = 'always'\n",
        "        self.schedule_sampling = schedule_sampling(mode = 'always')\n",
        "        self.store_model_path = os.path.join(self.base_path, 'ckpt')     # 儲存模型的位置\n",
        "        self.data_path =  os.path.join(self.base_path, 'cmn-eng')         # 資料存放的位置\n",
        "        \n",
        "        self.learning_rate = 0.00005\n",
        "        self.learning_rate = 0.001\n",
        "        self.learning_rate = 0.0003\n",
        "        self.load_model = True               # 是否需載入模型\n",
        "        self.load_model_path = os.path.join(self.base_path+'/ckpt', 'model_3600')\n",
        "        \n",
        "if __name__ == '__main__':\n",
        "    config = configurations()\n",
        "    print ('config:\\n', vars(config))\n",
        "    set_seed(9)\n",
        "    train_losses, val_dataset_losses, bleu_scores = train_process(config)\n",
        "    plot(train_lossesn, val_dataset_losses, bleu_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgU4aVuQRXFQ",
        "colab_type": "text"
      },
      "source": [
        "## Schedule Sampling\n",
        "\n",
        "结论：\n",
        "微调learning rate(0.001, 0.0003, 0.00001)，跑出来的最好的结果是0.566，应该算是正常成绩了。\n",
        ">self.learning_rate = 0.0003 # 0.559\n",
        ">\n",
        ">self.learning_rate = 0.00001 # 0.566"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY0_21QIhazx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class configurations(object):\n",
        "    def __init__(self):\n",
        "        self.batch_size = 60\n",
        "        self.emb_dim = 256\n",
        "        self.hid_dim = 512\n",
        "        self.n_layers = 3\n",
        "        self.dropout = 0.5\n",
        "\n",
        "        self.max_output_len = 50              # 最後輸出句子的最大長度\n",
        "        self.num_steps = 12000                # 總訓練次數\n",
        "        self.store_steps = 300                # 訓練多少次後須儲存模型\n",
        "        self.summary_steps = 300              # 訓練多少次後須檢驗是否有overfitting\n",
        "        \n",
        "        self.load_model_path = None           # 載入模型的位置 e.g. \"./ckpt/model_{step}\" \n",
        "        self.attention = True                # 是否使用 Attention Mechanism\n",
        "        self.base_path = './drive/My Drive/hw8'\n",
        "        self.schedule_sampling = schedule_sampling(mode = 'rev_sigmoid')\n",
        "        self.store_model_path = os.path.join(self.base_path, 'ckpt')     # 儲存模型的位置\n",
        "        self.data_path =  os.path.join(self.base_path, 'cmn-eng')         # 資料存放的位置\n",
        "\n",
        "        self.learning_rate = 0.0003\n",
        "        self.load_model = False               # 是否需載入模型\n",
        "        self.load_model_path = os.path.join(self.base_path+'/ckpt', 'model_3600')\n",
        "        \n",
        "if __name__ == '__main__':\n",
        "    config = configurations()\n",
        "    print ('config:\\n', vars(config))\n",
        "    set_seed(9)\n",
        "    train_losses, val_dataset_losses, bleu_scores = train_process(config)\n",
        "    plot(train_lossesn, val_dataset_losses, bleu_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjhtGuveiMvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class configurations(object):\n",
        "    def __init__(self):\n",
        "        self.batch_size = 60\n",
        "        self.emb_dim = 256\n",
        "        self.hid_dim = 512\n",
        "        self.n_layers = 3\n",
        "        self.dropout = 0.5\n",
        "\n",
        "        self.max_output_len = 50              # 最後輸出句子的最大長度\n",
        "        self.num_steps = 3000                # 總訓練次數\n",
        "        self.store_steps = 300                # 訓練多少次後須儲存模型\n",
        "        self.summary_steps = 300              # 訓練多少次後須檢驗是否有overfitting\n",
        "        \n",
        "        self.load_model_path = None           # 載入模型的位置 e.g. \"./ckpt/model_{step}\" \n",
        "        self.attention = True                # 是否使用 Attention Mechanism\n",
        "        self.base_path = './drive/My Drive/hw8'\n",
        "        self.schedule_sampling = schedule_sampling(mode = 'always')\n",
        "        self.store_model_path = os.path.join(self.base_path, 'ckpt')     # 儲存模型的位置\n",
        "        self.data_path =  os.path.join(self.base_path, 'cmn-eng')         # 資料存放的位置\n",
        "\n",
        "        self.load_model = True               # 是否需載入模型\n",
        "        self.load_model_path = os.path.join(self.base_path+'/ckpt', 'model_3000')\n",
        "        \n",
        "if __name__ == '__main__':\n",
        "    config = configurations()\n",
        "    print ('config:\\n', vars(config))\n",
        "    set_seed(9)\n",
        "    train_losses, val_dataset_losses, bleu_scores = train_process(config)\n",
        "    # plot(train_lossesn, val_dataset_losses, bleu_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ieCKZ1C80sS",
        "colab_type": "text"
      },
      "source": [
        "## Beam Search\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9RjXvVy9K_Q",
        "colab_type": "code",
        "outputId": "44425d22-3021-402a-b859-610a3a17d96e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        }
      },
      "source": [
        "class configurations(object):\n",
        "    def __init__(self):\n",
        "        self.batch_size = 60\n",
        "        self.emb_dim = 256\n",
        "        self.hid_dim = 512\n",
        "        self.n_layers = 3\n",
        "        self.dropout = 0.5\n",
        "\n",
        "        self.max_output_len = 50              # 最後輸出句子的最大長度\n",
        "        self.num_steps = 3000                # 總訓練次數\n",
        "        self.store_steps = 300                # 訓練多少次後須儲存模型\n",
        "        self.summary_steps = 300              # 訓練多少次後須檢驗是否有overfitting\n",
        "        \n",
        "        self.load_model_path = None           # 載入模型的位置 e.g. \"./ckpt/model_{step}\" \n",
        "        self.attention = True                # 是否使用 Attention Mechanism\n",
        "        self.base_path = './drive/My Drive/hw8'\n",
        "        self.schedule_sampling = schedule_sampling(mode = 'always')\n",
        "        self.store_model_path = os.path.join(self.base_path, 'ckpt')     # 儲存模型的位置\n",
        "        self.data_path =  os.path.join(self.base_path, 'cmn-eng')         # 資料存放的位置\n",
        "\n",
        "        self.learning_rate = 0.0001\n",
        "        self.load_model = True               # 是否需載入模型\n",
        "        self.load_model_path = os.path.join(self.base_path+'/ckpt', 'model_att_ss')\n",
        "        \n",
        "if __name__ == '__main__':\n",
        "    config = configurations()\n",
        "    print ('config:\\n', vars(config))\n",
        "    set_seed(9)\n",
        "    test_loss, bleu_scores = test_process(config)\n",
        "    # train_losses, val_dataset_losses, bleu_scores = train_process(config)\n",
        "    # plot(train_lossesn, val_dataset_losses, bleu_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "config:\n",
            " {'batch_size': 60, 'emb_dim': 256, 'hid_dim': 512, 'n_layers': 3, 'dropout': 0.5, 'max_output_len': 50, 'num_steps': 3000, 'store_steps': 300, 'summary_steps': 300, 'load_model_path': './drive/My Drive/hw8/ckpt/model_att_ss', 'attention': True, 'base_path': './drive/My Drive/hw8', 'schedule_sampling': <generator object schedule_sampling at 0x7f4ce7d3ef68>, 'store_model_path': './drive/My Drive/hw8/ckpt', 'data_path': './drive/My Drive/hw8/cmn-eng', 'learning_rate': 0.0001, 'load_model': True}\n",
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(3922, 256)\n",
            "    (rnn): GRU(256, 512, num_layers=3, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (emb): Embedding(3805, 256)\n",
            "    (att): Attention(\n",
            "      (W1): Linear(in_features=3072, out_features=1024, bias=True)\n",
            "      (W2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (V): Linear(in_features=1024, out_features=1, bias=True)\n",
            "    )\n",
            "    (rnn): GRU(1280, 1024, num_layers=3, batch_first=True, dropout=0.5)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "    (fc): Sequential(\n",
            "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "      (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
            "      (2): Linear(in_features=4096, out_features=3805, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.0001\n",
            "    weight_decay: 0\n",
            ")\n",
            "Load model from ./drive/My Drive/hw8/ckpt/model_att_ss\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-3dbfce5237bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'config:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;31m# train_losses, val_dataset_losses, bleu_scores = train_process(config)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# plot(train_lossesn, val_dataset_losses, bleu_scores)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-3d1d0b2883b3>\u001b[0m in \u001b[0;36mtest_process\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# 建構模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcn_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Finish build model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-cff25f2d83bd>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(config, en_vocab_size, cn_vocab_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-cff25f2d83bd>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model, load_model_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Load model from {load_model_path}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{load_model_path}.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    727\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torch_load_uninitialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mstorage_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    139\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0GAAMgw_Cmq",
        "colab_type": "code",
        "outputId": "5f9cc325-caa3-43e0-e1fd-90ceeee5d2de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# val \n",
        "test_dataset = SeqDataset(config.data_path, 'validation', config.max_output_len)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=1)\n",
        "# 建構模型\n",
        "model, optimizer = build_model(config, test_dataset.en_vocab_size, test_dataset.cn_vocab_size)\n",
        "print (\"Finish build model\")\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
        "test(model, test_loader, loss_function)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(3922, 256)\n",
            "    (rnn): GRU(256, 512, num_layers=3, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (emb): Embedding(3805, 256)\n",
            "    (att): Attention(\n",
            "      (W1): Linear(in_features=3072, out_features=1024, bias=True)\n",
            "      (W2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (V): Linear(in_features=1024, out_features=1, bias=True)\n",
            "    )\n",
            "    (rnn): GRU(1280, 1024, num_layers=3, batch_first=True, dropout=0.5)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "    (fc): Sequential(\n",
            "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "      (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
            "      (2): Linear(in_features=4096, out_features=3805, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.0001\n",
            "    weight_decay: 0\n",
            ")\n",
            "Load model from ./drive/My Drive/hw8/ckpt/model_att_ss\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-5a58c0c9b241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 建構模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcn_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Finish build model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-f37d77765960>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(config, en_vocab_size, cn_vocab_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-f37d77765960>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model, load_model_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Load model from {load_model_path}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{load_model_path}.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    727\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torch_load_uninitialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mstorage_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    139\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYgigOIQ-XIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# top1\n",
        "# test 0.49573285439587084\n",
        "# val 0.525045433745836\n",
        "# beam1\n",
        "# val \n",
        "# beam3\n",
        "# val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwldbRDPAXJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}