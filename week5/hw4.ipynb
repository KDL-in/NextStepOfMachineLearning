{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14262,
     "status": "ok",
     "timestamp": 1586679072474,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "BE-V3cxNaVb8",
    "outputId": "748aa346-6df5-4c77-ccde-9d64c800ceea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Error loading WordNetLemmatizer: Package\n",
      "[nltk_data]     'WordNetLemmatizer' not found in index\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "Collecting modelsummary\n",
      "  Downloading https://files.pythonhosted.org/packages/36/98/08d46b021de6aebad12ac368d03afe41d399285b2629cff2a2c076822981/modelsummary-1.1.7.tar.gz\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from modelsummary) (4.38.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from modelsummary) (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from modelsummary) (1.18.2)\n",
      "Building wheels for collected packages: modelsummary\n",
      "  Building wheel for modelsummary (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for modelsummary: filename=modelsummary-1.1.7-cp36-none-any.whl size=6955 sha256=d5c3cbb338d1222037a77349df758cc6f890953aee3d55a37dcec32a3fbc449a\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/d3/e8/1ed6d9523b8480be97ccded9ecfed3413e84c73dae12bcc329\n",
      "Successfully built modelsummary\n",
      "Installing collected packages: modelsummary\n",
      "Successfully installed modelsummary-1.1.7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "nltk.download('wordnet')\n",
    "nltk.download('WordNetLemmatizer')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download(\"stopwords\")\n",
    "!pip install modelsummary\n",
    "# model summary\n",
    "from modelsummary import summary\n",
    "\n",
    "# this is for filtering the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HHim9wS53ck"
   },
   "source": [
    "# 1.  Preprocessing\n",
    "\n",
    "数据预处理部分大概需要做：\n",
    "\n",
    "1. 读取数据\n",
    "2. 使用gensim.word2vec训练词向量，建立词向量映射\n",
    "3. sentenses to idx list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52152,
     "status": "ok",
     "timestamp": 1586679110386,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "dwiJgJSM5glP",
    "outputId": "8a15f7fb-af8d-4c0e-baf7-18f612e677c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n",
      "Archive:  data.zip\n",
      "  inflating: testing_data.txt        \n",
      "  inflating: training_label.txt      \n",
      "  inflating: training_nolabel.txt    \n"
     ]
    }
   ],
   "source": [
    "# gdrive\n",
    "from google.colab import drive  as gdrive\n",
    "gdrive.mount('/content/drive')\n",
    "!cp /content/drive/My\\ Drive/ml2020spring-hw4.zip ./data.zip\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nrLFNd8w8HhE"
   },
   "outputs": [],
   "source": [
    "# google\n",
    "# !gdown --id '1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8' --output data.zip\n",
    "# !unzip data.zip\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZ9Rz7_98Pyt"
   },
   "outputs": [],
   "source": [
    "# local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvlDqKERD8I0"
   },
   "source": [
    "## 1.1 文本预处理\n",
    "\n",
    "读取句子，对文本进行一些预处理。\n",
    "\n",
    "- **分词**，对于英文来说，分词只需要用空格，标点进行即可，给定数据中已经给标点前后增加空格。\n",
    "- **删除停用词** ，例如to，for等，这在文本处理中是可选项，这些词存在与否对于部分任务，例如文本分类，但在例如机器翻译中，就不进行。至于需不需要保留标点符号，我认为应该差不多。一方面，考虑标点符号和上下文关系不大，因而word2vec可能没有什么意义，但标点符号有利于句子的划分。所以这里不对标点符号进行处理。\n",
    "- **词型还原**，同样是可选项，英文中词有多种时态，但是word2vec中却会分为多个词，这显然是不太合理的，对于文本情感分析，直觉上，时态并不会给文本分类带来太多什么帮助。\n",
    "\n",
    "ref https://panchuang.net/2019/08/31/nlp-essentials-removing-stopwords-and-performing-text-normalization-using-nltk-and-spacy-in-python/\n",
    "\n",
    "更新：\n",
    "事实证明，删除停用词，词型还原都没有用，尤其是删除停用词会对结果产生负影响。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mfPihRmBE1dn"
   },
   "outputs": [],
   "source": [
    "def load_data(data_path, name):\n",
    "    y = None\n",
    "    # read\n",
    "    with open(os.path.join(data_path, name)) as f:\n",
    "        sentenses = f.readlines()\n",
    "        if name == 'testing_data.txt':\n",
    "            sentenses = sentenses[1:]\n",
    "            sentenses = [sen.split(',',1)[1] for sen in sentenses]\n",
    "        if name == 'training_label.txt':\n",
    "            y = [int(sen[0]) for sen in sentenses]\n",
    "            sentenses = [sen[10:] for sen in sentenses]\n",
    "            \n",
    "        sentences = [sen.strip('\\n').split() for sen in sentenses]\n",
    "    # deal with stopword and tense\n",
    "    \n",
    "    def lemmatize(word):\n",
    "        wnl = WordNetLemmatizer()\n",
    "        word = wnl.lemmatize(word, 'n')\n",
    "        word = wnl.lemmatize(word, 'v')\n",
    "        word = wnl.lemmatize(word, 'a')\n",
    "        return word\n",
    "\n",
    "    x = []\n",
    "    stopword = set(stopwords.words('english'))\n",
    "    # # without stopword and lemmatize\n",
    "    x = [sen.split() for sen in sentenses]\n",
    "    # lemmatize\n",
    "    # for sentence in sentences:\n",
    "    #     x.append([lemmatize(word) for word in sentence])    \n",
    "\n",
    "    # stopword  & lemmatize\n",
    "    # for sentence in sentences:\n",
    "    #     x.append([lemmatize(word) for word in sentence if word not in stopword])\n",
    "    \n",
    "    if y:\n",
    "        return x,y\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20073,
     "status": "ok",
     "timestamp": 1586583223411,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "DAlnCQeBB_6s",
    "outputId": "6681bd2c-86bc-472e-f35c-38cce8445847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====train data\n",
      "size: 200000\n",
      "[['are', 'wtf', '...', 'awww', 'thanks', '!'], ['leavingg', 'to', 'wait', 'for', 'kaysie', 'to', 'arrive', 'myspacin', 'itt', 'for', 'now', 'ilmmthek', '.!'], ['i', 'wish', 'i', 'could', 'go', 'and', 'see', 'duffy', 'when', 'she', 'comes', 'to', 'mamaia', 'romania', '.'], ['i', 'know', 'eep', '!', 'i', 'can', \"'\", 't', 'wait', 'for', 'one', 'more', 'day', '....'], ['so', 'scared', 'and', 'feeling', 'sick', '.', 'fuck', '!', 'hope', 'someone', 'at', 'hr', 'help', '...', 'wish', 'it', 'would', 'be', 'wendita', 'or', 'karen', '.'], ['my', 'b', 'day', 'was', 'thurs', '.', 'i', 'wanted', '2', 'do', '5', 'this', 'weekend', 'for', 'my', 'b', 'day', 'but', 'i', 'guess', 'close', 'enough', 'next', 'weekend', '.', 'going', 'alone'], ['e3', 'is', 'in', 'the', 'trending', 'topics', 'only', 'just', 'noticed', 'ive', 'been', 'tweeting', 'on', 'my', 'iphone', 'until', 'now'], ['where', 'did', 'you', 'get', 'him', 'from', 'i', 'know', 'someone', 'who', 'would', 'love', 'that', '!'], ['dam', 'just', 'got', 'buzzed', 'by', 'another', 'huge', 'fly', '!', 'this', 'time', 'it', 'landed', 'on', 'my', 'head', '...', 'not', 'impressed'], ['tomorrowwwwwwwww', '!!!', 'you', \"'\", 'll', 'love', 'tomorrow', \"'\", 's', 'news', '!']]\n",
      "[1, 1, 0, 1, 0, 0, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path = './'\n",
    "train_data,train_label = load_data(data_path, 'training_label.txt')\n",
    "print('====train data')\n",
    "print('size: {}'.format(len(train_data)))\n",
    "print(train_data[:10])\n",
    "print(train_label[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wheeJ9h7B_lP"
   },
   "outputs": [],
   "source": [
    "# test_data = load_data(data_path, 'testing_data.txt')\n",
    "# print('====test data')\n",
    "# print('size: {}'.format(len(test_data)))\n",
    "# print(test_data[:10])\n",
    "# train_nolabel = load_data(data_path, 'training_nolabel.txt')\n",
    "# print('====train data without label')\n",
    "# print('size: {}'.format(len(train_nolabel)))\n",
    "# print(train_nolabel[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "364X6vMCEni_"
   },
   "source": [
    "## 1.2 word2vec\n",
    "\n",
    "- 训练词向量，单层神经网络预测下一个词，取hidden layer的输出，以此向量化。把所有数据拿出来训练词向量。gensim.word2vec已经写好了框架，直接使用训练。\n",
    "- 建立word2idx，idx2vec的映射。\n",
    "- 然后将sentenses转为indies list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhtn4YXEGhfO"
   },
   "outputs": [],
   "source": [
    "emb_dim = 250\n",
    "# model = word2vec.Word2Vec(train_data+test_data+train_nolabel,\n",
    "#                           size = emb_dim,                                                                 # dimension\n",
    "#                           window = 5,                                                              # windlow of context\n",
    "#                           min_count = 5,                                                          # remove low frequency word\n",
    "#                           workers = 12,                                                            # precess tread \n",
    "#                           iter = 10,                                                                   # iteration\n",
    "#                           sg = 1                                                                       #  if 1, use skp-gram, 0 use cbow\n",
    "#                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQ6-abhkOzSh"
   },
   "outputs": [],
   "source": [
    "wv_name = 'w2v_all_without'\n",
    "# wv_name = 'w2v_all_lem'\n",
    "# wv_name = 'w2v_all'\n",
    "# model.save(os.path.join(data_path, wv_name))\n",
    "# !mkdir /content/drive/My\\ Drive/hw4/\n",
    "# !cp  w2v_all*  /content/drive/My\\ Drive/hw4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24629,
     "status": "ok",
     "timestamp": 1586583228405,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "9QHFsJ3AJD5_",
    "outputId": "6ca7e5d6-1da9-48e3-e221-b8a963893351"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# load word2vec model\n",
    "!cp /content/drive/My\\ Drive/hw4/w2v_all* ./\n",
    "w2v_path = os.path.join(data_path, wv_name)\n",
    "w2v_model = Word2Vec.load(w2v_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24607,
     "status": "ok",
     "timestamp": 1586583228407,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "AFbb0aGZJ6Cs",
    "outputId": "4949a742-72a2-4de9-cc51-828f855a7cb0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250,)\n"
     ]
    }
   ],
   "source": [
    "# map word to idx, idx to vector\n",
    "word2idx = {}\n",
    "idx2word = []\n",
    "embedding_matrix = []\n",
    "print(w2v_model['wtf'].shape)\n",
    "for word in w2v_model.wv.vocab:\n",
    "    idx2word.append(word)\n",
    "    word2idx[word] = len(idx2word)-1\n",
    "    embedding_matrix.append(w2v_model[word])\n",
    "\n",
    "# add <PAD> and <UNK> \n",
    "word  = '<PAD>'\n",
    "idx2word.append(word)\n",
    "word2idx[word] = len(idx2word)-1\n",
    "embedding_matrix.append(np.random.uniform(0,1,emb_dim))\n",
    "word  = '<UNK>'\n",
    "idx2word.append(word)\n",
    "word2idx[word] = len(idx2word)-1\n",
    "embedding_matrix.append(np.random.uniform(0,1,emb_dim))\n",
    "\n",
    "# embedding_matrix = np.concatenate(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBQGspvjco6T"
   },
   "outputs": [],
   "source": [
    "emb_matrix = np.zeros((len(embedding_matrix), emb_dim))\n",
    "for i in range(emb_matrix.shape[0]):\n",
    "    emb_matrix[i, :] = embedding_matrix[i][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4PHSbMStT-2W"
   },
   "source": [
    "sentense to ldx list 需要规整句子长度，查看长度分布，定义长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25932,
     "status": "ok",
     "timestamp": 1586583229843,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "PNxlTfj5UNl7",
    "outputId": "b8a87ff8-f120-46f8-eafc-2f16538fcb19"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAck0lEQVR4nO3de5ScdZ3n8fe3b+lcOjfSJG2STgKES2AlQJtBEUVABAZFVsaBdSGzcDbqDjtyxl2B4bjiYeas6ALuuohEYUQPg6DIwHEGlOEyLKMDk0gICeESAoGEJjeTzoV0d12++0c9FSqdqu6nqut5qp7qz+ucOl31q+ep37efqvr207/ndzF3R0REkq+p1gGIiEh1KKGLiDQIJXQRkQahhC4i0iCU0EVEGkRLnJXNmDHD58+fH2eVIiKJt3Llyu3u3jnSdrEm9Pnz57NixYo4qxQRSTwz2xhmu9BNLmbWbGbPm9mvgscLzOxZM1tvZveZWVulwYqIyOiV04b+FWBdweObgFvd/ShgJ3BlNQMTEZHyhEroZjYH+GPgR8FjA84EfhFscjfw2SgCFBGRcMKeoX8X+BqQDR4fBuxy93TweBMwu9iOZrbMzFaY2Ypt27aNKlgRESltxIRuZhcAW919ZSUVuPtyd+9x957OzhEv0oqISIXC9HI5DfiMmZ0PtAOTgf8NTDWzluAsfQ6wObowRURkJCOeobv7de4+x93nA5cAT7j7F4AngYuDzZYCD0UWpYiIjGg0I0WvAf7SzNaTa1O/szohiYg0jt6+/dz8m1d4Y/u+yOsqa2CRuz8FPBXc3wAsqX5IIiKN451d+/neE+vpmT+dBTMmRlqX5nIREYlQKpNbRKi1ySKvSwldRCRC6SChtzRHn26V0EVEIpTK5obvtDTrDF1EJNH6BzMAtDbpDF1EJNF6+/oBaG3RGbqISKK1tuTS7PSJ0U9Iq4QuIhKhdCbXht6mi6IiIsmWyuQviiqhi4gk2kAql9Bb1ctFRCTZHlu3BVAvFxGRxJs6oY1J41po0khREZFkS6WzHNfVEUtdSugiIhFKZ7O0xNDcAkroIiKRGsz4gb7oUVNCFxGJyNY9/bzw9i5aYmg/ByV0EZHIvLXjPQAWdU2OpT4ldBGRiAwGg4pOO2pGLPWNmNDNrN3MnjOzF8xsrZl9Myj/sZm9YWargtvi6MMVEUmO/OIWbTFMzAXhlqAbAM50971m1go8Y2aPBM/9d3f/RXThiYgkV34el9YYhv1DiITu7g7sDR62BjePMigRkaRzd/7nIy8D1Fe3RTNrNrNVwFbgMXd/Nnjqb8xstZndambjSuy7zMxWmNmKbdu2VSlsEZH61p/Ksn5r7lx43mETYqkzVEJ394y7LwbmAEvM7ATgOuBY4EPAdOCaEvsud/ced+/p7OysUtgiIvUtf0H06xcsYuK4MK3bo1fW/wHuvgt4EjjX3Xs9ZwD4W2BJFAGKiCTR++3n8VwQhXC9XDrNbGpwfzzwSeBlM+sKygz4LLAmykBFRJIk38MlrguiEK6XSxdwt5k1k/sDcL+7/8rMnjCzTsCAVcCXIoxTRCQx9g6k+cnv3gSIbZQohOvlsho4qUj5mZFEJCKScP/v1W18/6nXaWtuYsGMibHVG09LvYjIGNKfzgDw6NWnc0TnpNjq1dB/EZEqe3+EaLwpVgldRKTKUjGPEM1Tk4uISJW4Oys27mTtO7sBJXQRkcRav3Uvf/KD3wG55pYJbc2x1q+ELiJSJbv7UwDc8OlFfPL4WbS3xpvQ1YYuIlIl+YuhR8/qYPbU8bHXr4QuIlIltboYmqcmFxGRUepPZdjdn2LH3kFACV1EJJHcnY9/50m27B44UBb3xdA8JXQRkVHIZJ0tuwc4+7jDOeOYw5kyvpWFh8c3OrSQErqIyCiks7kLoSfPm8Z/PHVeTWPRRVERkVHIL2TRVqN280K1j0BEJMHSNZj3vBQ1uYiIlOlnz73Fd//pNQAyroQuIpJYz735B/b0p7jggx8AoLXF+MSxtV8zWQldRKRMqYxz+OR2brr4g7UO5SBh1hRtN7PnzOwFM1trZt8MyheY2bNmtt7M7jOztujDFRGpvXQmG+viz2GFafQZAM509xOBxcC5ZnYqcBNwq7sfBewErowuTBGR+pHKOC1NtW8zHyrMmqIO7A0etgY3B84E/kNQfjdwA3B79UMUEamddCbL1x9ay/a9748EXfX2TmZPm1DDqIoL1YZuZs3ASuAo4DbgdWCXu6eDTTYBs0vsuwxYBtDd3T3aeEVEYvXOrn7ufe4tPjClnSkTci3LnR3tfOr4mTWO7FChErq7Z4DFZjYVeBA4NmwF7r4cWA7Q09PjlQQpIlIr+YFD15x3LBcuLnreWjfKagRy913Ak8CHgalmlv+DMAfYXOXYRERqLp2tn5GgIwnTy6UzODPHzMYDnwTWkUvsFwebLQUeiipIEZFaSaVzDQstCUjoYZpcuoC7g3b0JuB+d/+Vmb0E/MzM/hp4HrgzwjhFRCL36pY9PLrm3YPKevv2A9RlN8WhwvRyWQ2cVKR8A7AkiqBERGrh9qde58HnD209bm9tYk4d9moZSiNFRUQC/akMRx0+iV9f/bGDyg1oamqAM3QRkbEilXHamptoTkDyLqb+W/lFRGKSqtMh/WHpDF1ExpyVG3eyuz91SPnWPQNMrNF6oNWghC4iY8r6rXv53O2/Lfn82cfV3wjQsJTQRWRMyZ+Zf/2CRZzcPfWQ54+s0QLP1aCELiJjSiqdG/l57KwOTuqeVuNoqksXRUVkTEln62fJuGrTGbqINKT3BtPsH8wcUr5j3yCQjJGf5VJCF5GGs2PvAB/51hMMBM0rxYxPcG+WUpTQRaThbN87yEA6y5/2zOX42ZMPeX5yeyvHzOyoQWTRUkIXkYaTCuYwP+u4wznn+Fk1jiY+jXdVQETGvPyiFK0tYyvFja3fVkTGhHQm15MlCYtSVJOaXEQkkR5d08uNv1pHbh37g+UvhrYkdJKtSimhi0girXhzJ1t293PRScXX+Zw4roV/N2dKzFHVlhK6iCRSKpNl4rgWvvMnJ9Y6lLoRZk3RuWb2pJm9ZGZrzewrQfkNZrbZzFYFt/OjD1dEJCeV9YYc7TkaYc7Q08BX3f33ZtYBrDSzx4LnbnX3/xVdeCIixaUTPnd5FMKsKdoL9Ab395jZOqB4o5WISJUMpDN8/e/X0Lf/0HnLAV7c1Kcz9CHKakM3s/nkFox+FjgNuMrMLgdWkDuL31lkn2XAMoDu7u5RhisiY8X6rXu5f8UmZk8dT0f7oalq8vhWPn50Zw0iq1+hE7qZTQIeAK52991mdjtwI+DBz5uBK4bu5+7LgeUAPT09h/YvEhEpIhX0Jb/xs8dz5rHJXXQiTqH+XzGzVnLJ/B53/yWAu29x94y7Z4EfAkuiC1NExpp0frSnmlVCC9PLxYA7gXXufktBeVfBZhcBa6ofnoiMVfnh+y1NSuhhhWlyOQ24DHjRzFYFZX8FXGpmi8k1ubwJfDGSCEWkYT3yYi8btu8r+twbQXlbi3qyhBWml8szQLEj+o/VD0dExgp357/e+/yBFYSKmdDWTNeU8TFGlWwaKSoiNZHJOums8xdnLeSqTxxVdJvmJqN5jM3HMhpK6CJSE/kz8/GtzbSNsWluo6KjKCI1cWDOco32rBqdoYtIZF7dsofevv6iz+3tTwPqllhNSugiEolUJsunv/fMsAs1A0yd0BpTRI1PCV1EIjGYzjKQzrL0w/P4zOLi0z+1NTex6AOHLuIslVFCF5FI5BdqnnfYRE6ZN63G0YwNarwSkUjk52LRRc/46AxdRCq2dyBNqkQb+Y59A4AuesZJCV1EKvLipj4uvO0ZhhnoCcD4tuZ4AhIldBGpzOZd+8k6fOnjRzJr8rii27S1NHP2cZr6Ni5K6CJSkfxFz8+dPJuFMztqHI2ALoqKSIVSmq+87uidEJGKpINeLC3qxVI31OQiIkUtf/p1fvK7jSWf3zuQG7rfpjP0uqGELiJFPbN+B+8NZjjjmNILMc+a3E5nR/ELohI/JXQRKSqVznJk50Ru+fziWociIYVZU3SumT1pZi+Z2Voz+0pQPt3MHjOz14KfGtsr0kDS2awueCZMmHcrDXzV3RcBpwJ/bmaLgGuBx919IfB48FhEGsRgxmlRQk+UMGuK9gK9wf09ZrYOmA1cCJwRbHY38BRwTSRRikhVPbRqM4+ueXfYbTZs28sfLZgeU0RSDWW1oZvZfOAk4FlgZpDsAd4Fig4HM7NlwDKA7u7uSuMUkSr66e82svad3cydXnoB5q4p7ZxxzOExRiWjFTqhm9kk4AHganffbfZ+31N3dzMrOqODuy8HlgP09PSMMOuDiMQhlcmyZMF07r5iSa1DkSoK1UBmZq3kkvk97v7LoHiLmXUFz3cBW6MJUUSqLZVxXfBsQGF6uRhwJ7DO3W8peOphYGlwfynwUPXDE5EopDJZzVPegMI0uZwGXAa8aGargrK/Ar4F3G9mVwIbgc9HE6KIhNX3Xoqfr3ybwczw63hu3zvAcV1a+q3RhOnl8gxQ6k/5WdUNR0RG45E1vfz1P6wLte2RnZMijkbippGiIg1kfyoDwL9edxZTJ7QOu217qxaeaDRK6CINJD8D4qT2FiXsMUiXuUUaSL7tvKVJFzzHIp2hiyRAb99+Xt+6b8TtNmzLbaMuiWOTErpIAnzxpytZvakv1LYd7S006wx9TFJCF0mAvv0pTl84g784a+GI286a3B5DRFKPlNBFEiCVzjJzcjsfmq/JsqQ0NbSJJMCghupLCDpDF6mhfQNp0tmR56xLZbK0aai+jEAJXaRGnn51G0v/9jk85Byk6lcuI1FCF6mRTTv34w5Xn72QjvbhR3U2GZx3QldMkUlSKaGL1EgqGAR02anzOGzSuBpHI41AV1lEaiSf0Ftb9DWU6tAnSaRGUsG8K61N+hpKdajJRaSKnn51G994eC2ZED1X+vanAGhR7xWpEiV0kSpauXEnb2zfx0UnzQ61/ZGdE9W/XKpGCV2kivJLu936p4trHYqMQWHWFL3LzLaa2ZqCshvMbLOZrQpu50cbpkgypLNOi9rEpUbCfPJ+DJxbpPxWd18c3P6xumGJJNNgWosvS+2EWVP0aTObH30oIvWnb3+Kv/mHl9g3mAm1/ZrNfWoTl5oZTRv6VWZ2ObAC+Kq77yy2kZktA5YBdHd3j6I6kfi98PYu7l+xidlTx9PeOnKibmkyzjl+VgyRiRyq0oR+O3Aj4MHPm4Erim3o7suB5QA9PT0hZ60QqQ/5wT/f/8LJnDh3ao2jERleRf8buvsWd8+4exb4IbCkumGJ1IcDg3/UjCIJUNGn1MwKZwm6CFhTaluRJDswPF8XOiUBRmxyMbN7gTOAGWa2CfgGcIaZLSbX5PIm8MUIYxSpmm17Bnho1eZQIzkB1ryzG9AZuiRDmF4ulxYpvjOCWEQi94uVm7jp0ZfL2qejvYXpk9oiikikejRSVMaU/lSu++Hab34KC9mK0trcpDN0SQQldBlTUpksLU3GxHH66Evj0WmHjCnprBZblsal0xRJLHdn9aY+9g2kQ+/z1o731GNFGpYSuiTW6k19XHjbv5S939zp4yOIRqT2lNAlsfILRNz42RM4+vBJofebO31CVCGJ1JQSuiRWftDPiXOm8ME5GpYvoqtDklj5hK75x0VydIYudWPfQJqsh5+/bU9/7mJoW4sucoqAErrUib9/fjNX37eqon3HtTRXORqRZFJCl7qwccd7AFx//nGhR3ACTJ/Yxpxp6rUiAkroUidSmSxNBv/5Y0fUOhSRxNLVJKkLqWxWIzhFRknfIKkLqbSG5IuMlppcpKoG01kuu/NZtu4ZKGu/7XsHNCRfZJSU0KWqduwb4Nk3/sCJc6cyr8wRmYu1ZqfIqCihS1Wl0rl+5JedOo+LT5lT42hExpYRGy3N7C4z22pmawrKppvZY2b2WvBzWrRhSlKkslqDU6RWwlyF+jFw7pCya4HH3X0h8HjwWKRgUWVd4BSJW5g1RZ82s/lDii8kt3A0wN3AU8A1VYxLamzH3gG+9cjL9KezZe2XnwFRCV0kfpW2oc90997g/rvAzFIbmtkyYBlAd3d3hdVJ3J574w/8fOUmZk8dz7iW8pLzoq7JHDurI6LIRKSUUV8UdXc3s5IzKrn7cmA5QE9PT/iZl6SmBoOmk7uvWMJRZcw1LiK1U+n/xVvMrAsg+Lm1eiFJPUhncn9729R0IpIYlX5bHwaWBveXAg9VJxypFwfmGldvFZHEGLHJxczuJXcBdIaZbQK+AXwLuN/MrgQ2Ap+PMkip3Lre3Tzz2vay91u5cSegi5siSRKml8ulJZ46q8qxSAS+/ejLPPnKtor2nT6xjY52jT0TSQp9WxtcfyrLyd1T+cmVf1T2vm3NTbSV2cNFRGpHCb3BpTJZ2lubmTROb7VIo9PpV4NLZTTPuMhYodO2hHhtyx527Bsse7++/Sk6O9ojiEhE6o0SegLs3DfIOd99Gq9wWNYp86ZXNyARqUtK6Amwpz+NO3z5jCM5feGMsvc/YfaUCKISkXqjhJ4A+WH4x3VN5iNHlp/QRWRs0NWyBDgwJW2TRm2KSGk6Q4/ZYDp7IEGHtac/DWjUpogMTwk9Rlv39PPxbz/F/lSmov3bW5urHJGINBIl9Bht3T3A/lSGi0+Zw9Ezy5uSdnxbCx9aoJX+RKQ0JfQY5S9u/vEHu/jEMYfXOBoRaTRqlI2R5hgXkSgps8TowBzj6q0iIhFQk0sFBtNZLr/rWbbuGShrv/2DuYuhrZrBUEQioIRegW17B/jXDX/gxLlTmTttfFn7drS3ctysyRFFJiJjmRJ6BVLpXNPJn31kHhedNKfG0YiI5IwqoZvZm8AeIAOk3b2nGkHVu3Q23xauphMRqR/VOEP/hLuXv2hlgg2mc71VWrWAsojUETW5APf/29v89vXwf5N27U8BGoovIvVltAndgd+YmQN3uPvyoRuY2TJgGUB3d/coq4vG959az7Y9A8zoGBd6n2NndXD0zI4IoxIRKc9oE/pH3X2zmR0OPGZmL7v704UbBEl+OUBPT0+FSzREK5Vxzj2hi5s/f2KtQxERqdio2gzcfXPwcyvwILCkGkHFLbfuptrDRSTZKk7oZjbRzDry94FzgDXVCixOWkhZRBrBaJpcZgIPmln+df7O3R+tSlRVkMk6D6zcxJ6B9IjbvjeYUUIXkcSrOKG7+wagbhud12zu42sPrA69/bzDJkQYjYhI9Bq222J+EYkfXd7DhxYMv+p9k+WG5IuIJFnDJvT8zIZTJrQyZbyStYg0voZtOM7PPa6pakVkrEj0GfpAOsOLm/rIFund/lLvbkCjOUVk7Eh0Qr/jnzdwy2OvDruNmltEZKxIdELf9V6K8a3N/Ghp8Ukep4xvZe509V4RkbEh0Qk9lcnS3trEaUfNqHUoIiI1l+gG5nRWIzxFRPISdYaezTqDQXdEgIGUErqISF6iEvrFP/gtv39r10FlR3ROrFE0IiL1JVEJfcP2fZwybxpnHzfzQNniuVNrGJGISP1IVEJPpbOcNHcqXz7jyFqHIiJSdxLVAJ3KOi1qMxcRKSox2dHdtRCFiMgwEtHk8r3HX+OhF97BHVqaEvM3SEQkVolI6J0d4zh65iSOndXBp06YOfIOIiJjUCIS+iVLurlkSXetwxARqWujar8ws3PN7BUzW29m11YrKBERKd9oFoluBm4DzgMWAZea2aJqBSYiIuUZzRn6EmC9u29w90HgZ8CF1QlLRETKNZqEPht4u+DxpqDsIGa2zMxWmNmKbdu2jaI6EREZTuR9AN19ubv3uHtPZ2dn1NWJiIxZo0nom4G5BY/nBGUiIlIDo0no/wYsNLMFZtYGXAI8XJ2wRESkXBX3Q3f3tJldBfwaaAbucve1VYtMRETKYu4eX2Vm24CNFe4+A9hexXCqRXGVR3GVR3GVp17jgtHFNs/dR7wIGWtCHw0zW+HuxVeDriHFVR7FVR7FVZ56jQviiU0zXYmINAgldBGRBpGkhL681gGUoLjKo7jKo7jKU69xQQyxJaYNXUREhpekM3QRERmGErqISKNw97q/AecCrwDrgWsjeP25wJPAS8Ba4CtB+Q3kpjNYFdzOL9jnuiCeV4BPjRQrsAB4Nii/D2gLGdubwItB/SuCsunAY8Brwc9pQbkB/yeoYzVwcsHrLA22fw1YWlB+SvD664N9LURMxxQck1XAbuDqWh0v4C5gK7CmoCzyY1SqjhHi+g7wclD3g8DUoHw+sL/g2P2g0vqH+x2HiSvy9w4YFzxeHzw/P0Rc9xXE9CawKs7jRencUPPPV9HvQrWTY7Vv5Eahvg4cAbQBLwCLqlxHV/7AAx3Aq+TmeL8B+G9Ftl8UxDEu+PC+HsRZMlbgfuCS4P4PgC+HjO1NYMaQsm/nv0DAtcBNwf3zgUeCD9WpwLMFH4wNwc9pwf38B/C5YFsL9j2vgvfnXWBerY4X8DHgZA5OBJEfo1J1jBDXOUBLcP+mgrjmF2435HXKqr/U7zhCXJG/d8B/IUi85KYKuW+kuIY8fzPwP+I8XpTODTX/fBX93ctNfnHfgA8Dvy54fB1wXcR1PgR8cpgP+UExkJv+4MOlYg3eqO28/0U+aLsRYnmTQxP6K0BXwQfuleD+HcClQ7cDLgXuKCi/IyjrAl4uKD9ou5DxnQP8S3C/ZseLIV/wOI5RqTqGi2vIcxcB9wy3XSX1l/odRzhekb93+X2D+y3BdjZcXAXlRm667oW1OF4Fz+VzQ118vobektCGHmre9Woxs/nASeT+JQS4ysxWm9ldZjZthJhKlR8G7HL39JDyMBz4jZmtNLNlQdlMd+8N7r8L5FfOLjeu2cH9oeXluAS4t+BxrY9XXhzHqFQdYV1B7owsb4GZPW9m/2xmpxfEW279lX5non7vDuwTPN8XbB/G6cAWd3+toCzW4zUkN9Tl5ysJCT02ZjYJeAC42t13A7cDRwKLgV5y//LF7aPufjK5pf7+3Mw+Vvik5/58ew3iIphl8zPAz4Oiejheh4jjGJVbh5ldD6SBe4KiXqDb3U8C/hL4OzObHFX9RdTle1fgUg4+cYj1eBXJDRW/ViXC1pGEhB7LvOtm1kruDbvH3X8J4O5b3D3j7lngh+SW3RsuplLlO4CpZtYypHxE7r45+LmV3EW0JcAWM+sK4u4idyGpkrg2B/eHlod1HvB7d98SxFjz41UgjmNUqo5hmdmfARcAXwi+qLj7gLvvCO6vJNc+fXSF9Zf9nYnpvTuwT/D8lGD7YQXb/ntyF0jz8cZ2vIrlhgpeK5bPVxISeuTzrpuZAXcC69z9loLyroLNLgLWBPcfBi4xs3FmtgBYSO7CRtFYgy/tk8DFwf5LybXFjRTXRDPryN8n1169Jqh/aZHXehi43HJOBfqCf9l+DZxjZtOCf6XPIdeu2QvsNrNTg2NweZi4Chx01lTr4zVEHMeoVB0lmdm5wNeAz7j7ewXlncHC65jZEeSO0YYK6y/1Ow4XVxzvXWG8FwNP5P+gjeBscu3MB5om4jpepXJDBa8Vy+crsguL1byRu3L8Krm/wtdH8PofJffvzGoKum0BPyXXnWh1cHC7Cva5PojnFQp6hpSKlVxvgOfIdU36OTAuRFxHkOs98AK5LlPXB+WHAY+T6870T8B0f//C0W1B3S8CPQWvdUVQ93rgPxWU95D78r4O/F9CdFsM9ptI7uxqSkFZTY4XuT8qvUCKXBvklXEco1J1jBDXenJtqQd1twM+F7zHq4DfA5+utP7hfsdh4or8vQPag8frg+ePGCmuoPzHwJeGbBvL8aJ0bqj556vYTUP/RUQaRBKaXEREJAQldBGRBqGELiLSIJTQRUQahBK6iEiDUEIXEWkQSugiIg3i/wMt6UfJbam6VQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def show_ditribution(data):\n",
    "    '''\n",
    "    show data ditribution\n",
    "    '''\n",
    "    n = len(data)\n",
    "    plt.plot(range(n),data)\n",
    "    plt.show()\n",
    "    plt.plot(range(n),np.sort(data))\n",
    "    plt.show()\n",
    "\n",
    "show_ditribution([len(sen) for sen in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sfbq0f1NTBx0"
   },
   "outputs": [],
   "source": [
    "len_sentence = 20\n",
    "def data_preprocess(train_data, train_label = None, word2idx = None, len_sentence = 20):\n",
    "    train_x = np.empty((len(train_data), len_sentence))\n",
    "    \n",
    "    for i,sentence in enumerate(train_data):\n",
    "            sen_len = len(sentence)\n",
    "            for j in range(len_sentence):\n",
    "                if j >= sen_len:\n",
    "                    train_x[i,j] = word2idx['<PAD>']\n",
    "                    continue\n",
    "                word = sentence[j]\n",
    "                if word in word2idx.keys():\n",
    "                    train_x[i,j] = word2idx[word]\n",
    "                else:\n",
    "                    train_x[i,j] = word2idx['<UNK>']\n",
    "    \n",
    "    # print(\"==train_x\")\n",
    "    # print(train_x[:10])\n",
    "    # print(\"size: {}\".format(train_x.shape))\n",
    "    # print(train_y[:10])\n",
    "\n",
    "    if train_label is not None:\n",
    "        train_y = np.array(train_label)\n",
    "        return train_x, train_y\n",
    "    else:\n",
    "        return train_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W40SQlqHKfhl"
   },
   "outputs": [],
   "source": [
    "train_x, train_y = data_preprocess(train_data, train_label, word2idx, 20)\n",
    "val_x = train_x[18000:]\n",
    "val_y = train_y[18000:]\n",
    "train_x = train_x[:18000]\n",
    "train_y = train_y[:18000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-e6BBMubvI8T"
   },
   "source": [
    "## 1.3 Extend Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ri0wJ4d4vf8_"
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, x, y=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __getitem__(self, i):\n",
    "        # if self.y is not None:\n",
    "        #     return torch.LongTensor(self.x[i, :]), torch.LongTensor(self.y[i])\n",
    "        # return torch.LongTensor(self.x[i, :])\n",
    "        if self.y is not None:\n",
    "            return self.x[i, :], self.y[i]\n",
    "        return self.x[i, :]\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-DuYIv0GNhXt"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_set = Dataset(train_x, train_y)\n",
    "val_set = Dataset(val_x, val_y)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_set,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = True,\n",
    "                                            num_workers = 8)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_set,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            num_workers = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FYwpIEVNcxZQ"
   },
   "source": [
    "# 2. Model\n",
    "\n",
    "构建模型，主要是embedding层注意一下即可。\n",
    "\n",
    "另外，pytorch里的LSTM参数很多，尤其是输入，输出，理解起来比较难。\n",
    "\n",
    "````python\n",
    "Inputs: input, (h_0, c_0)\n",
    "    input of shape (seq_len, batch, input_size)\n",
    "    h_0 of shape (num_layers * num_directions, batch, hidden_size)\n",
    "    c_0 of shape (num_layers * num_directions, batch, hidden_size)\n",
    "Outputs: output, (h_n, c_n)\n",
    "    output of shape (seq_len, batch, num_directions * hidden_size)\n",
    "    h_n of shape (num_layers * num_directions, batch, hidden_size)\n",
    "    c_n of shape (num_layers * num_directions, batch, hidden_size)\n",
    "````\n",
    "结合这个图，按照batch_first理解，即input为（batch, 序列长, 输入维度），忽略batch，都视为1个sample。所以输入是一个矩形（序列长度，输入维度），输出是一个矩形（序列长度，输出维度）。如果是双向，则输出维度x2.\n",
    "\n",
    "然后是c，h，c和h的维度是一致的，为单个LSTM单元的输出，pytorch可以指定初始c，h，得到所有输出c，h。同样，batch_first，忽略batch，则每个LSTM单元的输出为（输出维度），n层LSTM输出维度为（层数，输出维度），所以同样都是矩形。\n",
    "\n",
    "输入如果不指定c_0，h_0则默认为0.\n",
    "![](https://pic4.zhimg.com/80/v2-ebf8cd2faa564d9d80a958dcf25e6b3b_720w.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D4itqdAlWvZ1"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3w5rpA-a3yi"
   },
   "outputs": [],
   "source": [
    "\n",
    "class lstm_model(nn.Module):\n",
    "    def __init__(self, embedding_matrix, trainable_embedding = False):\n",
    "        super(lstm_model,self).__init__()\n",
    "        self.dim_emb =  embedding_matrix.shape[1]\n",
    "        self.embedding = nn.Embedding(embedding_matrix.shape[0], self.dim_emb )\n",
    "        self.embedding.weight = nn.Parameter(torch.Tensor(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = trainable_embedding\n",
    "        self.lstm_layers = nn.LSTM(self.dim_emb, 256, num_layers = 1, batch_first = True)\n",
    "        self.dnn = nn.Sequential(\n",
    "                        nn.Dropout(0.5),\n",
    "                        nn.Linear(256, 1),\n",
    "                        nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out,_ = self.lstm_layers(out)\n",
    "        out = out[:, -1, :] # 输出默认为sequence，使用最后一个time step的输出, (none，output_dim)，不需要再flatten\n",
    "        return self.dnn(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MVtly6EbK9VH"
   },
   "source": [
    "# 3. Training\n",
    "\n",
    "````python\n",
    "for epoch\n",
    "    for x_batch in train_data\n",
    "        # forward\n",
    "        pred_batch <- model.predict(x_batch)\n",
    "        loss_batch <- loss_function(pred_batch, y_batch)\n",
    "\n",
    "        # backward\n",
    "        optim: gradient descent\n",
    "\n",
    "        # record log\n",
    "        acc += calculate batch acc (pred_batch, y_batch)\n",
    "        loss += loss_batch\n",
    "\n",
    "    calculate val loss & acc\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WkdDYT15aP_6"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 155895,
     "status": "ok",
     "timestamp": 1586583359937,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "zjqrDICKYa7k",
    "outputId": "29033f13-94c4-42da-89e4-c1bc10874fd6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([112])) that is different to the input size (torch.Size([112, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/020] 6.11 sec(s) Train Acc: 0.601222 Loss: 0.005044 | Val Acc: 0.751066 loss: 0.003998\n",
      "Saving with acc : 0.751065934065934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type lstm_model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[002/020] 6.69 sec(s) Train Acc: 0.756222 Loss: 0.003872 | Val Acc: 0.774412 loss: 0.003691\n",
      "Saving with acc : 0.7744120879120879\n",
      "[003/020] 6.07 sec(s) Train Acc: 0.780500 Loss: 0.003606 | Val Acc: 0.776824 loss: 0.003635\n",
      "Saving with acc : 0.7768241758241758\n",
      "[004/020] 5.91 sec(s) Train Acc: 0.789444 Loss: 0.003485 | Val Acc: 0.779835 loss: 0.003686\n",
      "Saving with acc : 0.7798351648351648\n",
      "[005/020] 6.46 sec(s) Train Acc: 0.798278 Loss: 0.003357 | Val Acc: 0.779379 loss: 0.003596\n",
      "[006/020] 6.39 sec(s) Train Acc: 0.803667 Loss: 0.003301 | Val Acc: 0.783863 loss: 0.003565\n",
      "Saving with acc : 0.7838626373626374\n",
      "[007/020] 5.88 sec(s) Train Acc: 0.812278 Loss: 0.003157 | Val Acc: 0.781104 loss: 0.003619\n",
      "[008/020] 5.90 sec(s) Train Acc: 0.825278 Loss: 0.003007 | Val Acc: 0.778819 loss: 0.003711\n",
      "[009/020] 6.54 sec(s) Train Acc: 0.834556 Loss: 0.002854 | Val Acc: 0.775390 loss: 0.003870\n",
      "[010/020] 6.54 sec(s) Train Acc: 0.849000 Loss: 0.002622 | Val Acc: 0.770659 loss: 0.003931\n",
      "[011/020] 6.64 sec(s) Train Acc: 0.864778 Loss: 0.002427 | Val Acc: 0.773154 loss: 0.004453\n",
      "[012/020] 6.59 sec(s) Train Acc: 0.882833 Loss: 0.002102 | Val Acc: 0.761819 loss: 0.004590\n",
      "[013/020] 6.74 sec(s) Train Acc: 0.905889 Loss: 0.001802 | Val Acc: 0.761473 loss: 0.005177\n",
      "[014/020] 6.49 sec(s) Train Acc: 0.923722 Loss: 0.001499 | Val Acc: 0.758621 loss: 0.006045\n",
      "[015/020] 6.62 sec(s) Train Acc: 0.940056 Loss: 0.001211 | Val Acc: 0.748522 loss: 0.006630\n",
      "[016/020] 6.29 sec(s) Train Acc: 0.956667 Loss: 0.000914 | Val Acc: 0.749176 loss: 0.007390\n",
      "[017/020] 6.62 sec(s) Train Acc: 0.965333 Loss: 0.000753 | Val Acc: 0.754577 loss: 0.007793\n",
      "[018/020] 6.63 sec(s) Train Acc: 0.972611 Loss: 0.000606 | Val Acc: 0.745165 loss: 0.008343\n",
      "[019/020] 6.62 sec(s) Train Acc: 0.981944 Loss: 0.000424 | Val Acc: 0.751604 loss: 0.009453\n",
      "[020/020] 6.61 sec(s) Train Acc: 0.984556 Loss: 0.000367 | Val Acc: 0.745418 loss: 0.010113\n"
     ]
    }
   ],
   "source": [
    "def cal_acc(pred_b, y_b):\n",
    "    output_b = pred_b.cpu().detach().numpy().reshape(-1)\n",
    "    output_b[output_b>=0.5] = 1\n",
    "    output_b[output_b < 0.5] = 0\n",
    "    output_b = output_b.astype(np.int32)\n",
    "    return np.sum(output_b == y_b.numpy())\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epoch = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "path = './'\n",
    "\n",
    "model = lstm_model(emb_matrix).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) \n",
    "loss_func = nn.BCELoss()\n",
    "logs = {'time':[], 'train_loss':[], 'train_acc':[], 'val_acc':[], 'val_loss':[]}\n",
    "best_epochs = []\n",
    "best_acc = 0\n",
    "for epoch in range(num_epoch):\n",
    "    time_epoch_start = time.time()\n",
    "    train_acc = 0.0 \n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for i, (x_b, y_b) in enumerate(train_loader):\n",
    "        # forward\n",
    "        optimizer.zero_grad() \n",
    "        pred_b = model(x_b.to(device, dtype = torch.long)) # torch的特点，需要将数据设置到gpu上\n",
    "        loss_b = loss_func(pred_b, y_b.to(device, dtype = torch.float))\n",
    "        # backward\n",
    "        loss_b.backward()\n",
    "        optimizer.step()\n",
    "        # log\n",
    "        train_acc += cal_acc(pred_b, y_b)\n",
    "        train_loss += loss_b.item()\n",
    "   # calculate val acc and loss\n",
    "    model.eval()\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i,(x_b, y_b) in enumerate(val_loader):\n",
    "            pred_b = model(x_b.to(device, dtype = torch.long))\n",
    "            loss_b = loss_func(pred_b, y_b.to(device, dtype = torch.float))\n",
    "            val_acc += cal_acc(pred_b, y_b)\n",
    "            val_loss += loss_b.item()\n",
    "    # print log\n",
    "    logs['time'].append(time.time()-time_epoch_start)\n",
    "    logs['train_acc'].append(train_acc/train_set.__len__())\n",
    "    logs['train_loss'].append(train_loss/train_set.__len__())\n",
    "    logs['val_acc'].append(val_acc/val_set.__len__())\n",
    "    logs['val_loss'].append(val_loss/val_set.__len__()) \n",
    "    \n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, num_epoch,  logs['time'][-1], \\\n",
    "            logs['train_acc'][-1] , logs['train_loss'][-1], logs['val_acc'][-1], logs['val_loss'][-1])) \n",
    "    # save best model\n",
    "    # notes： early stop\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_epochs.append(epoch)\n",
    "        torch.save(model, os.path.join(path, 'best.model'))\n",
    "        print(\"Saving with acc : {}\".format(logs['val_acc'][-1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 155879,
     "status": "ok",
     "timestamp": 1586583359939,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "3NGFTdbZpqEK",
    "outputId": "42b0c637-ea6e-4d71-855f-56389107c669"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are', 'wtf', '...', 'awww', 'thanks', '!']"
      ]
     },
     "execution_count": 109,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m3-PNC5-Wrzc"
   },
   "source": [
    "无处理数据\n",
    "````\n",
    "[001/020] 6.22 sec(s) Train Acc: 0.577111 Loss: 0.005176 | Val Acc: 0.733516 loss: 0.004225\n",
    "Saving with acc : 0.0042251411809043566\n",
    "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type lstm_model. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "[002/020] 6.19 sec(s) Train Acc: 0.753889 Loss: 0.003981 | Val Acc: 0.763495 loss: 0.003845\n",
    "Saving with acc : 0.0038445410171707908\n",
    "[003/020] 6.18 sec(s) Train Acc: 0.774389 Loss: 0.003722 | Val Acc: 0.771890 loss: 0.003742\n",
    "Saving with acc : 0.0037423860882664774\n",
    "[004/020] 5.99 sec(s) Train Acc: 0.782167 Loss: 0.003582 | Val Acc: 0.747500 loss: 0.003972\n",
    "[005/020] 6.39 sec(s) Train Acc: 0.792333 Loss: 0.003466 | Val Acc: 0.779396 loss: 0.003628\n",
    "Saving with acc : 0.0036276583070610907\n",
    "[006/020] 6.38 sec(s) Train Acc: 0.802000 Loss: 0.003323 | Val Acc: 0.780951 loss: 0.003628\n",
    "Saving with acc : 0.003627572279546287\n",
    "[007/020] 6.30 sec(s) Train Acc: 0.811000 Loss: 0.003176 | Val Acc: 0.779802 loss: 0.003663\n",
    "[008/020] 6.25 sec(s) Train Acc: 0.820722 Loss: 0.003040 | Val Acc: 0.774808 loss: 0.003727\n",
    "[009/020] 6.09 sec(s) Train Acc: 0.830444 Loss: 0.002881 | Val Acc: 0.769989 loss: 0.003840\n",
    "[010/020] 6.28 sec(s) Train Acc: 0.849500 Loss: 0.002680 | Val Acc: 0.771621 loss: 0.003905\n",
    "[011/020] 6.60 sec(s) Train Acc: 0.864167 Loss: 0.002425 | Val Acc: 0.762593 loss: 0.004190\n",
    "[012/020] 5.92 sec(s) Train Acc: 0.880000 Loss: 0.002164 | Val Acc: 0.763824 loss: 0.004621\n",
    "[013/020] 6.35 sec(s) Train Acc: 0.901944 Loss: 0.001845 | Val Acc: 0.746473 loss: 0.005116\n",
    "[014/020] 5.91 sec(s) Train Acc: 0.919111 Loss: 0.001557 | Val Acc: 0.748291 loss: 0.005405\n",
    "[015/020] 6.34 sec(s) Train Acc: 0.934056 Loss: 0.001287 | Val Acc: 0.751000 loss: 0.006179\n",
    "[016/020] 6.33 sec(s) Train Acc: 0.952944 Loss: 0.001006 | Val Acc: 0.750341 loss: 0.006643\n",
    "[017/020] 6.10 sec(s) Train Acc: 0.961333 Loss: 0.000828 | Val Acc: 0.743890 loss: 0.007536\n",
    "[018/020] 6.02 sec(s) Train Acc: 0.967444 Loss: 0.000696 | Val Acc: 0.740676 loss: 0.008257\n",
    "[019/020] 5.97 sec(s) Train Acc: 0.978833 Loss: 0.000493 | Val Acc: 0.739808 loss: 0.008277\n",
    "[020/020] 6.48 sec(s) Train Acc: 0.985944 Loss: 0.000345 | Val Acc: 0.741335 loss: 0.009711\n",
    "````\n",
    "stopword and lemmatize\n",
    "\n",
    "````\n",
    "[002/020] 6.56 sec(s) Train Acc: 0.524556 Loss: 0.005401 | Val Acc: 0.500648 loss: 0.005383\n",
    "Saving with acc : 0.5006483516483516\n",
    "[003/020] 6.61 sec(s) Train Acc: 0.519611 Loss: 0.005416 | Val Acc: 0.514505 loss: 0.005399\n",
    "Saving with acc : 0.5145054945054945\n",
    "[004/020] 6.78 sec(s) Train Acc: 0.510944 Loss: 0.005426 | Val Acc: 0.513533 loss: 0.005419\n",
    "[005/020] 6.62 sec(s) Train Acc: 0.521333 Loss: 0.005421 | Val Acc: 0.514093 loss: 0.005400\n",
    "[006/020] 6.45 sec(s) Train Acc: 0.621667 Loss: 0.005070 | Val Acc: 0.708319 loss: 0.004523\n",
    "Saving with acc : 0.7083186813186814\n",
    "[007/020] 6.61 sec(s) Train Acc: 0.732889 Loss: 0.004248 | Val Acc: 0.747973 loss: 0.004009\n",
    "Saving with acc : 0.7479725274725275\n",
    "[008/020] 6.65 sec(s) Train Acc: 0.755611 Loss: 0.003969 | Val Acc: 0.756632 loss: 0.003949\n",
    "Saving with acc : 0.7566318681318681\n",
    "[009/020] 6.09 sec(s) Train Acc: 0.765944 Loss: 0.003807 | Val Acc: 0.761170 loss: 0.003840\n",
    "Saving with acc : 0.7611703296703297\n",
    "[010/020] 6.04 sec(s) Train Acc: 0.776500 Loss: 0.003702 | Val Acc: 0.757418 loss: 0.003863\n",
    "[011/020] 5.88 sec(s) Train Acc: 0.782000 Loss: 0.003625 | Val Acc: 0.762725 loss: 0.003822\n",
    "Saving with acc : 0.7627252747252747\n",
    "[012/020] 6.76 sec(s) Train Acc: 0.786111 Loss: 0.003564 | Val Acc: 0.759060 loss: 0.003862\n",
    "[013/020] 5.98 sec(s) Train Acc: 0.795500 Loss: 0.003448 | Val Acc: 0.753214 loss: 0.003969\n",
    "[014/020] 6.51 sec(s) Train Acc: 0.806056 Loss: 0.003327 | Val Acc: 0.759709 loss: 0.004027\n",
    "[015/020] 5.98 sec(s) Train Acc: 0.817278 Loss: 0.003174 | Val Acc: 0.754099 loss: 0.004141\n",
    "[016/020] 6.78 sec(s) Train Acc: 0.827889 Loss: 0.003053 | Val Acc: 0.740198 loss: 0.004076\n",
    "[017/020] 6.24 sec(s) Train Acc: 0.840889 Loss: 0.002830 | Val Acc: 0.738308 loss: 0.004236\n",
    "[018/020] 6.69 sec(s) Train Acc: 0.859667 Loss: 0.002600 | Val Acc: 0.740049 loss: 0.004642\n",
    "[019/020] 6.24 sec(s) Train Acc: 0.878056 Loss: 0.002363 | Val Acc: 0.728973 loss: 0.005339\n",
    "[020/020] 6.60 sec(s) Train Acc: 0.894278 Loss: 0.002109 | Val Acc: 0.736577 loss: 0.005142\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kj2gnFtTlvPY"
   },
   "source": [
    "only lematize\n",
    "\n",
    "````\n",
    "[002/020] 6.20 sec(s) Train Acc: 0.744833 Loss: 0.004122 | Val Acc: 0.762291 loss: 0.003953\n",
    "Saving with acc : 0.7622912087912088\n",
    "[003/020] 6.31 sec(s) Train Acc: 0.767500 Loss: 0.003845 | Val Acc: 0.769253 loss: 0.003753\n",
    "Saving with acc : 0.7692527472527473\n",
    "[004/020] 5.88 sec(s) Train Acc: 0.780278 Loss: 0.003646 | Val Acc: 0.773319 loss: 0.003708\n",
    "Saving with acc : 0.7733186813186813\n",
    "[005/020] 5.96 sec(s) Train Acc: 0.786500 Loss: 0.003551 | Val Acc: 0.773879 loss: 0.003673\n",
    "Saving with acc : 0.7738791208791209\n",
    "[006/020] 6.68 sec(s) Train Acc: 0.795833 Loss: 0.003400 | Val Acc: 0.771484 loss: 0.003692\n",
    "[007/020] 6.02 sec(s) Train Acc: 0.804778 Loss: 0.003307 | Val Acc: 0.775912 loss: 0.003695\n",
    "Saving with acc : 0.7759120879120879\n",
    "[008/020] 5.94 sec(s) Train Acc: 0.812889 Loss: 0.003190 | Val Acc: 0.778346 loss: 0.003694\n",
    "Saving with acc : 0.7783461538461538\n",
    "[009/020] 6.66 sec(s) Train Acc: 0.826389 Loss: 0.003014 | Val Acc: 0.771022 loss: 0.004065\n",
    "[010/020] 6.08 sec(s) Train Acc: 0.838667 Loss: 0.002835 | Val Acc: 0.771835 loss: 0.004120\n",
    "[011/020] 5.93 sec(s) Train Acc: 0.852278 Loss: 0.002624 | Val Acc: 0.756467 loss: 0.004178\n",
    "[012/020] 6.31 sec(s) Train Acc: 0.874167 Loss: 0.002345 | Val Acc: 0.759115 loss: 0.004357\n",
    "[013/020] 5.94 sec(s) Train Acc: 0.889667 Loss: 0.002035 | Val Acc: 0.760368 loss: 0.004817\n",
    "[014/020] 5.84 sec(s) Train Acc: 0.910444 Loss: 0.001735 | Val Acc: 0.756863 loss: 0.005646\n",
    "[015/020] 6.32 sec(s) Train Acc: 0.925667 Loss: 0.001460 | Val Acc: 0.746308 loss: 0.005780\n",
    "[016/020] 6.52 sec(s) Train Acc: 0.944111 Loss: 0.001153 | Val Acc: 0.741687 loss: 0.006552\n",
    "[017/020] 6.51 sec(s) Train Acc: 0.953556 Loss: 0.001004 | Val Acc: 0.745615 loss: 0.007161\n",
    "[018/020] 5.85 sec(s) Train Acc: 0.963333 Loss: 0.000794 | Val Acc: 0.744615 loss: 0.007531\n",
    "[019/020] 6.67 sec(s) Train Acc: 0.973389 Loss: 0.000588 | Val Acc: 0.744181 loss: 0.008812\n",
    "[020/020] 6.36 sec(s) Train Acc: 0.978778 Loss: 0.000494 | Val Acc: 0.737962 loss: 0.009193\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 159559,
     "status": "ok",
     "timestamp": 1586583363656,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "m7GpqQuHWDbx",
    "outputId": "15dd047c-e69b-4ffb-b2a3-10948c7380f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: modelsummary in /usr/local/lib/python3.6/dist-packages (1.1.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from modelsummary) (1.18.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from modelsummary) (1.4.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from modelsummary) (4.38.0)\n",
      "-----------------------------------------------------------------------\n",
      "             Layer (type)                Input Shape         Param #\n",
      "=======================================================================\n",
      "              Embedding-1                   [-1, 20]      13,944,500\n",
      "                    GRU-2              [-1, 20, 250]               0\n",
      "                Dropout-3                  [-1, 256]               0\n",
      "                 Linear-4                  [-1, 256]             257\n",
      "                Sigmoid-5                    [-1, 1]               0\n",
      "=======================================================================\n",
      "Total params: 13,944,757\n",
      "Trainable params: 257\n",
      "Non-trainable params: 13,944,500\n",
      "-----------------------------------------------------------------------\n",
      "13944500\n",
      "192000\n",
      "196608\n",
      "768\n",
      "768\n",
      "256\n",
      "1\n",
      "14334901\n"
     ]
    }
   ],
   "source": [
    "# notes：打印参数，summary\n",
    "!pip install modelsummary\n",
    "from modelsummary import summary\n",
    "summary(lstm_model(emb_matrix), torch.zeros((20,20), dtype = torch.long))\n",
    "sum_ = 0\n",
    "\n",
    "for p,n in zip(model.parameters(), model.state_dict()):\n",
    "    print(n, '||', p.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 159550,
     "status": "ok",
     "timestamp": 1586583363657,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "-9ICFK0YqsdW",
    "outputId": "5a2acab1-8f16-4d6a-d37a-5141cbd42a48"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 平滑函数\n",
    "def smooth_curve(points, factor = 0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            prev = smoothed_points[-1]\n",
    "            smoothed_points.append(prev * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "smooth_factor = 0\n",
    "plt.plot(logs['train_loss'], label = 'train loss')\n",
    "plt.plot(smooth_curve(logs['val_loss'], smooth_factor), label = 'val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(logs['train_acc'], label = 'train acc')\n",
    "plt.plot(smooth_curve(logs['val_acc'], smooth_factor), label = 'val acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1y_tUPC3fUIK"
   },
   "source": [
    "# 4. Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 168376,
     "status": "ok",
     "timestamp": 1586583372495,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "k8drrpM1gnxw",
    "outputId": "3225b63a-dcec-4474-fc63-26e2243a6dae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save csv ...\n",
      "Finish Predicting\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_data = load_data(data_path, 'testing_data.txt')\n",
    "test_x = data_preprocess(test_data, word2idx = word2idx, len_sentence = 20)\n",
    "test_set = Dataset(test_x)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_set,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            num_workers = 8)\n",
    "\n",
    "model = torch.load(os.path.join(data_path, 'best.model'))\n",
    "\n",
    "model.eval()\n",
    "ret_output = []\n",
    "with torch.no_grad():\n",
    "    for i, inputs in enumerate(test_loader):\n",
    "        inputs = inputs.to(device, dtype=torch.long)\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        outputs[outputs>=0.5] = 1 # 大於等於 0.5 為負面\n",
    "        outputs[outputs<0.5] = 0 # 小於 0.5 為正面\n",
    "        ret_output += outputs.int().tolist() \n",
    "tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"label\":ret_output})\n",
    "print(\"save csv ...\")\n",
    "tmp.to_csv(os.path.join(data_path, 'predict.csv'), index=False)\n",
    "print(\"Finish Predicting\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9plmvlsU0si3"
   },
   "source": [
    "# 5. Sum-up\n",
    "\n",
    "不知道为什么， 参考示例代码写的，它的performer就是比我好不少。\n",
    "\n",
    "示例代码给的经验是，应该慢慢写慢慢封装代码，要不然后期修改起来相当麻烦。可以参考以下聚合代码。使用方法或者类进行封装。\n",
    "\n",
    "- 加载数据模块\n",
    "- 预处理模块，可以使用类封装，因为涉及到的东西比较多，类的层次更好\n",
    "- 模型，类封装\n",
    "- 训练，torch的train过程比较琐碎\n",
    "- 验证\n",
    "\n",
    "下面对上面的部分代码进行封装。方便后续实验。\n",
    "\n",
    "**更新**\n",
    "\n",
    "后面整理了以下，突然发现我的模型一点问题也没有……准确率和示例代码是差不多的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bpx8bD_cd9C2"
   },
   "outputs": [],
   "source": [
    "def load_data(data_path, name):\n",
    "    y = None\n",
    "    # read\n",
    "    with open(os.path.join(data_path, name)) as f:\n",
    "        sentenses = f.readlines()\n",
    "        if name == 'testing_data.txt':\n",
    "            sentenses = sentenses[1:]\n",
    "            sentenses = [sen.split(',',1)[1] for sen in sentenses]\n",
    "        if name == 'training_label.txt':\n",
    "            y = [int(sen[0]) for sen in sentenses]\n",
    "            sentenses = [sen[10:] for sen in sentenses]\n",
    "            \n",
    "        sentences = [sen.strip('\\n').split() for sen in sentenses]\n",
    "    # deal with stopword and tense\n",
    "    \n",
    "    # def lemmatize(word):\n",
    "    #     wnl = WordNetLemmatizer()\n",
    "    #     word = wnl.lemmatize(word, 'n')\n",
    "    #     word = wnl.lemmatize(word, 'v')\n",
    "    #     word = wnl.lemmatize(word, 'a')\n",
    "    #     return word\n",
    "\n",
    "    x = []\n",
    "    stopword = set(stopwords.words('english'))\n",
    "    # # without stopword and lemmatize\n",
    "    x = [sen.split() for sen in sentenses]\n",
    "    # lemmatize\n",
    "    # for sentence in sentences:\n",
    "    #     x.append([lemmatize(word) for word in sentence])    \n",
    "\n",
    "    # stopword  & lemmatize\n",
    "    # for sentence in sentences:\n",
    "    #     x.append([lemmatize(word) for word in sentence if word not in stopword])\n",
    "    \n",
    "    if y:\n",
    "        return x,y\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "class PreprocesserBOW():\n",
    "    '''\n",
    "    BOW preprocesser\n",
    "    '''\n",
    "    def __init__(self, data, max_len = 5000):\n",
    "        # count word frequency statistics\n",
    "        self.max_len = max_len\n",
    "        self.vocab_len = 0\n",
    "        word_freq = {}\n",
    "        self.word_idx = {}\n",
    "\n",
    "        for sentence in data:\n",
    "            for word in sentence:\n",
    "                if word in word_freq.keys():\n",
    "                    word_freq[word]+=1\n",
    "                else:\n",
    "                    word_freq[word] = 1\n",
    "        # select first max_len word sorted by frequency\n",
    "        word_freq = sorted(word_freq.items(), key = lambda item: item[1], reverse = True)\n",
    "        self.vocab_len = len(word_freq) if self.max_len > len(word_freq) else self.max_len\n",
    "        for word,_ in word_freq:\n",
    "            if len(self.word_idx) >= self.vocab_len:\n",
    "                break\n",
    "            self.word_idx[word] = len(self.word_idx)\n",
    "\n",
    "    def _to_bow(self, data, label = None):\n",
    "        # sentense to bow idx\n",
    "        x_bow = np.zeros((len(data), self.vocab_len), np.int32)\n",
    "        for i,sentence in enumerate(data):\n",
    "            for j,word in enumerate(sentence):\n",
    "                if word in self.word_idx.keys():\n",
    "                    x_bow[i, self.word_idx[word]] += 1      \n",
    "          \n",
    "        if label is not None:\n",
    "            y = np.array(label).reshape(-1).astype(np.int8)\n",
    "            return x_bow,y\n",
    "        return x_bow\n",
    "\n",
    "from torch.utils import data\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, x, y=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __getitem__(self, i):\n",
    "        # if self.y is not None:\n",
    "        #     return torch.LongTensor(self.x[i, :]), torch.LongTensor(self.y[i])\n",
    "        # return torch.LongTensor(self.x[i, :])\n",
    "        if self.y is not None:\n",
    "            return self.x[i, :], self.y[i]\n",
    "        return self.x[i, :]\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "from torch import nn\n",
    "class bow_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(bow_model,self).__init__()\n",
    "        self.dnn = nn.Sequential( nn.Linear(3000, 512),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Dropout(0.5),\n",
    "                                         nn.Linear(512, 128),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Dropout(0.5),\n",
    "                                         nn.Linear(128, 1),\n",
    "                                         nn.Sigmoid() )\n",
    "    def forward(self, x):\n",
    "        out = self.dnn(x)\n",
    "        return out\n",
    "\n",
    "import time\n",
    "\n",
    "def training(model, train_loader, val_loader, device, num_epoch = 5, data_path = './',learning_rate = 0.001):\n",
    "    def cal_acc(pred_b, y_b):\n",
    "        output_b = pred_b.cpu().detach().numpy().reshape(-1)\n",
    "        output_b[output_b>=0.5] = 1\n",
    "        output_b[output_b < 0.5] = 0\n",
    "        output_b = output_b.astype(np.int32)\n",
    "        return np.sum(output_b == y_b.numpy())\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) \n",
    "    loss_func = nn.BCELoss()\n",
    "    logs = {'time':[], 'train_loss':[], 'train_acc':[], 'val_acc':[], 'val_loss':[]}\n",
    "    best_epochs = []\n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epoch):\n",
    "        time_epoch_start = time.time()\n",
    "        train_acc = 0.0 \n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        for i, (x_b, y_b) in enumerate(train_loader):\n",
    "            # forward\n",
    "            optimizer.zero_grad() \n",
    "            pred_b = model(x_b.to(device, dtype = torch.float)) # torch的特点，需要将数据设置到gpu上\n",
    "            loss_b = loss_func(pred_b, y_b.to(device, dtype = torch.float))\n",
    "            # backward\n",
    "            loss_b.backward()\n",
    "            optimizer.step()\n",
    "            # log\n",
    "            train_acc += cal_acc(pred_b, y_b)\n",
    "            train_loss += loss_b.item()\n",
    "    # calculate val acc and loss\n",
    "        model.eval()\n",
    "        val_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i,(x_b, y_b) in enumerate(val_loader):\n",
    "                pred_b = model(x_b.to(device, dtype = torch.float))\n",
    "                loss_b = loss_func(pred_b, y_b.to(device, dtype = torch.float))\n",
    "                val_acc += cal_acc(pred_b, y_b)\n",
    "                val_loss += loss_b.item()\n",
    "        # print log\n",
    "        logs['time'].append(time.time()-time_epoch_start)\n",
    "        logs['train_acc'].append(train_acc/train_set.__len__())\n",
    "        logs['train_loss'].append(train_loss/train_set.__len__())\n",
    "        logs['val_acc'].append(val_acc/val_set.__len__())\n",
    "        logs['val_loss'].append(val_loss/val_set.__len__()) \n",
    "        # notes：打印logs\n",
    "        # print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "                (epoch + 1, num_epoch,  logs['time'][-1], \\\n",
    "                logs['train_acc'][-1] , logs['train_loss'][-1], logs['val_acc'][-1], logs['val_loss'][-1])) \n",
    "        # save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_epochs.append(epoch)\n",
    "            torch.save(model, os.path.join(data_path, 'best_bow.model'))\n",
    "            print(\"Saving with acc : {}\".format(logs['val_acc'][-1]))\n",
    "    return logs\n",
    "\n",
    "\n",
    "def show_logs(logs, smooth_factor = 0):\n",
    "    # 平滑函数\n",
    "    def smooth_curve(points, factor = 0.9):\n",
    "        smoothed_points = []\n",
    "        for point in points:\n",
    "            if smoothed_points:\n",
    "                prev = smoothed_points[-1]\n",
    "                smoothed_points.append(prev * factor + point * (1 - factor))\n",
    "            else:\n",
    "                smoothed_points.append(point)\n",
    "        return smoothed_points\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    smooth_factor = 0\n",
    "    plt.plot(logs['train_loss'], label = 'train loss')\n",
    "    plt.plot(smooth_curve(logs['val_loss'], smooth_factor), label = 'val loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.plot(logs['train_acc'], label = 'train acc')\n",
    "    plt.plot(smooth_curve(logs['val_acc'], smooth_factor), label = 'val acc')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hWgNvAcPNVPa"
   },
   "source": [
    "# 6. 实验\n",
    "1. (1%) 請說明你實作的 RNN 的模型架構、word embedding 方法、訓練過程 (learning curve) 和準確率為何？ (盡量是過 public strong baseline 的 model)\n",
    "2. (2%) 請比較 BOW + DNN 與 RNN 兩種不同 model 對於 \"today is a good day, but it is hot\" 與 \"today is hot, but it is a good day\" 這兩句的分數 (過 softmax 後的數值)，並討論造成差異的原因。 \n",
    "3. (1%) 請敘述你如何 improve performance（preprocess、embedding、架構等等），並解釋為何這些做法可以使模型進步，並列出準確率與 improve 前的差異。（semi-supervised 的部分請在下題回答）\n",
    "4. (2%) 請描述你的semi-supervised方法是如何標記label，並比較有無semi-supervised training對準確率的影響並試著探討原因（因為 semi-supervise learning 在 labeled training data 數量較少時，比較能夠發揮作用，所以在實作本題時，建議把有 label 的training data從 20 萬筆減少到 2 萬筆以下，在這樣的實驗設定下，比較容易觀察到semi-supervise learning所帶來的幫助）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OzuIWax-Nu5K"
   },
   "source": [
    "\n",
    "## 6.1 RNN模型架构和实验结果\n",
    "\n",
    "\n",
    "使用的是示例代码中的实现，kaggle上最佳结果是81.2%，没有过strong basline。\n",
    "模型结构如下， 可训练参数为100w左右。参数来源如下\n",
    "\n",
    "- Embedding，50000 word x 250 dim = 100w\n",
    "- LSTM， 参数量和 输入维度 x 输出维度 成正相关\n",
    "- 全连接，参数量为 输入维度 x 输出维度\n",
    "\n",
    "可训练参数大部分来自与LSTM，大概每层25w左右。\n",
    "\n",
    "````\n",
    "LSTM_Net(\n",
    "  (embedding): Embedding(55779, 250)\n",
    "  (lstm): LSTM(250, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
    "  (classifier): Sequential(\n",
    "    (0): Dropout(p=0.5, inplace=False)\n",
    "    (1): Linear(in_features=256, out_features=512, bias=True)\n",
    "    (2): ReLU()\n",
    "    (3): Linear(in_features=512, out_features=1, bias=True)\n",
    "    (4): Sigmoid()\n",
    "  )\n",
    ")\n",
    "parameter total:15123375, trainable:1178625\n",
    "````\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d7K8WpZNS_r3"
   },
   "source": [
    "## 6.2 BOW + DNN model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvGLPAUAc7Zr"
   },
   "source": [
    "\n",
    "### 6.2.1 bag of word\n",
    "bag of word，思想很简单，和one-hot编码类似，但它每个位置上不再是1，而是单词在句子中出现的次数。\n",
    "\n",
    "步骤如下：\n",
    "\n",
    "- 获取单词集合，如果词量太大，可能需要统计词频，截取字典\n",
    "- 转换trian，test，val set to bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_TaFQ2adnSD"
   },
   "outputs": [],
   "source": [
    "data_path = './'\n",
    "batch_size = 128\n",
    "# load data\n",
    "train_data, train_label = load_data(data_path, 'training_label.txt')\n",
    "test_data = load_data(data_path, 'testing_data.txt')\n",
    "bow_preprocesser = PreprocesserBOW(train_data+test_data, 3000)\n",
    "# data to bow data\n",
    "train_x,train_y = bow_preprocesser._to_bow(train_data, train_label)\n",
    "test_x = bow_preprocesser._to_bow(test_data)\n",
    "# split data\n",
    "train_x, train_y, val_x, val_y = train_x[:180000],train_y[:180000],train_x[180000:],train_y[180000:]\n",
    "# torch data \n",
    "batch_size = 128\n",
    "train_set = Dataset(train_x,train_y)\n",
    "val_set = Dataset(val_x, val_y)\n",
    "test_set = Dataset(test_x)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_set,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = True,\n",
    "                                            num_workers = 8)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_set,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            num_workers = 8)\n",
    "# training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = bow_model().to(device)\n",
    "summary(bow_model(), torch.zeros((20, 3000)))\n",
    "learning_rate = 0.001\n",
    "num_epoch = 10\n",
    "logs = training(model, train_loader, val_loader, device, num_epoch = num_epoch, data_path= data_path, learning_rate = learning_rate)\n",
    "# show logs\n",
    "show_logs(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 134328,
     "status": "ok",
     "timestamp": 1586602826540,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "iT8j6VDKzaEo",
    "outputId": "37db42bf-4a05-42c6-93a1-867d35fe7a7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "             Layer (type)                Input Shape         Param #\n",
      "=======================================================================\n",
      "                 Linear-1                 [-1, 3000]       1,536,512\n",
      "                   ReLU-2                  [-1, 512]               0\n",
      "                Dropout-3                  [-1, 512]               0\n",
      "                 Linear-4                  [-1, 512]          65,664\n",
      "                   ReLU-5                  [-1, 128]               0\n",
      "                Dropout-6                  [-1, 128]               0\n",
      "                 Linear-7                  [-1, 128]             129\n",
      "                Sigmoid-8                    [-1, 1]               0\n",
      "=======================================================================\n",
      "Total params: 1,602,305\n",
      "Trainable params: 1,602,305\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/010] 13.38 sec(s) Train Acc: 0.771139 Loss: 0.003768 | Val Acc: 0.783900 loss: 0.003587\n",
      "Saving with acc : 0.7839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type bow_model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[002/010] 13.32 sec(s) Train Acc: 0.802272 Loss: 0.003365 | Val Acc: 0.792050 loss: 0.003517\n",
      "Saving with acc : 0.79205\n",
      "[003/010] 13.35 sec(s) Train Acc: 0.824561 Loss: 0.003041 | Val Acc: 0.794800 loss: 0.003529\n",
      "Saving with acc : 0.7948\n",
      "[004/010] 13.30 sec(s) Train Acc: 0.851667 Loss: 0.002645 | Val Acc: 0.789800 loss: 0.003715\n",
      "[005/010] 13.31 sec(s) Train Acc: 0.875617 Loss: 0.002267 | Val Acc: 0.788550 loss: 0.003938\n",
      "[006/010] 13.31 sec(s) Train Acc: 0.893006 Loss: 0.001966 | Val Acc: 0.783750 loss: 0.004252\n",
      "[007/010] 13.34 sec(s) Train Acc: 0.907600 Loss: 0.001722 | Val Acc: 0.784250 loss: 0.004537\n",
      "[008/010] 13.65 sec(s) Train Acc: 0.917967 Loss: 0.001549 | Val Acc: 0.783150 loss: 0.004938\n",
      "[009/010] 13.35 sec(s) Train Acc: 0.925828 Loss: 0.001412 | Val Acc: 0.782650 loss: 0.005094\n",
      "[010/010] 13.37 sec(s) Train Acc: 0.932344 Loss: 0.001302 | Val Acc: 0.784700 loss: 0.005251\n"
     ]
    }
   ],
   "source": [
    "# data_path\n",
    "# train_loader\n",
    "# val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1285,
     "status": "ok",
     "timestamp": 1586602928147,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "F5HYY_OGzJUb",
    "outputId": "d36b3f0c-ef3a-4c1c-c876-fd0100973ecf"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_logs(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KSbE4mYI6351"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "# !cp ./best_bow.model ./drive/My\\ Drive/hw4/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vXx44zlwVKth"
   },
   "source": [
    "测试[\"today is a good day, but it is hot\",\n",
    " \"today is hot, but it is a good day\" ]的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fTQ6nh8AVhBS"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/My\\ Drive/hw4/best_bow.model ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RLcMsiY2VOb6"
   },
   "outputs": [],
   "source": [
    "bow_model = torch.load('./best_bow.model')\n",
    "# load data\n",
    "train_data, train_label = load_data(data_path, 'training_label.txt')\n",
    "test_data = load_data(data_path, 'testing_data.txt')\n",
    "bow_preprocesser = PreprocesserBOW(train_data+test_data, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NU5xE9JbV2yE"
   },
   "outputs": [],
   "source": [
    "target_data = [\"today is a good day, but it is hot\",\n",
    " \"today is hot, but it is a good day\" ]\n",
    "target_data = [t.split() for t in target_data]\n",
    "test_x_bow = bow_preprocesser._to_bow(target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "50RK2QBbWv-h"
   },
   "source": [
    "bow model 处理[\"today is a good day, but it is hot\", \"today is hot, but it is a good day\" ]结果如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1454,
     "status": "ok",
     "timestamp": 1586610484377,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "ac7M7aOGWN29",
    "outputId": "b744de0d-12c5-46da-9b6c-5e65edb06374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5937],\n",
      "        [0.5491]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "bow_model.eval()\n",
    "with torch.no_grad():\n",
    "    input_ = torch.tensor(test_x_bow).to(device, dtype=torch.float)\n",
    "    output_ = bow_model(input_)\n",
    "    print(output_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GRs-I5_77T3b"
   },
   "source": [
    "### 6.2.2  rnn model\n",
    "\n",
    "下面使用示例代码中的模型，整理早先的代码，自己实现的performce差一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNIvBInMBD7T"
   },
   "outputs": [],
   "source": [
    "# model.py\n",
    "# 這個 block 是要拿來訓練的模型\n",
    "import torch\n",
    "from torch import nn\n",
    "class LSTM_Net(nn.Module):\n",
    "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
    "        super(LSTM_Net, self).__init__()\n",
    "        # 製作 embedding layer\n",
    "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
    "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
    "        # 是否將 embedding fix 住，如果 fix_embedding 為 False，在訓練過程中，embedding 也會跟著被訓練\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "        self.embedding_dim = embedding.size(1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        #                               input size             output size                 lstm layers num              \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, dropout = 0.5, batch_first=True)\n",
    "        self.classifier = nn.Sequential( nn.Dropout(dropout),\n",
    "                                         nn.Linear(hidden_dim, hidden_dim*2),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Linear(hidden_dim*2, 1),\n",
    "                                         nn.Sigmoid() )\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "        x, _ = self.lstm(inputs, None)\n",
    "        # x 的 dimension (batch, seq_len, hidden_size)\n",
    "        # 取用 LSTM 最後一層的 hidden state\n",
    "        x = x[:, -1, :] # lstm 返回的shape为 none，layer，output\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "class PreprocessserW2V():\n",
    "    def __init__(self, w2v_path=None, emb_dim = 250):\n",
    "        if w2v_path is None:\n",
    "            print(\"Please call train_embedding\")\n",
    "            return\n",
    "        w2v_model = Word2Vec.load(w2v_path)\n",
    "        # map word to idx and idx to vector\n",
    "        self.word2idx = {}\n",
    "        idx2word = []\n",
    "        embedding_matrix = []\n",
    "\n",
    "        for word in w2v_model.wv.vocab:\n",
    "            idx2word.append(word)\n",
    "            self.word2idx[word] = len(idx2word)-1\n",
    "            embedding_matrix.append(w2v_model[word])\n",
    "\n",
    "        # add <PAD> and <UNK> \n",
    "        word  = '<PAD>'\n",
    "        idx2word.append(word)\n",
    "        self.word2idx[word] = len(idx2word)-1\n",
    "        embedding_matrix.append(np.random.uniform(0,1,emb_dim))\n",
    "        word  = '<UNK>'\n",
    "        idx2word.append(word)\n",
    "        self.word2idx[word] = len(idx2word)-1\n",
    "        embedding_matrix.append(np.random.uniform(0,1,emb_dim))\n",
    "        self. emb_matrix = np.zeros((len(embedding_matrix), emb_dim))\n",
    "        for i in range(self.emb_matrix.shape[0]):\n",
    "            self.emb_matrix[i, :] = embedding_matrix[i][:]\n",
    "    \n",
    "    def _get_emb_matrix(self):\n",
    "        return self.emb_matrix\n",
    "\n",
    "    def _to_w2v(self, data, label = None, len_sentence = 20):\n",
    "        x = np.empty((len(data), len_sentence), dtype = np.int32)\n",
    "        \n",
    "        for i,sentence in enumerate(data):\n",
    "                sen_len = len(sentence)\n",
    "                for j in range(len_sentence):\n",
    "                    if j >= sen_len:\n",
    "                        x[i,j] = self.word2idx['<PAD>']\n",
    "                        continue\n",
    "                    word = sentence[j]\n",
    "                    if word in self.word2idx.keys():\n",
    "                        x[i,j] = self.word2idx[word]\n",
    "                    else:\n",
    "                        x[i,j] = self.word2idx['<UNK>']\n",
    "        if label is not None:\n",
    "            y = np.array(label).reshape(-1)\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "    def _train_embedding(self, data, dimension = 250, window = 5, min_count = 5, workers = 12, iter =10, sg = 10):\n",
    "        self.w2v_model = word2vec.Word2Vec(data,\n",
    "                          size = dimension,                                                                 # dimension\n",
    "                          window = window,                                                              # windlow of context\n",
    "                          min_count = min_count,                                                          # remove low frequency word\n",
    "                          workers = workers,                                                            # precess tread \n",
    "                          iter = iter,                                                                   # iteration\n",
    "                          sg = sg                                                                       #  if 1, use skp-gram, 0 use cbow\n",
    "        )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KV-o7SLmRNaK"
   },
   "outputs": [],
   "source": [
    "w2v_preprocesser._to_w2v(test_data)\n",
    "!cp ./drive/My\\ Drive/hw4/w2v_all* ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FbvLcjhcADx6"
   },
   "outputs": [],
   "source": [
    "# load test\n",
    "test_data = load_data(data_path, 'testing_data.txt')\n",
    "# create preprocessor\n",
    "w2v_path = os.path.join(data_path, 'w2v_all_without')\n",
    "w2v_preprocesser = PreprocessserW2V(w2v_path)\n",
    "# preprocess test data\n",
    "test_x = w2v_preprocesser._to_w2v(test_data)\n",
    "# load model\n",
    "data_path = './'\n",
    "rnn_model = torch.load(os.path.join(data_path, 'best_rnn.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WqARx3zNkiUN"
   },
   "outputs": [],
   "source": [
    "target_data = [\"today is a good day, but it is hot\",\n",
    " \"today is hot, but it is a good day\" ]\n",
    "target_data = [t.split() for t in target_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8iY5ilMeTEOG"
   },
   "outputs": [],
   "source": [
    "test_x_rnn = w2v_preprocesser._to_w2v(target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZquqD0MSUyo7"
   },
   "source": [
    "w2v模型对[\"today is a good day, but it is hot\",\n",
    " \"today is hot, but it is a good day\" ]的预测如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2303,
     "status": "ok",
     "timestamp": 1586609866865,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "pBq3X35RTeMp",
    "outputId": "8f7bcf85-2715-4063-eb63-236bc42f377c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6308],\n",
      "        [0.9297]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "rnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    input_ = torch.tensor(test_x_rnn).to(device, dtype=torch.long)\n",
    "    output_ = rnn_model(input_)\n",
    "    print(output_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nInfKc0eW8Wr"
   },
   "source": [
    "### 6.2.3  结论\n",
    "\n",
    "rnn + w2v模型表现如下：\n",
    "\n",
    "> acc 80.937\n",
    ">\n",
    "> [\"today is a good day, but it is hot\", \"today is hot, but it is a good day\" ]\n",
    ">   \n",
    "> 0.6308概率为good， 0.9297概率为good\n",
    "\n",
    "dnn + bow模型表现如下：\n",
    "\n",
    "> acc 79.4800\n",
    ">\n",
    "> [\"today is a good day, but it is hot\", \"today is hot, but it is a good day\" ]\n",
    ">   \n",
    "> 0.5937概率为good，0.5491概率为good\n",
    "\n",
    "从上面结果可以看出：\n",
    "- 两个模型表现接近。\n",
    "- 对于目标句子的识别，rnn+w2v显然正确的多。\n",
    "\n",
    "我自己实现的模型还不如bow。这个有些意外，因为bow模型不考虑上下文，向量化方式比较简单。应该就是因为，对于情感分析，lstm其实并发挥不了太多的作用——想目标句子这种上下文自带否定的句子很少，通常来说我们看某个句子好坏（表达的感受）只需要从关键词“开心“，“wtf”，“sad”就可以大概率判断出来，这也是bow模型效果不错的原因。\n",
    "\n",
    "再看对于目标句子的判别，bow模型显然搞错了，它以为第二句表达很好的感受，lstm模型就很好的handle这种情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ca1JbXtZ7gk"
   },
   "source": [
    "## 6.3 实验分析与总结\n",
    "\n",
    "这次调参上没有什么好说的，没跑过助教的示例代码，明明是同样原理的实现，不知道那部分出问题了，很郁闷。\n",
    "\n",
    "心得如下：\n",
    "\n",
    "- 边写边封装\n",
    "- lstm的调参，唯一有点用的，就是stack lstm，dropout影响很小\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4B5mcyYVuJ0"
   },
   "source": [
    "## 6.4 Semi-supervised learning\n",
    "\n",
    "这里首先实现最简单的self-learning的模型，为了更好的看到实验结果，按照提示，选择20000笔数据作为训练数据，10000笔作为val set，剩下的作为test set。大致流程：\n",
    "- 获取新数据集，label 和 no label混合\n",
    "- 训练，找到最佳模型，预测no label数据\n",
    "- 回到第一步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-GuHUWgYZSZ"
   },
   "outputs": [],
   "source": [
    "class PreprocessserW2V():\n",
    "    def __init__(self, w2v_path=None, emb_dim = 250):\n",
    "        if w2v_path is None:\n",
    "            print(\"Please call train_embedding\")\n",
    "            return\n",
    "        w2v_model = Word2Vec.load(w2v_path)\n",
    "        # map word to idx and idx to vector\n",
    "        self.word2idx = {}\n",
    "        idx2word = []\n",
    "        embedding_matrix = []\n",
    "\n",
    "        for word in w2v_model.wv.vocab:\n",
    "            idx2word.append(word)\n",
    "            self.word2idx[word] = len(idx2word)-1\n",
    "            embedding_matrix.append(w2v_model[word])\n",
    "\n",
    "        # add <PAD> and <UNK> \n",
    "        word  = '<PAD>'\n",
    "        idx2word.append(word)\n",
    "        self.word2idx[word] = len(idx2word)-1\n",
    "        embedding_matrix.append(np.random.uniform(0,1,emb_dim))\n",
    "        word  = '<UNK>'\n",
    "        idx2word.append(word)\n",
    "        self.word2idx[word] = len(idx2word)-1\n",
    "        embedding_matrix.append(np.random.uniform(0,1,emb_dim))\n",
    "        self. emb_matrix = np.zeros((len(embedding_matrix), emb_dim))\n",
    "        for i in range(self.emb_matrix.shape[0]):\n",
    "            self.emb_matrix[i, :] = embedding_matrix[i][:]\n",
    "    \n",
    "    def _get_emb_matrix(self):\n",
    "        return self.emb_matrix\n",
    "\n",
    "    def _to_w2v(self, data, label = None, len_sentence = 20):\n",
    "        x = np.empty((len(data), len_sentence), dtype = np.int32)\n",
    "        \n",
    "        for i,sentence in enumerate(data):\n",
    "                sen_len = len(sentence)\n",
    "                for j in range(len_sentence):\n",
    "                    if j >= sen_len:\n",
    "                        x[i,j] = self.word2idx['<PAD>']\n",
    "                        continue\n",
    "                    word = sentence[j]\n",
    "                    if word in self.word2idx.keys():\n",
    "                        x[i,j] = self.word2idx[word]\n",
    "                    else:\n",
    "                        x[i,j] = self.word2idx['<UNK>']\n",
    "        if label is not None:\n",
    "            y = np.array(label).reshape(-1)\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "    def _train_embedding(self, data, dimension = 250, window = 5, min_count = 5, workers = 12, iter =10, sg = 10):\n",
    "        self.w2v_model = word2vec.Word2Vec(data,\n",
    "                          size = dimension,                                                                 # dimension\n",
    "                          window = window,                                                              # windlow of context\n",
    "                          min_count = min_count,                                                          # remove low frequency word\n",
    "                          workers = workers,                                                            # precess tread \n",
    "                          iter = iter,                                                                   # iteration\n",
    "                          sg = sg                                                                       #  if 1, use skp-gram, 0 use cbow\n",
    "        )\n",
    "    \n",
    "def load_data(data_path, name):\n",
    "    y = None\n",
    "    # read\n",
    "    with open(os.path.join(data_path, name)) as f:\n",
    "        sentenses = f.readlines()\n",
    "        if name == 'testing_data.txt':\n",
    "            sentenses = sentenses[1:]\n",
    "            sentenses = [sen.split(',',1)[1] for sen in sentenses]\n",
    "        if name == 'training_label.txt':\n",
    "            y = [int(sen[0]) for sen in sentenses]\n",
    "            sentenses = [sen[10:] for sen in sentenses]\n",
    "            \n",
    "        sentences = [sen.strip('\\n').split() for sen in sentenses]\n",
    "    # deal with stopword and tense\n",
    "    \n",
    "    # def lemmatize(word):\n",
    "    #     wnl = WordNetLemmatizer()\n",
    "    #     word = wnl.lemmatize(word, 'n')\n",
    "    #     word = wnl.lemmatize(word, 'v')\n",
    "    #     word = wnl.lemmatize(word, 'a')\n",
    "    #     return word\n",
    "\n",
    "    x = []\n",
    "    stopword = set(stopwords.words('english'))\n",
    "    # # without stopword and lemmatize\n",
    "    x = [sen.split() for sen in sentenses]\n",
    "    # lemmatize\n",
    "    # for sentence in sentences:\n",
    "    #     x.append([lemmatize(word) for word in sentence])    \n",
    "\n",
    "    # stopword  & lemmatize\n",
    "    # for sentence in sentences:\n",
    "    #     x.append([lemmatize(word) for word in sentence if word not in stopword])\n",
    "    \n",
    "    if y:\n",
    "        return x,y\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def split_data(data, label, split_radios):\n",
    "    n = len(data)\n",
    "    idx = np.arange(n)\n",
    "    np.random.shuffle(idx)\n",
    "    if len(split_radios) == 2:\n",
    "        b1 = int(split_radios[0]/np.sum(split_radios) * n) \n",
    "        return data[idx[:b1]], label[idx[:b1]], data[idx[b1:]], label[idx[b1:]]\n",
    "    elif len(split_radios)==3:\n",
    "        b1 = int(split_radios[0]/np.sum(split_radios) * n) \n",
    "        b2 = int((split_radios[0]+ split_radios[1])/np.sum(split_radios) * n) \n",
    "        print(b1, b2)\n",
    "        return data[idx[:b1]], label[idx[:b1]], data[idx[b1: b2]], label[idx[b1: b2]], data[idx[b2:]], label[idx[b2:]]\n",
    "\n",
    "class lstm_model(nn.Module):\n",
    "    def __init__(self, embedding_matrix, trainable_embedding = False):\n",
    "        super(lstm_model,self).__init__()\n",
    "        self.dim_emb =  embedding_matrix.shape[1]\n",
    "        self.embedding = nn.Embedding(embedding_matrix.shape[0], self.dim_emb )\n",
    "        self.embedding.weight = nn.Parameter(torch.Tensor(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = trainable_embedding\n",
    "        self.lstm_layers = nn.LSTM(self.dim_emb, 256, num_layers = 2, dropout = 0.5, batch_first = True)\n",
    "        self.dnn = nn.Sequential(\n",
    "                        nn.Dropout(0.5),\n",
    "                        nn.Linear(256, 1),\n",
    "                        nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out,_ = self.lstm_layers(out)\n",
    "        out = out[:, -1, :] # 输出默认为sequence，使用最后一个time step的输出, (none，output_dim)，不需要再flatten\n",
    "        return self.dnn(out)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __getitem__(self, i):\n",
    "        # if self.y is not None:\n",
    "        #     return torch.LongTensor(self.x[i, :]), torch.LongTensor(self.y[i])\n",
    "        # return torch.LongTensor(self.x[i, :])\n",
    "        if self.y is not None:\n",
    "            return self.x[i, :], self.y[i]\n",
    "        return self.x[i, :]\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "def training(model, train_loader, val_loader, device, num_epoch = 5, data_path = './',learning_rate = 0.001):\n",
    "    def cal_acc(pred_b, y_b):\n",
    "        output_b = pred_b.cpu().detach().numpy().reshape(-1)\n",
    "        output_b[output_b>=0.5] = 1\n",
    "        output_b[output_b < 0.5] = 0\n",
    "        output_b = output_b.astype(np.int32)\n",
    "        return np.sum(output_b == y_b.numpy())\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) \n",
    "    loss_func = nn.BCELoss()\n",
    "    logs = {'time':[], 'train_loss':[], 'train_acc':[], 'val_acc':[], 'val_loss':[]}\n",
    "    best_epochs = []\n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epoch):\n",
    "        time_epoch_start = time.time()\n",
    "        train_acc = 0.0 \n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        for i, (x_b, y_b) in enumerate(train_loader):\n",
    "            # forward\n",
    "            optimizer.zero_grad() \n",
    "            pred_b = model(x_b.to(device, dtype = torch.long)) # torch的特点，需要将数据设置到gpu上\n",
    "            loss_b = loss_func(pred_b, y_b.to(device, dtype = torch.float))\n",
    "            # backward\n",
    "            loss_b.backward()\n",
    "            optimizer.step()\n",
    "            # log\n",
    "            train_acc += cal_acc(pred_b, y_b)\n",
    "            train_loss += loss_b.item()\n",
    "    # calculate val acc and loss\n",
    "        model.eval()\n",
    "        val_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i,(x_b, y_b) in enumerate(val_loader):\n",
    "                pred_b = model(x_b.to(device, dtype = torch.long))\n",
    "                loss_b = loss_func(pred_b, y_b.to(device, dtype = torch.float))\n",
    "                val_acc += cal_acc(pred_b, y_b)\n",
    "                val_loss += loss_b.item()\n",
    "        # print log\n",
    "        logs['time'].append(time.time()-time_epoch_start)\n",
    "        logs['train_acc'].append(train_acc/train_set.__len__())\n",
    "        logs['train_loss'].append(train_loss/train_set.__len__())\n",
    "        logs['val_acc'].append(val_acc/val_set.__len__())\n",
    "        logs['val_loss'].append(val_loss/val_set.__len__()) \n",
    "        # notes：打印logs\n",
    "        # print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "                (epoch + 1, num_epoch,  logs['time'][-1], \\\n",
    "                logs['train_acc'][-1] , logs['train_loss'][-1]*1000, logs['val_acc'][-1], logs['val_loss'][-1]*1000)) \n",
    "        # save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_epochs.append(epoch)\n",
    "            torch.save(model, os.path.join(data_path, 'best_semi.model'))\n",
    "            print(\"Saving with acc : {}\".format(logs['val_acc'][-1]))\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hDuWqO7ZXXbp"
   },
   "source": [
    "### 6.4.1  Self-learning\n",
    "\n",
    "这个模型就是实现上述步骤的方法，简单而且直观。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8276,
     "status": "ok",
     "timestamp": 1586686279526,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "A3yAtsRTYFSM",
    "outputId": "bc55f5f4-d03c-4f63-8ee5-6dde6f177127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 30000\n"
     ]
    }
   ],
   "source": [
    "!cp ./drive/My\\ Drive/hw4/w2v_all* ./\n",
    "data_path = './'\n",
    "# create preprocessor\n",
    "w2v_path = os.path.join(data_path, 'w2v_all_without')\n",
    "w2v_preprocesser = PreprocessserW2V(w2v_path)\n",
    "# load data\n",
    "train_data, train_label = load_data(data_path, 'training_label.txt')\n",
    "train_x, train_y = w2v_preprocesser._to_w2v(train_data, train_label)\n",
    "# split data\n",
    "train_x, train_y, val_x, val_y, test_x, test_y = split_data(train_x, train_y, [2, 1, 17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 768,
     "status": "ok",
     "timestamp": 1586684071677,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "cXUDtWPYvWq-",
    "outputId": "38fc0700-c575-4ff6-a7b2-744d5bd0d01c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2197, 20)"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2208,
     "status": "ok",
     "timestamp": 1586684083254,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "WXy8LBOevZgv",
    "outputId": "533ca69d-cfd2-4f74-f0ef-835440debaa0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10989, 20)"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1586684324341,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "H7fOWqTQwUl1",
    "outputId": "5e4b16d4-4fa0-4ab6-ff8c-0e3f40c418ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(186814, 20)"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ckci3f8wmn0n"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 0.001\n",
    "num_epoch = 10\n",
    "batch_size = 128\n",
    "low = 0.05\n",
    "high = 0.95\n",
    "log_acc = []\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_set,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            num_workers = 8)\n",
    "train_test_x, train_test_y = train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    train_set,val_set, test_set = Dataset(train_test_x, train_test_y),Dataset(val_x, val_y), Dataset(test_x)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_set,\n",
    "                                                batch_size = batch_size,\n",
    "                                                shuffle = True,\n",
    "                                                num_workers = 8)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset = val_set,\n",
    "                                                batch_size = batch_size,\n",
    "                                                shuffle = False,\n",
    "                                                num_workers = 8)\n",
    "\n",
    "    # model\n",
    "    rnn_model = lstm_model(w2v_preprocesser._get_emb_matrix()).to(device)\n",
    "    logs = training(rnn_model, train_loader, val_loader, device, num_epoch = num_epoch, data_path= data_path, learning_rate = learning_rate)\n",
    "    # test\n",
    "    model = torch.load(os.path.join(data_path, 'best_semi.model'))\n",
    "    model.eval()\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        for i, inputs in enumerate(test_loader):\n",
    "            inputs = inputs.to(device, dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            test_pred += outputs.tolist() \n",
    "    test_pred = np.array(test_pred)\n",
    "    test_label = np.zeros(test_pred.shape, dtype =np.int8)\n",
    "    test_label[test_pred>=0.5] = 1\n",
    "    test_label[test_pred< 0.5] = 0\n",
    "    log_acc.append(np.sum(test_label == test_y)/len(test_label))\n",
    "    # Choose the data that predicts the best results\n",
    "    add_x, add_y = test_x[(test_pred>high) | (test_pred < low)], test_label[(test_pred>high) | (test_pred < low)]\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(\"train size: {}\".format(train_test_x.shape[0]))\n",
    "    print(\"current acc: {}\".format(log_acc[-1]))\n",
    "    print(\"adding nolabel data,\")\n",
    "    print(\"- lable 1: {}\".format(np.sum(add_y==1)))\n",
    "    print(\"- lable 0: {}\".format(np.sum(add_y==0)))\n",
    "    print('-------------------------------------------------')\n",
    "    # new dataset\n",
    "    train_test_x, train_test_y = np.concatenate((train_x, add_x)),np.concatenate((train_y, add_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rlUgjacX2ymu"
   },
   "source": [
    "2000 笔训练数据，阈值0.1，0.9\n",
    "````\n",
    "\n",
    "[010/010] 1.15 sec(s) Train Acc: 0.776513 Loss: 3.956478 | Val Acc: 0.743198 loss: 4.180540\n",
    "Saving with acc : 0.7431977431977432\n",
    "------------------------------------------------\n",
    "train size: 2197\n",
    "current acc: 0.7428030019163446\n",
    "adding nolabel data,\n",
    "- lable 1: 11926\n",
    "- lable 0: 39648\n",
    "-------------------------------------------------\n",
    "[007/010] 4.67 sec(s) Train Acc: 0.993156 Loss: 0.169375 | Val Acc: 0.753299 loss: 5.880286\n",
    "Saving with acc : 0.7532987532987533\n",
    "------------------------------------------------\n",
    "train size: 53771\n",
    "current acc: 0.7501472052415772\n",
    "adding nolabel data,\n",
    "- lable 1: 50298\n",
    "- lable 0: 67212\n",
    "-------------------------------------------------\n",
    "[006/010] 8.83 sec(s) Train Acc: 0.996884 Loss: 0.098808 | Val Acc: 0.758213 loss: 9.933585\n",
    "Saving with acc : 0.7582127582127582\n",
    "------------------------------------------------\n",
    "train size: 119707\n",
    "current acc: 0.7533696618026486\n",
    "adding nolabel data,\n",
    "- lable 1: 83770\n",
    "- lable 0: 87302\n",
    "-------------------------------------------------\n",
    "[003/010] 12.39 sec(s) Train Acc: 0.990293 Loss: 0.243880 | Val Acc: 0.759032 loss: 9.990977\n",
    "Saving with acc : 0.759031759031759\n",
    "------------------------------------------------\n",
    "train size: 173269\n",
    "current acc: 0.7551040071943216\n",
    "adding nolabel data,\n",
    "- lable 1: 85105\n",
    "- lable 0: 88555\n",
    "-------------------------------------------------\n",
    "[009/010] 12.56 sec(s) Train Acc: 0.995593 Loss: 0.135653 | Val Acc: 0.759578 loss: 11.569117\n",
    "Saving with acc : 0.7595777595777595\n",
    "------------------------------------------------\n",
    "train size: 175857\n",
    "current acc: 0.7552967122378409\n",
    "adding nolabel data,\n",
    "- lable 1: 90647\n",
    "- lable 0: 89291\n",
    "-------------------------------------------------\n",
    "[001/010] 13.09 sec(s) Train Acc: 0.941543 Loss: 1.036670 | Val Acc: 0.759669 loss: 9.814486\n",
    "Saving with acc : 0.7596687596687597\n",
    "------------------------------------------------\n",
    "train size: 182135\n",
    "current acc: 0.7561478261800507\n",
    "adding nolabel data,\n",
    "- lable 1: 85990\n",
    "- lable 0: 86486\n",
    "````\n",
    "\n",
    "20000笔训练数据，阈值0.1，0.9\n",
    "\n",
    "````\n",
    "[007/010] 2.34 sec(s) Train Acc: 0.820600 Loss: 0.003122 | Val Acc: 0.787300 loss: 0.003761\n",
    "Saving with acc : 0.7873\n",
    "------------------------------------------------\n",
    "train size: 20000\n",
    "current acc: 0.7809823529411765\n",
    "adding nolabel data,\n",
    "- lable 1: 48917\n",
    "- lable 0: 62929\n",
    "-------------------------------------------------\n",
    "[003/010] 9.63 sec(s) Train Acc: 0.970170 Loss: 0.000765 | Val Acc: 0.789600 loss: 0.004963\n",
    "Saving with acc : 0.7896\n",
    "------------------------------------------------\n",
    "train size: 131846\n",
    "current acc: 0.7816235294117647\n",
    "adding nolabel data,\n",
    "- lable 1: 64053\n",
    "- lable 0: 74953\n",
    "-------------------------------------------------\n",
    "[008/010] 11.46 sec(s) Train Acc: 0.981994 Loss: 0.000544 | Val Acc: 0.790200 loss: 0.005861\n",
    "Saving with acc : 0.7902\n",
    "------------------------------------------------\n",
    "train size: 159006\n",
    "current acc: 0.7817\n",
    "adding nolabel data,\n",
    "- lable 1: 76039\n",
    "- lable 0: 82730\n",
    "-------------------------------------------------\n",
    "[001/010] 12.80 sec(s) Train Acc: 0.909515 Loss: 0.001736 | Val Acc: 0.791300 loss: 0.005236\n",
    "Saving with acc : 0.7913\n",
    "------------------------------------------------\n",
    "train size: 178769\n",
    "current acc: 0.782035294117647\n",
    "adding nolabel data,\n",
    "- lable 1: 76408\n",
    "- lable 0: 77083\n",
    "-------------------------------------------------\n",
    "[005/010] 12.35 sec(s) Train Acc: 0.977388 Loss: 0.000700 | Val Acc: 0.793300 loss: 0.005935\n",
    "Saving with acc : 0.7933\n",
    "------------------------------------------------\n",
    "train size: 173491\n",
    "current acc: 0.7831176470588235\n",
    "adding nolabel data,\n",
    "- lable 1: 80023\n",
    "- lable 0: 82715\n",
    "-------------------------------------------------\n",
    "\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1648,
     "status": "ok",
     "timestamp": 1586685979165,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "oarz1CB22m2Z",
    "outputId": "52b0aa1f-0e2f-4289-ae5d-029023ab1607"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7428030019163446, 0.7501472052415772, 0.7533696618026486, 0.7551040071943216, 0.7552967122378409, 0.7561478261800507, 0.7541351290588499, 0.7559658269722825, 0.7566884708854796, 0.7570096459580117]\n"
     ]
    }
   ],
   "source": [
    "print(log_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sN3-diqQyQ6J"
   },
   "source": [
    "#### 结论\n",
    "\n",
    "上面是self-learning的实验日志，实验将train data分为3部分，2000笔作为训练数据（为了更好地看到模型提升，所以减少到2000笔），10000笔作为验证集用于early stop，180000笔作为测试集，模拟no label data。test部分的label可以计算准确率，不过只用于模型评价，不参与训练。\n",
    "\n",
    "模型每次迭代选用预测概率大于0.9或小于0.1的数据假如训练集，迭代10次。\n",
    "\n",
    "结果如下：\n",
    "\n",
    "- 初始准确率74，模型经过10次迭代，准确率为75.7%，提升尚可\n",
    "- 可以观察到一开始模型预测数据中，符合阈值的只有10000和30000，随着迭代，慢慢变多，第十次迭代选用接近90000和90000笔数据，这说明，模型预测结果越来越极端。\n",
    "- 经过一次迭代，后面第一个epoch的train acc一开始就达到很高。这个现象可能是这样的，假如模型最终总是容易到达某种状态，而我用这种状态取预测，取出极端结果，加入训练集，那么模型再次到达接近该状态的时候，极端选择的数据符合“处于这个状态的模型”的预测，所以这时候对于这些极端数据，预测结果会和它们的伪标签很接近，因而此时准确率很高，loss很低。\n",
    "\n",
    "结论：\n",
    "\n",
    "self-training会记忆error，模型训练后预测的结果会存在某些error，一旦你把这些error加入训练集作为新模型的目标开始训练，则这种error会在每次迭代中传播。\n",
    "所以结果不会提升非常大就是。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9REakzT8Dx75"
   },
   "source": [
    "### 6.4.2 Entropy-based Regularization\n",
    "\n",
    "$$E\\left(y^{u}\\right)=-\\sum_{m=1}^{5} y_{m}^{u} \\ln \\left(y_{m}^{u}\\right)$$\n",
    "\n",
    "可视为正则项，它让模型输出结果尽可能极端。所以基本和上面的模型一模一样，只需要自定义损失函数即可。\n",
    "\n",
    "**结论**\n",
    "\n",
    "提升微乎其微。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zMq0hymHFuTN"
   },
   "outputs": [],
   "source": [
    "class EBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.crossy = nn.BCELoss()\n",
    "    def forward(self, y_hat, y, lambda_= 1):\n",
    "        E = -torch.mean( y_hat * torch.log(y_hat) + (1 - y_hat) * torch.log(1 - y_hat))\n",
    "        # print(E)\n",
    "        #  不知道为什么，我这边自己写的binary crossy entropy 不行，结果是一样的。\n",
    "        # return -torch.sum(y*torch.log(y_hat)+(1-y)*torch.log(1-y_hat)) + lambda_ * E\n",
    "        return self.crossy(y_hat, y) + lambda_ * E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 559,
     "status": "ok",
     "timestamp": 1586693755169,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "iCNBtYqiJrE4",
    "outputId": "cba215ba-ab6d-47b0-e2d8-10e73cd32471"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5004024235381879\n",
      "tensor(1.6094)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.6094)"
      ]
     },
     "execution_count": 159,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(0.2*np.log(0.2)+0.8*np.log(0.8))\n",
    "a = [0.2]\n",
    "input = torch.Tensor(np.array(a))\n",
    "target = torch.empty(1).random_(2)\n",
    "m = nn.Sigmoid()\n",
    "m(input)\n",
    "print(nn.BCELoss()(input, target))\n",
    "EBCELoss()(input, target, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGDbGEsgGw5a"
   },
   "outputs": [],
   "source": [
    "def e_training(model, train_loader, val_loader, device,num_epoch = 5,  lambda_ = 1,data_path = './',learning_rate = 0.001):\n",
    "    def cal_acc(pred_b, y_b):\n",
    "        output_b = pred_b.cpu().detach().numpy().reshape(-1)\n",
    "        output_b[output_b>=0.5] = 1\n",
    "        output_b[output_b < 0.5] = 0\n",
    "        output_b = output_b.astype(np.int32)\n",
    "        return np.sum(output_b == y_b.numpy())\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) \n",
    "    loss_func = EBCELoss()\n",
    "    logs = {'time':[], 'train_loss':[], 'train_acc':[], 'val_acc':[], 'val_loss':[]}\n",
    "    best_epochs = []\n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epoch):\n",
    "        time_epoch_start = time.time()\n",
    "        train_acc = 0.0 \n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        for i, (x_b, y_b) in enumerate(train_loader):\n",
    "            # forward\n",
    "            optimizer.zero_grad() \n",
    "            pred_b = model(x_b.to(device, dtype = torch.long)) # torch的特点，需要将数据设置到gpu上\n",
    "            loss_b = loss_func(pred_b, y_b.to(device, dtype = torch.float), lambda_)\n",
    "            # backward\n",
    "            loss_b.backward()\n",
    "            optimizer.step()\n",
    "            # log\n",
    "            train_acc += cal_acc(pred_b, y_b)\n",
    "            train_loss += loss_b.item()\n",
    "    # calculate val acc and loss\n",
    "        model.eval()\n",
    "        val_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i,(x_b, y_b) in enumerate(val_loader):\n",
    "                pred_b = model(x_b.to(device, dtype = torch.long))\n",
    "                loss_b = loss_func(pred_b, y_b.to(device, dtype = torch.float), lambda_)\n",
    "                val_acc += cal_acc(pred_b, y_b)\n",
    "                val_loss += loss_b.item()\n",
    "        # print log\n",
    "        logs['time'].append(time.time()-time_epoch_start)\n",
    "        logs['train_acc'].append(train_acc/train_set.__len__())\n",
    "        logs['train_loss'].append(train_loss/train_set.__len__())\n",
    "        logs['val_acc'].append(val_acc/val_set.__len__())\n",
    "        logs['val_loss'].append(val_loss/val_set.__len__()) \n",
    "        # notes：打印logs\n",
    "        # print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "                (epoch + 1, num_epoch,  logs['time'][-1], \\\n",
    "                logs['train_acc'][-1] , logs['train_loss'][-1]*1000, logs['val_acc'][-1], logs['val_loss'][-1]*1000)) \n",
    "        # save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_epochs.append(epoch)\n",
    "            torch.save(model, os.path.join(data_path, 'best_semi.model'))\n",
    "            print(\"Saving with acc : {}\".format(logs['val_acc'][-1]))\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Em0hZcJRIROu"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 0.001\n",
    "lambda_ = 0.3 # entropy rate\n",
    "num_epoch = 10\n",
    "batch_size = 128\n",
    "low = 0.01\n",
    "high = 0.99\n",
    "log_acc = []\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_set,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            num_workers = 8)\n",
    "train_test_x, train_test_y = train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 262584,
     "status": "ok",
     "timestamp": 1586694570433,
     "user": {
      "displayName": "KD Lin",
      "photoUrl": "",
      "userId": "07150804307204552996"
     },
     "user_tz": -480
    },
    "id": "VkCL5Ov4Iav8",
    "outputId": "aa0ba5e6-7eef-4093-e639-eae85da3de30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/010] 2.51 sec(s) Train Acc: 0.648200 Loss: 6.248311 | Val Acc: 0.732100 loss: 5.433541\n",
      "Saving with acc : 0.7321\n",
      "[002/010] 2.48 sec(s) Train Acc: 0.756600 Loss: 5.035489 | Val Acc: 0.770900 loss: 5.084585\n",
      "Saving with acc : 0.7709\n",
      "[003/010] 2.51 sec(s) Train Acc: 0.776800 Loss: 4.704654 | Val Acc: 0.771600 loss: 4.827197\n",
      "Saving with acc : 0.7716\n",
      "[004/010] 2.58 sec(s) Train Acc: 0.787650 Loss: 4.540770 | Val Acc: 0.772700 loss: 4.819413\n",
      "Saving with acc : 0.7727\n",
      "[005/010] 2.53 sec(s) Train Acc: 0.796650 Loss: 4.352282 | Val Acc: 0.782600 loss: 4.724758\n",
      "Saving with acc : 0.7826\n",
      "[006/010] 2.52 sec(s) Train Acc: 0.805500 Loss: 4.222201 | Val Acc: 0.776800 loss: 4.759089\n",
      "[007/010] 2.51 sec(s) Train Acc: 0.817250 Loss: 3.989873 | Val Acc: 0.767900 loss: 5.013383\n",
      "[008/010] 2.47 sec(s) Train Acc: 0.827650 Loss: 3.828512 | Val Acc: 0.780300 loss: 4.852379\n",
      "[009/010] 2.47 sec(s) Train Acc: 0.846750 Loss: 3.474392 | Val Acc: 0.772700 loss: 5.089710\n",
      "[010/010] 2.49 sec(s) Train Acc: 0.862650 Loss: 3.178172 | Val Acc: 0.764600 loss: 5.482867\n",
      "------------------------------------------------\n",
      "train size: 20000\n",
      "current acc: 0.7816941176470589\n",
      "adding nolabel data,\n",
      "- lable 1: 5819\n",
      "- lable 0: 11768\n",
      "-------------------------------------------------\n",
      "[001/010] 3.73 sec(s) Train Acc: 0.824886 Loss: 3.634790 | Val Acc: 0.753800 loss: 5.076803\n",
      "Saving with acc : 0.7538\n",
      "[002/010] 3.70 sec(s) Train Acc: 0.878256 Loss: 2.642953 | Val Acc: 0.760800 loss: 4.920262\n",
      "Saving with acc : 0.7608\n",
      "[003/010] 3.74 sec(s) Train Acc: 0.884508 Loss: 2.489763 | Val Acc: 0.779600 loss: 4.954840\n",
      "Saving with acc : 0.7796\n",
      "[004/010] 3.74 sec(s) Train Acc: 0.889669 Loss: 2.382325 | Val Acc: 0.775700 loss: 4.809128\n",
      "[005/010] 3.74 sec(s) Train Acc: 0.892117 Loss: 2.300082 | Val Acc: 0.786200 loss: 4.717941\n",
      "Saving with acc : 0.7862\n",
      "[006/010] 3.74 sec(s) Train Acc: 0.897943 Loss: 2.217749 | Val Acc: 0.780300 loss: 4.772831\n",
      "[007/010] 3.74 sec(s) Train Acc: 0.904169 Loss: 2.095070 | Val Acc: 0.780600 loss: 5.012825\n",
      "[008/010] 3.72 sec(s) Train Acc: 0.910714 Loss: 1.952419 | Val Acc: 0.772700 loss: 5.115722\n",
      "[009/010] 3.77 sec(s) Train Acc: 0.919493 Loss: 1.781327 | Val Acc: 0.768600 loss: 5.353241\n",
      "[010/010] 3.70 sec(s) Train Acc: 0.928007 Loss: 1.606774 | Val Acc: 0.765900 loss: 5.583426\n",
      "------------------------------------------------\n",
      "train size: 37587\n",
      "current acc: 0.7834470588235294\n",
      "adding nolabel data,\n",
      "- lable 1: 10714\n",
      "- lable 0: 13063\n",
      "-------------------------------------------------\n",
      "[001/010] 4.18 sec(s) Train Acc: 0.602280 Loss: 6.610866 | Val Acc: 0.498600 loss: 7.138659\n",
      "Saving with acc : 0.4986\n",
      "[002/010] 4.19 sec(s) Train Acc: 0.591429 Loss: 6.611729 | Val Acc: 0.672400 loss: 6.834016\n",
      "Saving with acc : 0.6724\n",
      "[003/010] 4.18 sec(s) Train Acc: 0.881582 Loss: 2.672971 | Val Acc: 0.766600 loss: 5.116840\n",
      "Saving with acc : 0.7666\n",
      "[004/010] 4.15 sec(s) Train Acc: 0.894100 Loss: 2.309241 | Val Acc: 0.733700 loss: 5.394599\n",
      "[005/010] 4.19 sec(s) Train Acc: 0.897024 Loss: 2.235628 | Val Acc: 0.767700 loss: 5.057441\n",
      "Saving with acc : 0.7677\n",
      "[006/010] 4.17 sec(s) Train Acc: 0.900724 Loss: 2.126157 | Val Acc: 0.768800 loss: 5.240816\n",
      "Saving with acc : 0.7688\n",
      "[007/010] 4.19 sec(s) Train Acc: 0.903031 Loss: 2.117521 | Val Acc: 0.760000 loss: 4.999765\n",
      "[008/010] 4.18 sec(s) Train Acc: 0.899879 Loss: 2.156356 | Val Acc: 0.772900 loss: 5.211196\n",
      "Saving with acc : 0.7729\n",
      "[009/010] 4.17 sec(s) Train Acc: 0.907052 Loss: 1.999255 | Val Acc: 0.756000 loss: 5.031468\n",
      "[010/010] 4.22 sec(s) Train Acc: 0.908354 Loss: 1.947141 | Val Acc: 0.786000 loss: 4.827530\n",
      "Saving with acc : 0.786\n",
      "------------------------------------------------\n",
      "train size: 43777\n",
      "current acc: 0.7831117647058824\n",
      "adding nolabel data,\n",
      "- lable 1: 22115\n",
      "- lable 0: 18594\n",
      "-------------------------------------------------\n",
      "[001/010] 5.39 sec(s) Train Acc: 0.640399 Loss: 6.259273 | Val Acc: 0.640200 loss: 6.942075\n",
      "Saving with acc : 0.6402\n",
      "[002/010] 5.36 sec(s) Train Acc: 0.668188 Loss: 5.794994 | Val Acc: 0.630000 loss: 6.886653\n",
      "[003/010] 5.37 sec(s) Train Acc: 0.877003 Loss: 2.978478 | Val Acc: 0.755800 loss: 5.997949\n",
      "Saving with acc : 0.7558\n",
      "[004/010] 5.37 sec(s) Train Acc: 0.920358 Loss: 1.945035 | Val Acc: 0.765900 loss: 5.085778\n",
      "Saving with acc : 0.7659\n",
      "[005/010] 5.44 sec(s) Train Acc: 0.923240 Loss: 1.826652 | Val Acc: 0.755500 loss: 5.416344\n",
      "[006/010] 5.50 sec(s) Train Acc: 0.926551 Loss: 1.708863 | Val Acc: 0.780200 loss: 5.207455\n",
      "Saving with acc : 0.7802\n",
      "[007/010] 5.42 sec(s) Train Acc: 0.928083 Loss: 1.628750 | Val Acc: 0.782300 loss: 4.850074\n",
      "Saving with acc : 0.7823\n",
      "[008/010] 5.40 sec(s) Train Acc: 0.931328 Loss: 1.557861 | Val Acc: 0.776100 loss: 4.968523\n",
      "[009/010] 5.38 sec(s) Train Acc: 0.932761 Loss: 1.498654 | Val Acc: 0.785200 loss: 5.012819\n",
      "Saving with acc : 0.7852\n",
      "[010/010] 5.40 sec(s) Train Acc: 0.933717 Loss: 1.463784 | Val Acc: 0.784900 loss: 4.992991\n",
      "------------------------------------------------\n",
      "train size: 60709\n",
      "current acc: 0.7812588235294118\n",
      "adding nolabel data,\n",
      "- lable 1: 23699\n",
      "- lable 0: 26262\n",
      "-------------------------------------------------\n",
      "[001/010] 6.05 sec(s) Train Acc: 0.888895 Loss: 2.464490 | Val Acc: 0.772900 loss: 5.375454\n",
      "Saving with acc : 0.7729\n",
      "[002/010] 6.08 sec(s) Train Acc: 0.935550 Loss: 1.544315 | Val Acc: 0.777600 loss: 4.956572\n",
      "Saving with acc : 0.7776\n",
      "[003/010] 6.06 sec(s) Train Acc: 0.939138 Loss: 1.424580 | Val Acc: 0.776800 loss: 5.066558\n",
      "[004/010] 6.03 sec(s) Train Acc: 0.940910 Loss: 1.347964 | Val Acc: 0.769600 loss: 5.286130\n",
      "[005/010] 6.08 sec(s) Train Acc: 0.943583 Loss: 1.298447 | Val Acc: 0.786500 loss: 5.139339\n",
      "Saving with acc : 0.7865\n",
      "[006/010] 6.08 sec(s) Train Acc: 0.945655 Loss: 1.213438 | Val Acc: 0.787400 loss: 5.791182\n",
      "Saving with acc : 0.7874\n",
      "[007/010] 6.07 sec(s) Train Acc: 0.948757 Loss: 1.143379 | Val Acc: 0.776700 loss: 5.686602\n",
      "[008/010] 6.09 sec(s) Train Acc: 0.953260 Loss: 1.046369 | Val Acc: 0.779300 loss: 6.012300\n",
      "[009/010] 6.07 sec(s) Train Acc: 0.958734 Loss: 0.939025 | Val Acc: 0.768200 loss: 5.745823\n",
      "[010/010] 6.07 sec(s) Train Acc: 0.964895 Loss: 0.811657 | Val Acc: 0.761500 loss: 7.072319\n",
      "------------------------------------------------\n",
      "train size: 69961\n",
      "current acc: 0.7847176470588235\n",
      "adding nolabel data,\n",
      "- lable 1: 40404\n",
      "- lable 0: 41588\n",
      "-------------------------------------------------\n",
      "[001/010] 8.39 sec(s) Train Acc: 0.926396 Loss: 1.826598 | Val Acc: 0.781500 loss: 5.730542\n",
      "Saving with acc : 0.7815\n",
      "[002/010] 8.44 sec(s) Train Acc: 0.957301 Loss: 1.179705 | Val Acc: 0.783400 loss: 5.919544\n",
      "Saving with acc : 0.7834\n",
      "[003/010] 8.36 sec(s) Train Acc: 0.959752 Loss: 1.077968 | Val Acc: 0.786200 loss: 5.979901\n",
      "Saving with acc : 0.7862\n",
      "[004/010] 8.33 sec(s) Train Acc: 0.961958 Loss: 1.005803 | Val Acc: 0.786600 loss: 5.466662\n",
      "Saving with acc : 0.7866\n",
      "[005/010] 8.36 sec(s) Train Acc: 0.963850 Loss: 0.926975 | Val Acc: 0.784800 loss: 5.953185\n",
      "[006/010] 8.32 sec(s) Train Acc: 0.966213 Loss: 0.850783 | Val Acc: 0.786700 loss: 5.528215\n",
      "Saving with acc : 0.7867\n",
      "[007/010] 8.34 sec(s) Train Acc: 0.968752 Loss: 0.785852 | Val Acc: 0.780800 loss: 6.088794\n",
      "[008/010] 8.38 sec(s) Train Acc: 0.972547 Loss: 0.703420 | Val Acc: 0.780900 loss: 6.510166\n",
      "[009/010] 8.40 sec(s) Train Acc: 0.975508 Loss: 0.640039 | Val Acc: 0.772400 loss: 6.351744\n",
      "[010/010] 8.57 sec(s) Train Acc: 0.979998 Loss: 0.542574 | Val Acc: 0.763000 loss: 7.153377\n",
      "------------------------------------------------\n",
      "train size: 101992\n",
      "current acc: 0.7828529411764706\n",
      "adding nolabel data,\n",
      "- lable 1: 39968\n",
      "- lable 0: 36142\n",
      "-------------------------------------------------\n",
      "[001/010] 7.91 sec(s) Train Acc: 0.914681 Loss: 2.005486 | Val Acc: 0.779000 loss: 5.510721\n",
      "Saving with acc : 0.779\n",
      "[002/010] 7.93 sec(s) Train Acc: 0.954916 Loss: 1.219321 | Val Acc: 0.780700 loss: 5.547425\n",
      "Saving with acc : 0.7807\n",
      "[003/010] 7.96 sec(s) Train Acc: 0.957830 Loss: 1.088896 | Val Acc: 0.784500 loss: 5.453128\n",
      "Saving with acc : 0.7845\n",
      "[004/010] 7.95 sec(s) Train Acc: 0.961097 Loss: 0.987985 | Val Acc: 0.782700 loss: 5.761620\n",
      "[005/010] 7.89 sec(s) Train Acc: 0.963844 Loss: 0.900723 | Val Acc: 0.780600 loss: 5.626979\n",
      "[006/010] 7.91 sec(s) Train Acc: 0.966195 Loss: 0.832595 | Val Acc: 0.777600 loss: 6.446960\n",
      "[007/010] 7.94 sec(s) Train Acc: 0.968515 Loss: 0.772845 | Val Acc: 0.777300 loss: 6.638481\n",
      "[008/010] 7.87 sec(s) Train Acc: 0.972271 Loss: 0.676902 | Val Acc: 0.775400 loss: 6.174359\n",
      "[009/010] 7.92 sec(s) Train Acc: 0.976641 Loss: 0.593471 | Val Acc: 0.765600 loss: 6.886308\n",
      "[010/010] 7.97 sec(s) Train Acc: 0.980793 Loss: 0.509478 | Val Acc: 0.771100 loss: 7.779644\n",
      "------------------------------------------------\n",
      "train size: 96110\n",
      "current acc: 0.780035294117647\n",
      "adding nolabel data,\n",
      "- lable 1: 29715\n",
      "- lable 0: 41848\n",
      "-------------------------------------------------\n",
      "[001/010] 7.63 sec(s) Train Acc: 0.905267 Loss: 2.301191 | Val Acc: 0.773800 loss: 5.604280\n",
      "Saving with acc : 0.7738\n",
      "[002/010] 7.58 sec(s) Train Acc: 0.951749 Loss: 1.299346 | Val Acc: 0.781700 loss: 5.068060\n",
      "Saving with acc : 0.7817\n",
      "[003/010] 7.58 sec(s) Train Acc: 0.954774 Loss: 1.168665 | Val Acc: 0.782300 loss: 5.172992\n",
      "Saving with acc : 0.7823\n",
      "[004/010] 7.64 sec(s) Train Acc: 0.956729 Loss: 1.096386 | Val Acc: 0.773400 loss: 5.497969\n",
      "[005/010] 7.64 sec(s) Train Acc: 0.959121 Loss: 1.015608 | Val Acc: 0.781100 loss: 5.508895\n",
      "[006/010] 7.68 sec(s) Train Acc: 0.961360 Loss: 0.957932 | Val Acc: 0.774200 loss: 5.562803\n",
      "[007/010] 7.58 sec(s) Train Acc: 0.963861 Loss: 0.894220 | Val Acc: 0.773900 loss: 5.555977\n",
      "[008/010] 7.55 sec(s) Train Acc: 0.966777 Loss: 0.808648 | Val Acc: 0.776800 loss: 6.355165\n",
      "[009/010] 7.56 sec(s) Train Acc: 0.970021 Loss: 0.744905 | Val Acc: 0.774800 loss: 6.744621\n",
      "[010/010] 7.58 sec(s) Train Acc: 0.974193 Loss: 0.657691 | Val Acc: 0.769800 loss: 6.615981\n",
      "------------------------------------------------\n",
      "train size: 91563\n",
      "current acc: 0.7790588235294118\n",
      "adding nolabel data,\n",
      "- lable 1: 22447\n",
      "- lable 0: 34533\n",
      "-------------------------------------------------\n",
      "[001/010] 6.51 sec(s) Train Acc: 0.906534 Loss: 2.214701 | Val Acc: 0.765400 loss: 5.495915\n",
      "Saving with acc : 0.7654\n",
      "[002/010] 6.55 sec(s) Train Acc: 0.942414 Loss: 1.415526 | Val Acc: 0.779700 loss: 5.376507\n",
      "Saving with acc : 0.7797\n",
      "[003/010] 6.54 sec(s) Train Acc: 0.946220 Loss: 1.281419 | Val Acc: 0.782700 loss: 5.126262\n",
      "Saving with acc : 0.7827\n",
      "[004/010] 6.58 sec(s) Train Acc: 0.948246 Loss: 1.219694 | Val Acc: 0.766400 loss: 5.297795\n",
      "[005/010] 6.59 sec(s) Train Acc: 0.950169 Loss: 1.168877 | Val Acc: 0.780600 loss: 5.333243\n",
      "[006/010] 6.53 sec(s) Train Acc: 0.952975 Loss: 1.084392 | Val Acc: 0.771400 loss: 5.522798\n",
      "[007/010] 6.55 sec(s) Train Acc: 0.956443 Loss: 1.009961 | Val Acc: 0.773100 loss: 5.586048\n",
      "[008/010] 6.52 sec(s) Train Acc: 0.959912 Loss: 0.927781 | Val Acc: 0.775500 loss: 5.612648\n",
      "[009/010] 6.55 sec(s) Train Acc: 0.965822 Loss: 0.819807 | Val Acc: 0.778200 loss: 6.480778\n",
      "[010/010] 6.57 sec(s) Train Acc: 0.970200 Loss: 0.738627 | Val Acc: 0.765000 loss: 6.397323\n",
      "------------------------------------------------\n",
      "train size: 76980\n",
      "current acc: 0.7803529411764706\n",
      "adding nolabel data,\n",
      "- lable 1: 25173\n",
      "- lable 0: 35252\n",
      "-------------------------------------------------\n",
      "[001/010] 6.80 sec(s) Train Acc: 0.911097 Loss: 2.106971 | Val Acc: 0.775200 loss: 5.172036\n",
      "Saving with acc : 0.7752\n",
      "[002/010] 6.80 sec(s) Train Acc: 0.945713 Loss: 1.366718 | Val Acc: 0.775300 loss: 4.937779\n",
      "Saving with acc : 0.7753\n",
      "[003/010] 6.77 sec(s) Train Acc: 0.947976 Loss: 1.244043 | Val Acc: 0.782700 loss: 5.064082\n",
      "Saving with acc : 0.7827\n",
      "[004/010] 6.77 sec(s) Train Acc: 0.949978 Loss: 1.172460 | Val Acc: 0.778800 loss: 5.164232\n",
      "[005/010] 6.82 sec(s) Train Acc: 0.951881 Loss: 1.118712 | Val Acc: 0.781100 loss: 5.304127\n",
      "[006/010] 6.85 sec(s) Train Acc: 0.954554 Loss: 1.059475 | Val Acc: 0.779700 loss: 5.691582\n",
      "[007/010] 6.80 sec(s) Train Acc: 0.957911 Loss: 0.982293 | Val Acc: 0.769600 loss: 5.638190\n",
      "[008/010] 6.76 sec(s) Train Acc: 0.961728 Loss: 0.899343 | Val Acc: 0.770100 loss: 6.221616\n",
      "[009/010] 6.77 sec(s) Train Acc: 0.965807 Loss: 0.812320 | Val Acc: 0.766200 loss: 6.150774\n",
      "[010/010] 6.91 sec(s) Train Acc: 0.970643 Loss: 0.711119 | Val Acc: 0.771700 loss: 6.375217\n",
      "------------------------------------------------\n",
      "train size: 80425\n",
      "current acc: 0.7810529411764706\n",
      "adding nolabel data,\n",
      "- lable 1: 20791\n",
      "- lable 0: 33793\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    train_set,val_set, test_set = Dataset(train_test_x, train_test_y),Dataset(val_x, val_y), Dataset(test_x)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_set,\n",
    "                                                batch_size = batch_size,\n",
    "                                                shuffle = True,\n",
    "                                                num_workers = 8)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset = val_set,\n",
    "                                                batch_size = batch_size,\n",
    "                                                shuffle = False,\n",
    "                                                num_workers = 8)\n",
    "\n",
    "    # model\n",
    "    rnn_model = lstm_model(w2v_preprocesser._get_emb_matrix()).to(device)\n",
    "    logs = e_training(rnn_model, train_loader, val_loader, device, lambda_ = lambda_, num_epoch = num_epoch, data_path= data_path, learning_rate = learning_rate)\n",
    "    # test\n",
    "    model = torch.load(os.path.join(data_path, 'best_semi.model'))\n",
    "    model.eval()\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        for i, inputs in enumerate(test_loader):\n",
    "            inputs = inputs.to(device, dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            test_pred += outputs.tolist() \n",
    "    test_pred = np.array(test_pred)\n",
    "    test_label = np.zeros(test_pred.shape, dtype =np.int8)\n",
    "    test_label[test_pred>=0.5] = 1\n",
    "    test_label[test_pred< 0.5] = 0\n",
    "    log_acc.append(np.sum(test_label == test_y)/len(test_label))\n",
    "    # Choose the data that predicts the best results\n",
    "    add_x, add_y = test_x[(test_pred>high) | (test_pred < low)], test_label[(test_pred>high) | (test_pred < low)]\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(\"train size: {}\".format(train_test_x.shape[0]))\n",
    "    print(\"current acc: {}\".format(log_acc[-1]))\n",
    "    print(\"adding nolabel data,\")\n",
    "    print(\"- lable 1: {}\".format(np.sum(add_y==1)))\n",
    "    print(\"- lable 0: {}\".format(np.sum(add_y==0)))\n",
    "    print('-------------------------------------------------')\n",
    "    # new dataset\n",
    "    train_test_x, train_test_y = np.concatenate((train_x, add_x)),np.concatenate((train_y, add_y))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "hw4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "215.813px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
